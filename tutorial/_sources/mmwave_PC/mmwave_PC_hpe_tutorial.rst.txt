
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "mmwave_PC/mmwave_PC_hpe_tutorial.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_mmwave_PC_mmwave_PC_hpe_tutorial.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_mmwave_PC_mmwave_PC_hpe_tutorial.py:


Tutorial for Human Pose Estimation
==============================================================

.. GENERATED FROM PYTHON SOURCE LINES 8-9

In[1]:

.. GENERATED FROM PYTHON SOURCE LINES 9-16

.. code-block:: Python


    import yaml
    import torch
    import torch.nn as nn
    from tqdm import tqdm
    import os








.. GENERATED FROM PYTHON SOURCE LINES 17-19

Dataset with MetaFi: 
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 21-24

Point cloud Pose reconstruction dataset collected by Ti 6843 mmWave radar. 40 subjects are included and the human poses are obtained by 2 RGB camera.
We provide cross-subject experiment settings with all daily activities. 
In the library, we provide a dataloader to use mmWave PC data, and predict these human poses. 

.. GENERATED FROM PYTHON SOURCE LINES 27-29

Load the data
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 31-32

In[3]:

.. GENERATED FROM PYTHON SOURCE LINES 32-39

.. code-block:: Python


    from pysensing.mmwave.PC.dataset.hpe import load_hpe_dataset
    # The path contains the radHAR dataset

    train_dataset, test_dataset = load_hpe_dataset("MetaFi")






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Try to download MetaFi dateset in /home/kemove/yyz/av-gihub/tutorials/mmwave_PC_source/mmfi
    Downloading MetaFi to /home/kemove/yyz/av-gihub/tutorials/mmwave_PC_source/mmfi.zip...
    Downloading:   0%|          | 0.00/260M [00:00<?, ?iB/s]    Downloading:   0%|          | 492k/260M [00:00<00:53, 4.81MiB/s]    Downloading:   0%|          | 974k/260M [00:00<01:10, 3.64MiB/s]    Downloading:   1%|          | 1.41M/260M [00:00<01:07, 3.80MiB/s]    Downloading:   1%|          | 1.80M/260M [00:00<01:09, 3.69MiB/s]    Downloading:   1%|          | 2.18M/260M [00:00<01:09, 3.69MiB/s]    Downloading:   1%|          | 2.55M/260M [00:00<01:10, 3.62MiB/s]    Downloading:   1%|          | 2.91M/260M [00:00<01:11, 3.60MiB/s]    Downloading:   1%|▏         | 3.33M/260M [00:00<01:08, 3.74MiB/s]    Downloading:   2%|▏         | 3.92M/260M [00:00<00:58, 4.34MiB/s]    Downloading:   2%|▏         | 4.44M/260M [00:01<00:55, 4.59MiB/s]    Downloading:   2%|▏         | 5.03M/260M [00:01<00:51, 4.90MiB/s]    Downloading:   2%|▏         | 5.59M/260M [00:01<00:49, 5.09MiB/s]    Downloading:   2%|▏         | 6.19M/260M [00:01<00:47, 5.37MiB/s]    Downloading:   3%|▎         | 6.73M/260M [00:01<00:47, 5.27MiB/s]    Downloading:   3%|▎         | 7.26M/260M [00:01<00:49, 5.13MiB/s]    Downloading:   3%|▎         | 7.88M/260M [00:01<00:46, 5.40MiB/s]    Downloading:   3%|▎         | 8.49M/260M [00:01<00:53, 4.71MiB/s]    Downloading:   3%|▎         | 9.04M/260M [00:01<00:51, 4.89MiB/s]    Downloading:   4%|▎         | 9.55M/260M [00:02<00:52, 4.79MiB/s]    Downloading:   4%|▍         | 10.0M/260M [00:02<00:52, 4.77MiB/s]    Downloading:   4%|▍         | 10.5M/260M [00:02<00:52, 4.77MiB/s]    Downloading:   4%|▍         | 11.1M/260M [00:02<00:49, 5.04MiB/s]    Downloading:   5%|▍         | 11.8M/260M [00:02<00:44, 5.61MiB/s]    Downloading:   5%|▍         | 12.4M/260M [00:02<00:43, 5.63MiB/s]    Downloading:   5%|▍         | 13.0M/260M [00:02<00:45, 5.45MiB/s]    Downloading:   5%|▌         | 13.6M/260M [00:02<00:42, 5.82MiB/s]    Downloading:   6%|▌         | 14.3M/260M [00:02<00:39, 6.18MiB/s]    Downloading:   6%|▌         | 15.0M/260M [00:03<00:39, 6.13MiB/s]    Downloading:   6%|▌         | 15.6M/260M [00:03<00:41, 5.84MiB/s]    Downloading:   6%|▌         | 16.2M/260M [00:03<00:41, 5.92MiB/s]    Downloading:   6%|▋         | 16.8M/260M [00:03<00:44, 5.51MiB/s]    Downloading:   7%|▋         | 17.4M/260M [00:03<00:43, 5.54MiB/s]    Downloading:   7%|▋         | 17.9M/260M [00:03<00:48, 5.03MiB/s]    Downloading:   7%|▋         | 18.5M/260M [00:03<00:45, 5.33MiB/s]    Downloading:   7%|▋         | 19.1M/260M [00:03<00:51, 4.64MiB/s]    Downloading:   8%|▊         | 19.6M/260M [00:03<00:51, 4.69MiB/s]    Downloading:   8%|▊         | 20.1M/260M [00:04<00:51, 4.69MiB/s]    Downloading:   8%|▊         | 20.7M/260M [00:04<00:47, 5.00MiB/s]    Downloading:   8%|▊         | 21.3M/260M [00:04<00:44, 5.35MiB/s]    Downloading:   8%|▊         | 22.0M/260M [00:04<00:41, 5.77MiB/s]    Downloading:   9%|▊         | 22.6M/260M [00:04<00:40, 5.80MiB/s]    Downloading:   9%|▉         | 23.2M/260M [00:04<00:41, 5.69MiB/s]    Downloading:   9%|▉         | 23.8M/260M [00:04<00:43, 5.40MiB/s]    Downloading:   9%|▉         | 24.3M/260M [00:04<00:49, 4.77MiB/s]    Downloading:  10%|▉         | 24.8M/260M [00:04<00:48, 4.84MiB/s]    Downloading:  10%|▉         | 25.4M/260M [00:05<00:48, 4.85MiB/s]    Downloading:  10%|█         | 26.0M/260M [00:05<00:46, 5.06MiB/s]    Downloading:  10%|█         | 26.5M/260M [00:05<00:47, 4.87MiB/s]    Downloading:  10%|█         | 27.0M/260M [00:05<00:57, 4.06MiB/s]    Downloading:  11%|█         | 27.4M/260M [00:05<00:58, 4.00MiB/s]    Downloading:  11%|█         | 27.8M/260M [00:05<00:59, 3.87MiB/s]    Downloading:  11%|█         | 28.3M/260M [00:05<00:57, 4.02MiB/s]    Downloading:  11%|█         | 28.8M/260M [00:05<00:54, 4.24MiB/s]    Downloading:  11%|█▏        | 29.3M/260M [00:06<00:51, 4.49MiB/s]    Downloading:  11%|█▏        | 29.8M/260M [00:06<00:48, 4.74MiB/s]    Downloading:  12%|█▏        | 30.3M/260M [00:06<00:48, 4.71MiB/s]    Downloading:  12%|█▏        | 30.9M/260M [00:06<00:46, 4.89MiB/s]    Downloading:  12%|█▏        | 31.5M/260M [00:06<00:44, 5.11MiB/s]    Downloading:  12%|█▏        | 32.2M/260M [00:06<00:38, 5.88MiB/s]    Downloading:  13%|█▎        | 33.0M/260M [00:06<00:36, 6.22MiB/s]    Downloading:  13%|█▎        | 33.6M/260M [00:06<00:38, 5.81MiB/s]    Downloading:  13%|█▎        | 34.2M/260M [00:06<00:37, 6.03MiB/s]    Downloading:  13%|█▎        | 34.9M/260M [00:06<00:37, 6.03MiB/s]    Downloading:  14%|█▎        | 35.6M/260M [00:07<00:35, 6.24MiB/s]    Downloading:  14%|█▍        | 36.2M/260M [00:07<00:37, 6.00MiB/s]    Downloading:  14%|█▍        | 36.8M/260M [00:07<00:37, 5.93MiB/s]    Downloading:  14%|█▍        | 37.4M/260M [00:07<00:36, 6.08MiB/s]    Downloading:  15%|█▍        | 38.1M/260M [00:07<00:35, 6.25MiB/s]    Downloading:  15%|█▍        | 38.8M/260M [00:07<00:34, 6.47MiB/s]    Downloading:  15%|█▌        | 39.5M/260M [00:07<00:33, 6.54MiB/s]    Downloading:  16%|█▌        | 40.3M/260M [00:07<00:31, 6.92MiB/s]    Downloading:  16%|█▌        | 41.0M/260M [00:07<00:31, 6.97MiB/s]    Downloading:  16%|█▌        | 41.7M/260M [00:07<00:30, 7.07MiB/s]    Downloading:  16%|█▋        | 42.4M/260M [00:08<00:31, 6.79MiB/s]    Downloading:  17%|█▋        | 43.1M/260M [00:08<00:37, 5.82MiB/s]    Downloading:  17%|█▋        | 43.7M/260M [00:08<00:43, 4.93MiB/s]    Downloading:  17%|█▋        | 44.3M/260M [00:08<00:42, 5.07MiB/s]    Downloading:  17%|█▋        | 44.9M/260M [00:08<00:41, 5.22MiB/s]    Downloading:  18%|█▊        | 45.5M/260M [00:08<00:39, 5.43MiB/s]    Downloading:  18%|█▊        | 46.1M/260M [00:08<00:37, 5.74MiB/s]    Downloading:  18%|█▊        | 46.7M/260M [00:08<00:36, 5.86MiB/s]    Downloading:  18%|█▊        | 47.5M/260M [00:09<00:33, 6.25MiB/s]    Downloading:  19%|█▊        | 48.2M/260M [00:09<00:32, 6.44MiB/s]    Downloading:  19%|█▉        | 48.8M/260M [00:09<00:32, 6.44MiB/s]    Downloading:  19%|█▉        | 49.5M/260M [00:09<00:32, 6.55MiB/s]    Downloading:  19%|█▉        | 50.2M/260M [00:09<00:31, 6.71MiB/s]    Downloading:  20%|█▉        | 50.9M/260M [00:09<00:32, 6.35MiB/s]    Downloading:  20%|█▉        | 51.6M/260M [00:09<00:32, 6.38MiB/s]    Downloading:  20%|██        | 52.3M/260M [00:09<00:31, 6.66MiB/s]    Downloading:  20%|██        | 53.0M/260M [00:09<00:30, 6.72MiB/s]    Downloading:  21%|██        | 53.7M/260M [00:09<00:31, 6.59MiB/s]    Downloading:  21%|██        | 54.3M/260M [00:10<00:34, 6.01MiB/s]    Downloading:  21%|██        | 54.9M/260M [00:10<00:34, 5.94MiB/s]    Downloading:  21%|██▏       | 55.5M/260M [00:10<00:34, 5.85MiB/s]    Downloading:  22%|██▏       | 56.2M/260M [00:10<00:34, 5.97MiB/s]    Downloading:  22%|██▏       | 56.9M/260M [00:10<00:32, 6.23MiB/s]    Downloading:  22%|██▏       | 57.5M/260M [00:10<00:32, 6.29MiB/s]    Downloading:  22%|██▏       | 58.2M/260M [00:10<00:31, 6.36MiB/s]    Downloading:  23%|██▎       | 58.9M/260M [00:10<00:31, 6.28MiB/s]    Downloading:  23%|██▎       | 59.5M/260M [00:10<00:35, 5.63MiB/s]    Downloading:  23%|██▎       | 60.1M/260M [00:11<00:35, 5.64MiB/s]    Downloading:  23%|██▎       | 60.7M/260M [00:11<00:33, 5.91MiB/s]    Downloading:  24%|██▎       | 61.3M/260M [00:11<00:34, 5.76MiB/s]    Downloading:  24%|██▍       | 61.9M/260M [00:11<00:34, 5.68MiB/s]    Downloading:  24%|██▍       | 62.6M/260M [00:11<00:32, 6.04MiB/s]    Downloading:  24%|██▍       | 63.3M/260M [00:11<00:31, 6.32MiB/s]    Downloading:  25%|██▍       | 64.0M/260M [00:11<00:30, 6.51MiB/s]    Downloading:  25%|██▍       | 64.7M/260M [00:11<00:29, 6.52MiB/s]    Downloading:  25%|██▌       | 65.5M/260M [00:11<00:28, 6.81MiB/s]    Downloading:  26%|██▌       | 66.2M/260M [00:11<00:27, 7.03MiB/s]    Downloading:  26%|██▌       | 67.0M/260M [00:12<00:27, 6.98MiB/s]    Downloading:  26%|██▌       | 67.7M/260M [00:12<00:28, 6.79MiB/s]    Downloading:  26%|██▋       | 68.4M/260M [00:12<00:28, 6.74MiB/s]    Downloading:  27%|██▋       | 69.1M/260M [00:12<00:29, 6.50MiB/s]    Downloading:  27%|██▋       | 69.8M/260M [00:12<00:28, 6.67MiB/s]    Downloading:  27%|██▋       | 70.4M/260M [00:12<00:28, 6.69MiB/s]    Downloading:  27%|██▋       | 71.1M/260M [00:12<00:28, 6.55MiB/s]    Downloading:  28%|██▊       | 71.9M/260M [00:12<00:27, 6.71MiB/s]    Downloading:  28%|██▊       | 72.6M/260M [00:12<00:26, 6.93MiB/s]    Downloading:  28%|██▊       | 73.3M/260M [00:13<00:27, 6.84MiB/s]    Downloading:  29%|██▊       | 74.1M/260M [00:13<00:26, 7.01MiB/s]    Downloading:  29%|██▉       | 74.8M/260M [00:13<00:28, 6.55MiB/s]    Downloading:  29%|██▉       | 75.4M/260M [00:13<00:28, 6.39MiB/s]    Downloading:  29%|██▉       | 76.1M/260M [00:13<00:30, 5.94MiB/s]    Downloading:  30%|██▉       | 76.7M/260M [00:13<00:30, 5.95MiB/s]    Downloading:  30%|██▉       | 77.3M/260M [00:13<00:30, 5.94MiB/s]    Downloading:  30%|███       | 77.9M/260M [00:13<00:30, 5.92MiB/s]    Downloading:  30%|███       | 78.7M/260M [00:13<00:28, 6.42MiB/s]    Downloading:  31%|███       | 79.3M/260M [00:14<00:29, 6.15MiB/s]    Downloading:  31%|███       | 80.1M/260M [00:14<00:27, 6.59MiB/s]    Downloading:  31%|███       | 80.8M/260M [00:14<00:26, 6.71MiB/s]    Downloading:  31%|███▏      | 81.5M/260M [00:14<00:27, 6.42MiB/s]    Downloading:  32%|███▏      | 82.2M/260M [00:14<00:26, 6.62MiB/s]    Downloading:  32%|███▏      | 82.9M/260M [00:14<00:26, 6.64MiB/s]    Downloading:  32%|███▏      | 83.7M/260M [00:14<00:25, 6.96MiB/s]    Downloading:  33%|███▎      | 84.4M/260M [00:14<00:24, 7.08MiB/s]    Downloading:  33%|███▎      | 85.1M/260M [00:14<00:24, 7.06MiB/s]    Downloading:  33%|███▎      | 85.8M/260M [00:14<00:25, 6.92MiB/s]    Downloading:  33%|███▎      | 86.5M/260M [00:15<00:26, 6.60MiB/s]    Downloading:  34%|███▎      | 87.2M/260M [00:15<00:26, 6.54MiB/s]    Downloading:  34%|███▍      | 87.8M/260M [00:15<00:27, 6.36MiB/s]    Downloading:  34%|███▍      | 88.6M/260M [00:15<00:25, 6.70MiB/s]    Downloading:  34%|███▍      | 89.3M/260M [00:15<00:26, 6.51MiB/s]    Downloading:  35%|███▍      | 90.0M/260M [00:15<00:24, 6.85MiB/s]    Downloading:  35%|███▍      | 90.8M/260M [00:15<00:24, 6.96MiB/s]    Downloading:  35%|███▌      | 91.5M/260M [00:15<00:24, 6.94MiB/s]    Downloading:  36%|███▌      | 92.2M/260M [00:15<00:23, 6.99MiB/s]    Downloading:  36%|███▌      | 92.9M/260M [00:16<00:24, 6.92MiB/s]    Downloading:  36%|███▌      | 93.6M/260M [00:16<00:24, 6.86MiB/s]    Downloading:  36%|███▋      | 94.4M/260M [00:16<00:22, 7.19MiB/s]    Downloading:  37%|███▋      | 95.1M/260M [00:16<00:23, 7.02MiB/s]    Downloading:  37%|███▋      | 95.9M/260M [00:16<00:23, 7.02MiB/s]    Downloading:  37%|███▋      | 96.6M/260M [00:16<00:25, 6.49MiB/s]    Downloading:  37%|███▋      | 97.2M/260M [00:16<00:24, 6.50MiB/s]    Downloading:  38%|███▊      | 97.9M/260M [00:16<00:24, 6.60MiB/s]    Downloading:  38%|███▊      | 98.6M/260M [00:16<00:24, 6.57MiB/s]    Downloading:  38%|███▊      | 99.3M/260M [00:16<00:23, 6.69MiB/s]    Downloading:  39%|███▊      | 100M/260M [00:17<00:24, 6.57MiB/s]     Downloading:  39%|███▉      | 101M/260M [00:17<00:24, 6.55MiB/s]    Downloading:  39%|███▉      | 101M/260M [00:17<00:25, 6.31MiB/s]    Downloading:  39%|███▉      | 102M/260M [00:17<00:24, 6.48MiB/s]    Downloading:  40%|███▉      | 103M/260M [00:17<00:23, 6.66MiB/s]    Downloading:  40%|███▉      | 103M/260M [00:17<00:22, 6.91MiB/s]    Downloading:  40%|████      | 104M/260M [00:17<00:23, 6.69MiB/s]    Downloading:  40%|████      | 105M/260M [00:17<00:21, 7.06MiB/s]    Downloading:  41%|████      | 106M/260M [00:17<00:22, 6.97MiB/s]    Downloading:  41%|████      | 106M/260M [00:18<00:22, 6.84MiB/s]    Downloading:  41%|████      | 107M/260M [00:18<00:22, 6.79MiB/s]    Downloading:  42%|████▏     | 108M/260M [00:18<00:22, 6.71MiB/s]    Downloading:  42%|████▏     | 108M/260M [00:18<00:22, 6.68MiB/s]    Downloading:  42%|████▏     | 109M/260M [00:18<00:23, 6.43MiB/s]    Downloading:  42%|████▏     | 110M/260M [00:18<00:22, 6.69MiB/s]    Downloading:  43%|████▎     | 111M/260M [00:18<00:22, 6.73MiB/s]    Downloading:  43%|████▎     | 111M/260M [00:18<00:23, 6.40MiB/s]    Downloading:  43%|████▎     | 112M/260M [00:18<00:22, 6.56MiB/s]    Downloading:  43%|████▎     | 113M/260M [00:18<00:22, 6.66MiB/s]    Downloading:  44%|████▎     | 113M/260M [00:19<00:22, 6.59MiB/s]    Downloading:  44%|████▍     | 114M/260M [00:19<00:20, 6.93MiB/s]    Downloading:  44%|████▍     | 115M/260M [00:19<00:22, 6.58MiB/s]    Downloading:  44%|████▍     | 115M/260M [00:19<00:30, 4.79MiB/s]    Downloading:  45%|████▍     | 116M/260M [00:19<00:31, 4.63MiB/s]    Downloading:  45%|████▍     | 117M/260M [00:19<00:28, 5.09MiB/s]    Downloading:  45%|████▌     | 117M/260M [00:19<00:25, 5.58MiB/s]    Downloading:  45%|████▌     | 118M/260M [00:19<00:24, 5.86MiB/s]    Downloading:  46%|████▌     | 119M/260M [00:20<00:24, 5.68MiB/s]    Downloading:  46%|████▌     | 119M/260M [00:20<00:23, 5.90MiB/s]    Downloading:  46%|████▌     | 120M/260M [00:20<00:22, 6.29MiB/s]    Downloading:  46%|████▋     | 121M/260M [00:20<00:21, 6.37MiB/s]    Downloading:  47%|████▋     | 121M/260M [00:20<00:21, 6.33MiB/s]    Downloading:  47%|████▋     | 122M/260M [00:20<00:21, 6.26MiB/s]    Downloading:  47%|████▋     | 123M/260M [00:20<00:21, 6.26MiB/s]    Downloading:  48%|████▊     | 123M/260M [00:20<00:20, 6.61MiB/s]    Downloading:  48%|████▊     | 124M/260M [00:20<00:21, 6.44MiB/s]    Downloading:  48%|████▊     | 125M/260M [00:21<00:20, 6.46MiB/s]    Downloading:  48%|████▊     | 125M/260M [00:21<00:20, 6.66MiB/s]    Downloading:  49%|████▊     | 126M/260M [00:21<00:19, 6.72MiB/s]    Downloading:  49%|████▉     | 127M/260M [00:21<00:19, 6.95MiB/s]    Downloading:  49%|████▉     | 128M/260M [00:21<00:18, 7.14MiB/s]    Downloading:  50%|████▉     | 129M/260M [00:21<00:17, 7.49MiB/s]    Downloading:  50%|████▉     | 129M/260M [00:21<00:17, 7.40MiB/s]    Downloading:  50%|█████     | 130M/260M [00:21<00:19, 6.78MiB/s]    Downloading:  50%|█████     | 131M/260M [00:21<00:18, 6.83MiB/s]    Downloading:  51%|█████     | 131M/260M [00:21<00:18, 6.99MiB/s]    Downloading:  51%|█████     | 132M/260M [00:22<00:19, 6.58MiB/s]    Downloading:  51%|█████     | 133M/260M [00:22<00:21, 5.76MiB/s]    Downloading:  51%|█████▏    | 134M/260M [00:22<00:21, 5.91MiB/s]    Downloading:  52%|█████▏    | 134M/260M [00:22<00:19, 6.29MiB/s]    Downloading:  52%|█████▏    | 135M/260M [00:22<00:19, 6.51MiB/s]    Downloading:  52%|█████▏    | 136M/260M [00:22<00:18, 6.58MiB/s]    Downloading:  53%|█████▎    | 136M/260M [00:22<00:18, 6.66MiB/s]    Downloading:  53%|█████▎    | 137M/260M [00:22<00:18, 6.66MiB/s]    Downloading:  53%|█████▎    | 138M/260M [00:22<00:17, 6.85MiB/s]    Downloading:  53%|█████▎    | 139M/260M [00:23<00:17, 6.84MiB/s]    Downloading:  54%|█████▎    | 139M/260M [00:23<00:18, 6.61MiB/s]    Downloading:  54%|█████▍    | 140M/260M [00:23<00:17, 6.83MiB/s]    Downloading:  54%|█████▍    | 141M/260M [00:23<00:17, 6.80MiB/s]    Downloading:  54%|█████▍    | 141M/260M [00:23<00:17, 6.89MiB/s]    Downloading:  55%|█████▍    | 142M/260M [00:23<00:16, 6.96MiB/s]    Downloading:  55%|█████▌    | 143M/260M [00:23<00:16, 6.87MiB/s]    Downloading:  55%|█████▌    | 144M/260M [00:23<00:16, 6.85MiB/s]    Downloading:  56%|█████▌    | 144M/260M [00:23<00:16, 7.04MiB/s]    Downloading:  56%|█████▌    | 145M/260M [00:23<00:16, 7.12MiB/s]    Downloading:  56%|█████▌    | 146M/260M [00:24<00:15, 7.42MiB/s]    Downloading:  56%|█████▋    | 147M/260M [00:24<00:15, 7.33MiB/s]    Downloading:  57%|█████▋    | 147M/260M [00:24<00:15, 7.41MiB/s]    Downloading:  57%|█████▋    | 148M/260M [00:24<00:15, 7.23MiB/s]    Downloading:  57%|█████▋    | 149M/260M [00:24<00:15, 7.19MiB/s]    Downloading:  58%|█████▊    | 150M/260M [00:24<00:15, 6.94MiB/s]    Downloading:  58%|█████▊    | 150M/260M [00:24<00:16, 6.77MiB/s]    Downloading:  58%|█████▊    | 151M/260M [00:24<00:17, 6.37MiB/s]    Downloading:  58%|█████▊    | 152M/260M [00:24<00:17, 6.13MiB/s]    Downloading:  59%|█████▊    | 152M/260M [00:25<00:18, 5.85MiB/s]    Downloading:  59%|█████▉    | 153M/260M [00:25<00:17, 6.15MiB/s]    Downloading:  59%|█████▉    | 154M/260M [00:25<00:16, 6.41MiB/s]    Downloading:  59%|█████▉    | 154M/260M [00:25<00:15, 6.77MiB/s]    Downloading:  60%|█████▉    | 155M/260M [00:25<00:15, 6.68MiB/s]    Downloading:  60%|██████    | 156M/260M [00:25<00:16, 6.20MiB/s]    Downloading:  60%|██████    | 156M/260M [00:25<00:16, 6.35MiB/s]    Downloading:  61%|██████    | 157M/260M [00:25<00:15, 6.50MiB/s]    Downloading:  61%|██████    | 158M/260M [00:25<00:15, 6.56MiB/s]    Downloading:  61%|██████    | 159M/260M [00:26<00:15, 6.62MiB/s]    Downloading:  61%|██████▏   | 159M/260M [00:26<00:15, 6.45MiB/s]    Downloading:  62%|██████▏   | 160M/260M [00:26<00:16, 6.12MiB/s]    Downloading:  62%|██████▏   | 160M/260M [00:26<00:15, 6.26MiB/s]    Downloading:  62%|██████▏   | 161M/260M [00:26<00:15, 6.42MiB/s]    Downloading:  62%|██████▏   | 162M/260M [00:26<00:15, 6.38MiB/s]    Downloading:  63%|██████▎   | 162M/260M [00:26<00:15, 6.32MiB/s]    Downloading:  63%|██████▎   | 163M/260M [00:26<00:15, 6.37MiB/s]    Downloading:  63%|██████▎   | 164M/260M [00:26<00:14, 6.72MiB/s]    Downloading:  63%|██████▎   | 165M/260M [00:26<00:13, 6.84MiB/s]    Downloading:  64%|██████▎   | 165M/260M [00:27<00:13, 6.99MiB/s]    Downloading:  64%|██████▍   | 166M/260M [00:27<00:13, 6.89MiB/s]    Downloading:  64%|██████▍   | 167M/260M [00:27<00:13, 6.83MiB/s]    Downloading:  65%|██████▍   | 167M/260M [00:27<00:13, 6.88MiB/s]    Downloading:  65%|██████▍   | 168M/260M [00:27<00:12, 7.18MiB/s]    Downloading:  65%|██████▌   | 169M/260M [00:27<00:12, 6.98MiB/s]    Downloading:  65%|██████▌   | 170M/260M [00:27<00:12, 7.12MiB/s]    Downloading:  66%|██████▌   | 171M/260M [00:27<00:12, 7.15MiB/s]    Downloading:  66%|██████▌   | 171M/260M [00:27<00:11, 7.51MiB/s]    Downloading:  66%|██████▋   | 172M/260M [00:28<00:11, 7.45MiB/s]    Downloading:  67%|██████▋   | 173M/260M [00:28<00:12, 7.10MiB/s]    Downloading:  67%|██████▋   | 174M/260M [00:28<00:11, 7.25MiB/s]    Downloading:  67%|██████▋   | 174M/260M [00:28<00:11, 7.16MiB/s]    Downloading:  67%|██████▋   | 175M/260M [00:28<00:12, 6.92MiB/s]    Downloading:  68%|██████▊   | 176M/260M [00:28<00:12, 6.90MiB/s]    Downloading:  68%|██████▊   | 176M/260M [00:28<00:12, 6.69MiB/s]    Downloading:  68%|██████▊   | 177M/260M [00:28<00:11, 6.97MiB/s]    Downloading:  69%|██████▊   | 178M/260M [00:28<00:11, 7.01MiB/s]    Downloading:  69%|██████▉   | 179M/260M [00:28<00:12, 6.71MiB/s]    Downloading:  69%|██████▉   | 179M/260M [00:29<00:11, 6.80MiB/s]    Downloading:  69%|██████▉   | 180M/260M [00:29<00:11, 6.65MiB/s]    Downloading:  70%|██████▉   | 181M/260M [00:29<00:11, 6.72MiB/s]    Downloading:  70%|██████▉   | 181M/260M [00:29<00:11, 6.71MiB/s]    Downloading:  70%|███████   | 182M/260M [00:29<00:16, 4.72MiB/s]    Downloading:  70%|███████   | 183M/260M [00:29<00:15, 4.82MiB/s]    Downloading:  71%|███████   | 183M/260M [00:29<00:14, 5.27MiB/s]    Downloading:  71%|███████   | 184M/260M [00:29<00:13, 5.64MiB/s]    Downloading:  71%|███████   | 185M/260M [00:30<00:12, 5.92MiB/s]    Downloading:  71%|███████▏  | 185M/260M [00:30<00:11, 6.30MiB/s]    Downloading:  72%|███████▏  | 186M/260M [00:30<00:11, 6.42MiB/s]    Downloading:  72%|███████▏  | 187M/260M [00:30<00:11, 6.49MiB/s]    Downloading:  72%|███████▏  | 187M/260M [00:30<00:11, 6.45MiB/s]    Downloading:  72%|███████▏  | 188M/260M [00:30<00:11, 6.31MiB/s]    Downloading:  73%|███████▎  | 189M/260M [00:30<00:11, 6.11MiB/s]    Downloading:  73%|███████▎  | 189M/260M [00:30<00:11, 5.89MiB/s]    Downloading:  73%|███████▎  | 190M/260M [00:30<00:10, 6.35MiB/s]    Downloading:  73%|███████▎  | 191M/260M [00:30<00:10, 6.29MiB/s]    Downloading:  74%|███████▍  | 191M/260M [00:31<00:10, 6.42MiB/s]    Downloading:  74%|███████▍  | 192M/260M [00:31<00:10, 6.54MiB/s]    Downloading:  74%|███████▍  | 193M/260M [00:31<00:09, 6.68MiB/s]    Downloading:  75%|███████▍  | 194M/260M [00:31<00:10, 6.53MiB/s]    Downloading:  75%|███████▍  | 194M/260M [00:31<00:09, 6.72MiB/s]    Downloading:  75%|███████▌  | 195M/260M [00:31<00:09, 6.75MiB/s]    Downloading:  75%|███████▌  | 196M/260M [00:31<00:09, 6.74MiB/s]    Downloading:  76%|███████▌  | 196M/260M [00:31<00:10, 6.28MiB/s]    Downloading:  76%|███████▌  | 197M/260M [00:31<00:09, 6.38MiB/s]    Downloading:  76%|███████▌  | 198M/260M [00:32<00:09, 6.37MiB/s]    Downloading:  76%|███████▋  | 198M/260M [00:32<00:10, 5.87MiB/s]    Downloading:  77%|███████▋  | 199M/260M [00:32<00:11, 5.34MiB/s]    Downloading:  77%|███████▋  | 199M/260M [00:32<00:11, 5.12MiB/s]    Downloading:  77%|███████▋  | 200M/260M [00:32<00:11, 5.03MiB/s]    Downloading:  77%|███████▋  | 200M/260M [00:32<00:11, 5.16MiB/s]    Downloading:  77%|███████▋  | 201M/260M [00:32<00:10, 5.60MiB/s]    Downloading:  78%|███████▊  | 202M/260M [00:32<00:10, 5.68MiB/s]    Downloading:  78%|███████▊  | 202M/260M [00:32<00:09, 5.80MiB/s]    Downloading:  78%|███████▊  | 203M/260M [00:33<00:09, 5.87MiB/s]    Downloading:  78%|███████▊  | 204M/260M [00:33<00:08, 6.48MiB/s]    Downloading:  79%|███████▉  | 205M/260M [00:33<00:08, 6.63MiB/s]    Downloading:  79%|███████▉  | 205M/260M [00:33<00:08, 6.28MiB/s]    Downloading:  79%|███████▉  | 206M/260M [00:33<00:08, 6.27MiB/s]    Downloading:  80%|███████▉  | 206M/260M [00:33<00:08, 6.26MiB/s]    Downloading:  80%|███████▉  | 207M/260M [00:33<00:07, 6.71MiB/s]    Downloading:  80%|████████  | 208M/260M [00:33<00:07, 6.53MiB/s]    Downloading:  80%|████████  | 209M/260M [00:33<00:08, 6.24MiB/s]    Downloading:  81%|████████  | 209M/260M [00:34<00:08, 6.00MiB/s]    Downloading:  81%|████████  | 210M/260M [00:34<00:08, 5.94MiB/s]    Downloading:  81%|████████  | 210M/260M [00:34<00:08, 5.83MiB/s]    Downloading:  81%|████████▏ | 211M/260M [00:34<00:08, 5.68MiB/s]    Downloading:  82%|████████▏ | 212M/260M [00:34<00:08, 5.91MiB/s]    Downloading:  82%|████████▏ | 212M/260M [00:34<00:07, 6.00MiB/s]    Downloading:  82%|████████▏ | 213M/260M [00:34<00:07, 5.91MiB/s]    Downloading:  82%|████████▏ | 214M/260M [00:34<00:07, 5.88MiB/s]    Downloading:  82%|████████▏ | 214M/260M [00:34<00:07, 5.97MiB/s]    Downloading:  83%|████████▎ | 215M/260M [00:34<00:07, 6.22MiB/s]    Downloading:  83%|████████▎ | 215M/260M [00:35<00:07, 6.25MiB/s]    Downloading:  83%|████████▎ | 216M/260M [00:35<00:07, 5.86MiB/s]    Downloading:  83%|████████▎ | 217M/260M [00:35<00:07, 5.99MiB/s]    Downloading:  84%|████████▍ | 217M/260M [00:35<00:06, 6.27MiB/s]    Downloading:  84%|████████▍ | 218M/260M [00:35<00:07, 5.64MiB/s]    Downloading:  84%|████████▍ | 219M/260M [00:35<00:07, 5.67MiB/s]    Downloading:  85%|████████▍ | 219M/260M [00:35<00:06, 6.14MiB/s]    Downloading:  85%|████████▍ | 220M/260M [00:35<00:06, 6.13MiB/s]    Downloading:  85%|████████▌ | 221M/260M [00:35<00:06, 6.36MiB/s]    Downloading:  85%|████████▌ | 221M/260M [00:36<00:05, 6.69MiB/s]    Downloading:  86%|████████▌ | 222M/260M [00:36<00:05, 6.87MiB/s]    Downloading:  86%|████████▌ | 223M/260M [00:36<00:05, 7.11MiB/s]    Downloading:  86%|████████▌ | 224M/260M [00:36<00:05, 6.81MiB/s]    Downloading:  86%|████████▋ | 224M/260M [00:36<00:04, 7.07MiB/s]    Downloading:  87%|████████▋ | 225M/260M [00:36<00:04, 6.94MiB/s]    Downloading:  87%|████████▋ | 226M/260M [00:36<00:05, 6.56MiB/s]    Downloading:  87%|████████▋ | 227M/260M [00:36<00:05, 6.51MiB/s]    Downloading:  88%|████████▊ | 227M/260M [00:36<00:05, 6.39MiB/s]    Downloading:  88%|████████▊ | 228M/260M [00:37<00:05, 6.17MiB/s]    Downloading:  88%|████████▊ | 229M/260M [00:37<00:04, 6.33MiB/s]    Downloading:  88%|████████▊ | 229M/260M [00:37<00:05, 5.79MiB/s]    Downloading:  89%|████████▊ | 230M/260M [00:37<00:05, 5.83MiB/s]    Downloading:  89%|████████▉ | 230M/260M [00:37<00:04, 5.98MiB/s]    Downloading:  89%|████████▉ | 231M/260M [00:37<00:04, 6.03MiB/s]    Downloading:  89%|████████▉ | 232M/260M [00:37<00:04, 6.35MiB/s]    Downloading:  90%|████████▉ | 232M/260M [00:37<00:04, 6.32MiB/s]    Downloading:  90%|████████▉ | 233M/260M [00:37<00:04, 6.37MiB/s]    Downloading:  90%|█████████ | 234M/260M [00:37<00:04, 6.37MiB/s]    Downloading:  90%|█████████ | 235M/260M [00:38<00:03, 6.71MiB/s]    Downloading:  91%|█████████ | 235M/260M [00:38<00:03, 6.58MiB/s]    Downloading:  91%|█████████ | 236M/260M [00:38<00:03, 6.66MiB/s]    Downloading:  91%|█████████ | 237M/260M [00:38<00:03, 6.50MiB/s]    Downloading:  91%|█████████▏| 237M/260M [00:38<00:03, 6.51MiB/s]    Downloading:  92%|█████████▏| 238M/260M [00:38<00:03, 6.45MiB/s]    Downloading:  92%|█████████▏| 239M/260M [00:38<00:03, 6.10MiB/s]    Downloading:  92%|█████████▏| 239M/260M [00:38<00:03, 6.20MiB/s]    Downloading:  92%|█████████▏| 240M/260M [00:38<00:03, 6.41MiB/s]    Downloading:  93%|█████████▎| 241M/260M [00:39<00:02, 6.69MiB/s]    Downloading:  93%|█████████▎| 241M/260M [00:39<00:02, 6.63MiB/s]    Downloading:  93%|█████████▎| 242M/260M [00:39<00:02, 6.62MiB/s]    Downloading:  93%|█████████▎| 243M/260M [00:39<00:02, 6.58MiB/s]    Downloading:  94%|█████████▍| 243M/260M [00:39<00:02, 6.83MiB/s]    Downloading:  94%|█████████▍| 244M/260M [00:39<00:02, 6.68MiB/s]    Downloading:  94%|█████████▍| 245M/260M [00:39<00:02, 6.14MiB/s]    Downloading:  95%|█████████▍| 245M/260M [00:39<00:02, 6.33MiB/s]    Downloading:  95%|█████████▍| 246M/260M [00:39<00:02, 6.40MiB/s]    Downloading:  95%|█████████▌| 247M/260M [00:39<00:01, 6.43MiB/s]    Downloading:  95%|█████████▌| 247M/260M [00:40<00:01, 6.42MiB/s]    Downloading:  96%|█████████▌| 248M/260M [00:40<00:01, 6.21MiB/s]    Downloading:  96%|█████████▌| 249M/260M [00:40<00:01, 6.16MiB/s]    Downloading:  96%|█████████▌| 249M/260M [00:40<00:01, 6.19MiB/s]    Downloading:  96%|█████████▋| 250M/260M [00:40<00:01, 5.30MiB/s]    Downloading:  97%|█████████▋| 250M/260M [00:40<00:01, 4.88MiB/s]    Downloading:  97%|█████████▋| 251M/260M [00:40<00:01, 4.93MiB/s]    Downloading:  97%|█████████▋| 252M/260M [00:40<00:01, 5.24MiB/s]    Downloading:  97%|█████████▋| 252M/260M [00:40<00:01, 5.55MiB/s]    Downloading:  97%|█████████▋| 253M/260M [00:41<00:01, 5.57MiB/s]    Downloading:  98%|█████████▊| 254M/260M [00:41<00:01, 5.90MiB/s]    Downloading:  98%|█████████▊| 254M/260M [00:41<00:00, 6.10MiB/s]    Downloading:  98%|█████████▊| 255M/260M [00:41<00:00, 5.93MiB/s]    Downloading:  98%|█████████▊| 255M/260M [00:41<00:00, 5.71MiB/s]    Downloading:  99%|█████████▊| 256M/260M [00:41<00:00, 5.11MiB/s]    Downloading:  99%|█████████▉| 257M/260M [00:41<00:00, 4.43MiB/s]    Downloading:  99%|█████████▉| 257M/260M [00:41<00:00, 4.57MiB/s]    Downloading:  99%|█████████▉| 258M/260M [00:42<00:00, 4.70MiB/s]    Downloading:  99%|█████████▉| 258M/260M [00:42<00:00, 4.41MiB/s]    Downloading: 100%|█████████▉| 259M/260M [00:42<00:00, 4.32MiB/s]    Downloading: 100%|█████████▉| 259M/260M [00:42<00:00, 4.59MiB/s]    Downloading: 100%|██████████| 260M/260M [00:42<00:00, 6.12MiB/s]
    Extracting:   0%|          | 0/324045 [00:00<?, ?file/s]    Extracting:   1%|          | 3131/324045 [00:00<00:10, 31302.42file/s]    Extracting:   2%|▏         | 6262/324045 [00:00<00:10, 31008.42file/s]    Extracting:   3%|▎         | 9364/324045 [00:00<00:10, 30468.86file/s]    Extracting:   4%|▍         | 12659/324045 [00:00<00:09, 31431.07file/s]    Extracting:   5%|▍         | 16028/324045 [00:00<00:09, 32234.99file/s]    Extracting:   6%|▌         | 19254/324045 [00:00<00:09, 31384.79file/s]    Extracting:   7%|▋         | 22409/324045 [00:00<00:09, 31434.86file/s]    Extracting:   8%|▊         | 25677/324045 [00:00<00:09, 31824.94file/s]    Extracting:   9%|▉         | 28863/324045 [00:00<00:09, 31100.98file/s]    Extracting:  10%|▉         | 32106/324045 [00:01<00:09, 31486.94file/s]    Extracting:  11%|█         | 35259/324045 [00:01<00:09, 30672.01file/s]    Extracting:  12%|█▏        | 38346/324045 [00:01<00:09, 30729.30file/s]    Extracting:  13%|█▎        | 41554/324045 [00:01<00:09, 31128.87file/s]    Extracting:  14%|█▍        | 44671/324045 [00:01<00:09, 30925.90file/s]    Extracting:  15%|█▍        | 47767/324045 [00:01<00:09, 30210.16file/s]    Extracting:  16%|█▌        | 50944/324045 [00:01<00:08, 30665.54file/s]    Extracting:  17%|█▋        | 54016/324045 [00:01<00:08, 30065.11file/s]    Extracting:  18%|█▊        | 57395/324045 [00:01<00:08, 31153.25file/s]    Extracting:  19%|█▊        | 60517/324045 [00:01<00:08, 30892.20file/s]    Extracting:  20%|█▉        | 63842/324045 [00:02<00:08, 31585.26file/s]    Extracting:  21%|██        | 67130/324045 [00:02<00:08, 31966.39file/s]    Extracting:  22%|██▏       | 70345/324045 [00:02<00:07, 32017.90file/s]    Extracting:  23%|██▎       | 73645/324045 [00:02<00:07, 32310.18file/s]    Extracting:  24%|██▎       | 76879/324045 [00:02<00:07, 31269.90file/s]    Extracting:  25%|██▍       | 80015/324045 [00:02<00:07, 30958.60file/s]    Extracting:  26%|██▌       | 83312/324045 [00:02<00:07, 31544.23file/s]    Extracting:  27%|██▋       | 86620/324045 [00:02<00:07, 31993.08file/s]    Extracting:  28%|██▊       | 89825/324045 [00:02<00:07, 31749.26file/s]    Extracting:  29%|██▉       | 93167/324045 [00:02<00:07, 32240.56file/s]    Extracting:  30%|██▉       | 96452/324045 [00:03<00:07, 32420.95file/s]    Extracting:  31%|███       | 99697/324045 [00:03<00:07, 31778.09file/s]    Extracting:  32%|███▏      | 102923/324045 [00:03<00:06, 31917.47file/s]    Extracting:  33%|███▎      | 106230/324045 [00:03<00:06, 32256.27file/s]    Extracting:  34%|███▍      | 109459/324045 [00:03<00:06, 31533.68file/s]    Extracting:  35%|███▍      | 112686/324045 [00:03<00:06, 31748.77file/s]    Extracting:  36%|███▌      | 115865/324045 [00:03<00:06, 31736.34file/s]    Extracting:  37%|███▋      | 119042/324045 [00:03<00:06, 30880.25file/s]    Extracting:  38%|███▊      | 122373/324045 [00:03<00:06, 31587.98file/s]    Extracting:  39%|███▉      | 125632/324045 [00:03<00:06, 31882.03file/s]    Extracting:  40%|███▉      | 128826/324045 [00:04<00:06, 31345.80file/s]    Extracting:  41%|████      | 132196/324045 [00:04<00:05, 32034.13file/s]    Extracting:  42%|████▏     | 135633/324045 [00:04<00:05, 32721.79file/s]    Extracting:  43%|████▎     | 138910/324045 [00:04<00:05, 32275.18file/s]    Extracting:  44%|████▍     | 142142/324045 [00:04<00:05, 32184.40file/s]    Extracting:  45%|████▍     | 145364/324045 [00:04<00:05, 32150.09file/s]    Extracting:  46%|████▌     | 148581/324045 [00:04<00:05, 31555.11file/s]    Extracting:  47%|████▋     | 151915/324045 [00:04<00:05, 32077.90file/s]    Extracting:  48%|████▊     | 155352/324045 [00:04<00:05, 32755.18file/s]    Extracting:  49%|████▉     | 158631/324045 [00:05<00:05, 32182.70file/s]    Extracting:  50%|█████     | 162167/324045 [00:05<00:04, 33114.89file/s]    Extracting:  51%|█████     | 165484/324045 [00:05<00:04, 32656.16file/s]    Extracting:  52%|█████▏    | 168754/324045 [00:05<00:04, 31381.61file/s]    Extracting:  53%|█████▎    | 171927/324045 [00:05<00:04, 30951.53file/s]    Extracting:  54%|█████▍    | 175175/324045 [00:05<00:04, 31388.27file/s]    Extracting:  55%|█████▌    | 178555/324045 [00:05<00:04, 32090.22file/s]    Extracting:  56%|█████▌    | 181772/324045 [00:05<00:04, 31985.04file/s]    Extracting:  57%|█████▋    | 184976/324045 [00:05<00:04, 30699.43file/s]    Extracting:  58%|█████▊    | 188226/324045 [00:05<00:04, 31215.70file/s]    Extracting:  59%|█████▉    | 191359/324045 [00:06<00:04, 31074.70file/s]    Extracting:  60%|██████    | 194867/324045 [00:06<00:04, 32246.52file/s]    Extracting:  61%|██████    | 198100/324045 [00:06<00:03, 32172.28file/s]    Extracting:  62%|██████▏   | 201323/324045 [00:06<00:03, 31212.82file/s]    Extracting:  63%|██████▎   | 204589/324045 [00:06<00:03, 31630.33file/s]    Extracting:  64%|██████▍   | 207944/324045 [00:06<00:03, 32192.45file/s]    Extracting:  65%|██████▌   | 211232/324045 [00:06<00:03, 31960.15file/s]    Extracting:  66%|██████▌   | 214478/324045 [00:06<00:03, 32104.47file/s]    Extracting:  67%|██████▋   | 217700/324045 [00:06<00:03, 32138.01file/s]    Extracting:  68%|██████▊   | 220947/324045 [00:06<00:03, 32233.32file/s]    Extracting:  69%|██████▉   | 224173/324045 [00:07<00:03, 31388.57file/s]    Extracting:  70%|███████   | 227528/324045 [00:07<00:03, 32019.35file/s]    Extracting:  71%|███████   | 230736/324045 [00:07<00:02, 31599.35file/s]    Extracting:  72%|███████▏  | 234105/324045 [00:07<00:02, 32210.69file/s]    Extracting:  73%|███████▎  | 237401/324045 [00:07<00:02, 32431.78file/s]    Extracting:  74%|███████▍  | 240648/324045 [00:07<00:02, 31770.00file/s]    Extracting:  75%|███████▌  | 244072/324045 [00:07<00:02, 32491.62file/s]    Extracting:  76%|███████▋  | 247327/324045 [00:07<00:02, 32405.55file/s]    Extracting:  77%|███████▋  | 250571/324045 [00:07<00:02, 31504.02file/s]    Extracting:  78%|███████▊  | 253729/324045 [00:08<00:02, 31486.06file/s]    Extracting:  79%|███████▉  | 256883/324045 [00:08<00:02, 30404.31file/s]    Extracting:  80%|████████  | 260138/324045 [00:08<00:02, 31009.79file/s]    Extracting:  81%|████████  | 263249/324045 [00:08<00:01, 30888.81file/s]    Extracting:  82%|████████▏ | 266552/324045 [00:08<00:01, 31514.63file/s]    Extracting:  83%|████████▎ | 269710/324045 [00:08<00:01, 31083.69file/s]    Extracting:  84%|████████▍ | 272824/324045 [00:08<00:01, 30714.89file/s]    Extracting:  85%|████████▌ | 276174/324045 [00:08<00:01, 31527.33file/s]    Extracting:  86%|████████▌ | 279332/324045 [00:08<00:01, 31512.54file/s]    Extracting:  87%|████████▋ | 282487/324045 [00:08<00:01, 30697.91file/s]    Extracting:  88%|████████▊ | 285687/324045 [00:09<00:01, 31076.26file/s]    Extracting:  89%|████████▉ | 288800/324045 [00:09<00:01, 30704.99file/s]    Extracting:  90%|█████████ | 292183/324045 [00:09<00:01, 31621.04file/s]    Extracting:  91%|█████████ | 295351/324045 [00:09<00:00, 31508.48file/s]    Extracting:  92%|█████████▏| 298506/324045 [00:09<00:00, 30382.02file/s]    Extracting:  93%|█████████▎| 301681/324045 [00:09<00:00, 30775.99file/s]    Extracting:  94%|█████████▍| 304767/324045 [00:09<00:00, 30273.39file/s]    Extracting:  95%|█████████▌| 308043/324045 [00:09<00:00, 30995.70file/s]    Extracting:  96%|█████████▌| 311317/324045 [00:09<00:00, 31507.72file/s]    Extracting:  97%|█████████▋| 314474/324045 [00:09<00:00, 30836.40file/s]    Extracting:  98%|█████████▊| 317726/324045 [00:10<00:00, 31328.20file/s]    Extracting:  99%|█████████▉| 320865/324045 [00:10<00:00, 30658.17file/s]    Extracting: 100%|██████████| 324045/324045 [00:10<00:00, 31524.32file/s]
    Extracted to /home/kemove/yyz/av-gihub/tutorials/mmwave_PC_source/
    Using default config file.
    using dataset: MetaFi DATA
    S02 ['A02', 'A03', 'A04', 'A05', 'A14', 'A18', 'A19', 'A21', 'A22', 'A23', 'A27']
    S03 ['A02', 'A03', 'A04', 'A05', 'A13', 'A14', 'A17', 'A18', 'A19', 'A20', 'A21', 'A22']
    S05 ['A02', 'A03', 'A04', 'A05', 'A13', 'A14', 'A17', 'A19', 'A20', 'A21', 'A22', 'A23', 'A27']
    S06 ['A02', 'A04', 'A05', 'A14', 'A17', 'A18', 'A20', 'A21', 'A22', 'A23', 'A27']
    S08 ['A02', 'A03', 'A05', 'A13', 'A14', 'A17', 'A18', 'A19', 'A20', 'A21', 'A23', 'A27']
    S09 ['A02', 'A17', 'A18', 'A20', 'A21', 'A22', 'A23', 'A27']
    S11 ['A02', 'A03', 'A04', 'A13', 'A14', 'A18', 'A20', 'A21', 'A22', 'A23']
    S12 ['A02', 'A05', 'A13', 'A14', 'A17', 'A18', 'A19', 'A20', 'A21', 'A22', 'A27']
    S13 ['A02', 'A04', 'A05', 'A13', 'A14', 'A17', 'A18', 'A19', 'A20', 'A21', 'A27']
    S14 ['A02', 'A03', 'A04', 'A05', 'A13', 'A14', 'A17', 'A18', 'A19', 'A20', 'A21', 'A23']
    S15 ['A02', 'A03', 'A04', 'A05', 'A13', 'A17', 'A19', 'A20', 'A21', 'A22', 'A23', 'A27']
    S16 ['A02', 'A05', 'A13', 'A14', 'A18', 'A19', 'A20', 'A22', 'A23', 'A27']
    S17 ['A02', 'A03', 'A04', 'A05', 'A13', 'A18', 'A19', 'A20', 'A21', 'A23']
    S18 ['A02', 'A03', 'A04', 'A05', 'A13', 'A14', 'A17', 'A18', 'A19', 'A20', 'A21', 'A23', 'A27']
    S19 ['A02', 'A03', 'A05', 'A13', 'A14', 'A17', 'A18', 'A19', 'A20', 'A21', 'A22', 'A23']
    S21 ['A02', 'A03', 'A04', 'A05', 'A13', 'A14', 'A18', 'A20', 'A21', 'A22', 'A23', 'A27']
    S23 ['A02', 'A03', 'A05', 'A13', 'A14', 'A17', 'A18', 'A19', 'A21', 'A22', 'A27']
    S25 ['A02', 'A03', 'A04', 'A13', 'A14', 'A17', 'A18', 'A19', 'A20', 'A21', 'A22', 'A23', 'A27']
    S26 ['A02', 'A03', 'A04', 'A05', 'A13', 'A14', 'A19', 'A20', 'A23']
    S27 ['A02', 'A03', 'A04', 'A05', 'A13', 'A14', 'A17', 'A18', 'A20', 'A21', 'A22', 'A23']
    S28 ['A02', 'A03', 'A04', 'A05', 'A13', 'A17', 'A18', 'A19', 'A21', 'A27']
    S29 ['A02', 'A03', 'A04', 'A05', 'A13', 'A14', 'A17', 'A19', 'A22', 'A23', 'A27']
    S30 ['A02', 'A03', 'A04', 'A05', 'A13', 'A14', 'A17', 'A18', 'A19', 'A22', 'A23', 'A27']
    S31 ['A02', 'A03', 'A04', 'A05', 'A17', 'A18', 'A19', 'A20', 'A21', 'A22', 'A23', 'A27']
    S32 ['A02', 'A03', 'A05', 'A13', 'A14', 'A17', 'A18', 'A19', 'A20', 'A21', 'A22', 'A23', 'A27']
    S33 ['A02', 'A03', 'A04', 'A05', 'A13', 'A14', 'A17', 'A18', 'A19', 'A20', 'A21', 'A22', 'A23', 'A27']
    S34 ['A02', 'A03', 'A04', 'A13', 'A14', 'A17', 'A18', 'A19', 'A23', 'A27']
    S35 ['A02', 'A03', 'A04', 'A05', 'A13', 'A14', 'A17', 'A18', 'A19', 'A20', 'A21', 'A22', 'A23']
    S36 ['A02', 'A03', 'A05', 'A13', 'A19', 'A20', 'A21', 'A22', 'A27']
    S37 ['A02', 'A03', 'A04', 'A05', 'A13', 'A14', 'A17', 'A18', 'A19', 'A20', 'A22', 'A23', 'A27']
    S38 ['A02', 'A04', 'A05', 'A13', 'A17', 'A18', 'A19', 'A20', 'A21', 'A22', 'A23', 'A27']
    S40 ['A02', 'A03', 'A04', 'A05', 'A13', 'A14', 'A17', 'A18', 'A19', 'A21', 'A22', 'A23', 'A27']
    S04 ['A03', 'A04', 'A14', 'A17', 'A20', 'A21', 'A22', 'A27']
    S07 ['A03', 'A04', 'A05', 'A13', 'A14', 'A17', 'A18', 'A19', 'A20', 'A21', 'A22', 'A27']
    S20 ['A03', 'A04', 'A13', 'A14', 'A17', 'A20', 'A21', 'A22', 'A23', 'A27']
    S22 ['A03', 'A04', 'A13', 'A14', 'A17', 'A18', 'A20', 'A21', 'A22', 'A23', 'A27']
    S24 ['A03', 'A04', 'A05', 'A14', 'A17', 'A19', 'A20', 'A21', 'A22', 'A23', 'A27']
    S39 ['A03', 'A04', 'A05', 'A13', 'A14', 'A17', 'A18', 'A19', 'A21', 'A22', 'A23', 'A27']
    S01 ['A04', 'A14', 'A17', 'A18', 'A19', 'A20', 'A22', 'A23', 'A27']
    S10 ['A04', 'A05', 'A18', 'A19', 'A20', 'A22', 'A23', 'A27']
    S01 ['A02', 'A03', 'A05', 'A13', 'A21']
    S04 ['A02', 'A05', 'A13', 'A18', 'A19', 'A23']
    S07 ['A02', 'A23']
    S10 ['A02', 'A03', 'A13', 'A14', 'A17', 'A21']
    S20 ['A02', 'A05', 'A18', 'A19']
    S22 ['A02', 'A05', 'A19']
    S24 ['A02', 'A13', 'A18']
    S39 ['A02', 'A20']
    S06 ['A03', 'A13', 'A19']
    S09 ['A03', 'A04', 'A05', 'A13', 'A14', 'A19']
    S12 ['A03', 'A04', 'A23']
    S13 ['A03', 'A22', 'A23']
    S16 ['A03', 'A04', 'A17', 'A21']
    S38 ['A03', 'A14']
    S08 ['A04', 'A22']
    S19 ['A04', 'A27']
    S23 ['A04', 'A20', 'A23']
    S32 ['A04']
    S36 ['A04', 'A14', 'A17', 'A18', 'A23']
    S11 ['A05', 'A17', 'A19', 'A27']
    S25 ['A05']
    S34 ['A05', 'A20', 'A21', 'A22']
    S02 ['A13', 'A17', 'A20']
    S31 ['A13', 'A14']
    S15 ['A14', 'A18']
    S17 ['A14', 'A17', 'A22', 'A27']
    S28 ['A14', 'A20', 'A22', 'A23']
    S21 ['A17', 'A19']
    S26 ['A17', 'A18', 'A21', 'A22', 'A27']
    S05 ['A18']
    S29 ['A18', 'A20', 'A21']
    S27 ['A19', 'A27']
    S30 ['A20', 'A21']
    S40 ['A20']
    S37 ['A21']
    S14 ['A22', 'A27']
    S18 ['A22']
    S03 ['A23', 'A27']
    S35 ['A27']




.. GENERATED FROM PYTHON SOURCE LINES 40-42

Visualize the PC data
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 44-45

In[6]:

.. GENERATED FROM PYTHON SOURCE LINES 45-54

.. code-block:: Python


    from matplotlib import pyplot as plt
    from pysensing.mmwave.PC.tutorial.plot import plot_3d_graph
    # Example of the samples in the dataset
    index = 10  # Randomly select an index
    pc,pose = train_dataset.__getitem__(index)
    print(pc.shape, type(pose))
    plot_3d_graph(pose, pc[0])




.. image-sg:: /mmwave_PC/images/sphx_glr_mmwave_PC_hpe_tutorial_001.png
   :alt: mmwave PC hpe tutorial
   :srcset: /mmwave_PC/images/sphx_glr_mmwave_PC_hpe_tutorial_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    (5, 150, 5) <class 'torch.Tensor'>




.. GENERATED FROM PYTHON SOURCE LINES 55-57

Create model 
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 59-62

mmFi utilizes PointTransformer model as a baseline hpe method. From model.hpe, we can import 
desired hpe model designed for mmWave PC. The model parameter for PointTransformer reimplemented 
for mmFi is as follows:

.. GENERATED FROM PYTHON SOURCE LINES 64-65

In[7]:

.. GENERATED FROM PYTHON SOURCE LINES 65-75

.. code-block:: Python


    from pysensing.mmwave.PC.model.hpe import PointTransformerReg
    model = PointTransformerReg(
                        input_dim = 5,
                        nblocks = 5,
                        n_p = 17
                    )
    print(model)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    PointTransformerReg(
      (backbone): Backbone(
        (fc1): Sequential(
          (0): Linear(in_features=5, out_features=32, bias=True)
          (1): ReLU()
          (2): Linear(in_features=32, out_features=32, bias=True)
        )
        (transformer1): TransformerBlock(
          (fc1): Linear(in_features=32, out_features=128, bias=True)
          (fc2): Linear(in_features=128, out_features=32, bias=True)
          (fc_delta): Sequential(
            (0): Linear(in_features=3, out_features=128, bias=True)
            (1): ReLU()
            (2): Linear(in_features=128, out_features=128, bias=True)
          )
          (fc_gamma): Sequential(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): ReLU()
            (2): Linear(in_features=128, out_features=128, bias=True)
          )
          (w_qs): Linear(in_features=128, out_features=128, bias=False)
          (w_ks): Linear(in_features=128, out_features=128, bias=False)
          (w_vs): Linear(in_features=128, out_features=128, bias=False)
        )
        (transition_downs): ModuleList(
          (0): TransitionDown(
            (conv1): Sequential(
              (0): Conv1d(35, 64, kernel_size=(1,), stride=(1,))
              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU()
            )
            (conv2): Sequential(
              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))
              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU()
            )
          )
          (1): TransitionDown(
            (conv1): Sequential(
              (0): Conv1d(67, 128, kernel_size=(1,), stride=(1,))
              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU()
            )
            (conv2): Sequential(
              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU()
            )
          )
          (2): TransitionDown(
            (conv1): Sequential(
              (0): Conv1d(131, 256, kernel_size=(1,), stride=(1,))
              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU()
            )
            (conv2): Sequential(
              (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU()
            )
          )
          (3): TransitionDown(
            (conv1): Sequential(
              (0): Conv1d(259, 512, kernel_size=(1,), stride=(1,))
              (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU()
            )
            (conv2): Sequential(
              (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
              (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU()
            )
          )
        )
        (transformers): ModuleList(
          (0): TransformerBlock(
            (fc1): Linear(in_features=64, out_features=128, bias=True)
            (fc2): Linear(in_features=128, out_features=64, bias=True)
            (fc_delta): Sequential(
              (0): Linear(in_features=3, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=128, bias=True)
            )
            (fc_gamma): Sequential(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=128, bias=True)
            )
            (w_qs): Linear(in_features=128, out_features=128, bias=False)
            (w_ks): Linear(in_features=128, out_features=128, bias=False)
            (w_vs): Linear(in_features=128, out_features=128, bias=False)
          )
          (1): TransformerBlock(
            (fc1): Linear(in_features=128, out_features=128, bias=True)
            (fc2): Linear(in_features=128, out_features=128, bias=True)
            (fc_delta): Sequential(
              (0): Linear(in_features=3, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=128, bias=True)
            )
            (fc_gamma): Sequential(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=128, bias=True)
            )
            (w_qs): Linear(in_features=128, out_features=128, bias=False)
            (w_ks): Linear(in_features=128, out_features=128, bias=False)
            (w_vs): Linear(in_features=128, out_features=128, bias=False)
          )
          (2): TransformerBlock(
            (fc1): Linear(in_features=256, out_features=128, bias=True)
            (fc2): Linear(in_features=128, out_features=256, bias=True)
            (fc_delta): Sequential(
              (0): Linear(in_features=3, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=128, bias=True)
            )
            (fc_gamma): Sequential(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=128, bias=True)
            )
            (w_qs): Linear(in_features=128, out_features=128, bias=False)
            (w_ks): Linear(in_features=128, out_features=128, bias=False)
            (w_vs): Linear(in_features=128, out_features=128, bias=False)
          )
          (3): TransformerBlock(
            (fc1): Linear(in_features=512, out_features=128, bias=True)
            (fc2): Linear(in_features=128, out_features=512, bias=True)
            (fc_delta): Sequential(
              (0): Linear(in_features=3, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=128, bias=True)
            )
            (fc_gamma): Sequential(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=128, bias=True)
            )
            (w_qs): Linear(in_features=128, out_features=128, bias=False)
            (w_ks): Linear(in_features=128, out_features=128, bias=False)
            (w_vs): Linear(in_features=128, out_features=128, bias=False)
          )
        )
      )
      (transformer): Transformer(
        (layers): ModuleList(
          (0-4): 5 x ModuleList(
            (0): Residual(
              (fn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Attention(
                  (to_k): Linear(in_features=512, out_features=512, bias=False)
                  (to_v): Linear(in_features=512, out_features=512, bias=False)
                  (to_q): Linear(in_features=512, out_features=512, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=512, out_features=512, bias=True)
                    (1): GELU(approximate='none')
                    (2): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): Residual(
              (fn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=512, out_features=256, bias=True)
                    (1): GELU(approximate='none')
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=256, out_features=512, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (fc2): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Dropout(p=0.0, inplace=False)
        (2): ReLU()
        (3): Linear(in_features=256, out_features=32, bias=True)
      )
      (fc3): Sequential(
        (0): ReLU()
        (1): Linear(in_features=32, out_features=64, bias=True)
        (2): Dropout(p=0.0, inplace=False)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=3, bias=True)
      )
    )




.. GENERATED FROM PYTHON SOURCE LINES 76-77

A shortcut for loading the hpe model to avoid the tedious hyper-parameter setting.

.. GENERATED FROM PYTHON SOURCE LINES 80-81

In[8]:

.. GENERATED FROM PYTHON SOURCE LINES 81-88

.. code-block:: Python



    from pysensing.mmwave.PC.model.hpe import load_hpe_model
    model = load_hpe_model("MetaFi", "PointTransformer")
    print(model)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    PointTransformerReg(
      (backbone): Backbone(
        (fc1): Sequential(
          (0): Linear(in_features=5, out_features=32, bias=True)
          (1): ReLU()
          (2): Linear(in_features=32, out_features=32, bias=True)
        )
        (transformer1): TransformerBlock(
          (fc1): Linear(in_features=32, out_features=128, bias=True)
          (fc2): Linear(in_features=128, out_features=32, bias=True)
          (fc_delta): Sequential(
            (0): Linear(in_features=3, out_features=128, bias=True)
            (1): ReLU()
            (2): Linear(in_features=128, out_features=128, bias=True)
          )
          (fc_gamma): Sequential(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): ReLU()
            (2): Linear(in_features=128, out_features=128, bias=True)
          )
          (w_qs): Linear(in_features=128, out_features=128, bias=False)
          (w_ks): Linear(in_features=128, out_features=128, bias=False)
          (w_vs): Linear(in_features=128, out_features=128, bias=False)
        )
        (transition_downs): ModuleList(
          (0): TransitionDown(
            (conv1): Sequential(
              (0): Conv1d(35, 64, kernel_size=(1,), stride=(1,))
              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU()
            )
            (conv2): Sequential(
              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))
              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU()
            )
          )
          (1): TransitionDown(
            (conv1): Sequential(
              (0): Conv1d(67, 128, kernel_size=(1,), stride=(1,))
              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU()
            )
            (conv2): Sequential(
              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU()
            )
          )
          (2): TransitionDown(
            (conv1): Sequential(
              (0): Conv1d(131, 256, kernel_size=(1,), stride=(1,))
              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU()
            )
            (conv2): Sequential(
              (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU()
            )
          )
          (3): TransitionDown(
            (conv1): Sequential(
              (0): Conv1d(259, 512, kernel_size=(1,), stride=(1,))
              (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU()
            )
            (conv2): Sequential(
              (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
              (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU()
            )
          )
        )
        (transformers): ModuleList(
          (0): TransformerBlock(
            (fc1): Linear(in_features=64, out_features=128, bias=True)
            (fc2): Linear(in_features=128, out_features=64, bias=True)
            (fc_delta): Sequential(
              (0): Linear(in_features=3, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=128, bias=True)
            )
            (fc_gamma): Sequential(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=128, bias=True)
            )
            (w_qs): Linear(in_features=128, out_features=128, bias=False)
            (w_ks): Linear(in_features=128, out_features=128, bias=False)
            (w_vs): Linear(in_features=128, out_features=128, bias=False)
          )
          (1): TransformerBlock(
            (fc1): Linear(in_features=128, out_features=128, bias=True)
            (fc2): Linear(in_features=128, out_features=128, bias=True)
            (fc_delta): Sequential(
              (0): Linear(in_features=3, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=128, bias=True)
            )
            (fc_gamma): Sequential(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=128, bias=True)
            )
            (w_qs): Linear(in_features=128, out_features=128, bias=False)
            (w_ks): Linear(in_features=128, out_features=128, bias=False)
            (w_vs): Linear(in_features=128, out_features=128, bias=False)
          )
          (2): TransformerBlock(
            (fc1): Linear(in_features=256, out_features=128, bias=True)
            (fc2): Linear(in_features=128, out_features=256, bias=True)
            (fc_delta): Sequential(
              (0): Linear(in_features=3, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=128, bias=True)
            )
            (fc_gamma): Sequential(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=128, bias=True)
            )
            (w_qs): Linear(in_features=128, out_features=128, bias=False)
            (w_ks): Linear(in_features=128, out_features=128, bias=False)
            (w_vs): Linear(in_features=128, out_features=128, bias=False)
          )
          (3): TransformerBlock(
            (fc1): Linear(in_features=512, out_features=128, bias=True)
            (fc2): Linear(in_features=128, out_features=512, bias=True)
            (fc_delta): Sequential(
              (0): Linear(in_features=3, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=128, bias=True)
            )
            (fc_gamma): Sequential(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=128, bias=True)
            )
            (w_qs): Linear(in_features=128, out_features=128, bias=False)
            (w_ks): Linear(in_features=128, out_features=128, bias=False)
            (w_vs): Linear(in_features=128, out_features=128, bias=False)
          )
        )
      )
      (transformer): Transformer(
        (layers): ModuleList(
          (0-4): 5 x ModuleList(
            (0): Residual(
              (fn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Attention(
                  (to_k): Linear(in_features=512, out_features=512, bias=False)
                  (to_v): Linear(in_features=512, out_features=512, bias=False)
                  (to_q): Linear(in_features=512, out_features=512, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=512, out_features=512, bias=True)
                    (1): GELU(approximate='none')
                    (2): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): Residual(
              (fn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=512, out_features=256, bias=True)
                    (1): GELU(approximate='none')
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=256, out_features=512, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (fc2): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Dropout(p=0.0, inplace=False)
        (2): ReLU()
        (3): Linear(in_features=256, out_features=32, bias=True)
      )
      (fc3): Sequential(
        (0): ReLU()
        (1): Linear(in_features=32, out_features=64, bias=True)
        (2): Dropout(p=0.0, inplace=False)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=3, bias=True)
      )
    )




.. GENERATED FROM PYTHON SOURCE LINES 89-91

Model Train
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 93-96

pysensing library support quick training of model with the following steps. The training interface 
incorporates pytorch loss functions, optimizers and dataloaders to facilate training. 
An example is provided for how to define the aforemetioned terms.

.. GENERATED FROM PYTHON SOURCE LINES 99-100

In[11]:

.. GENERATED FROM PYTHON SOURCE LINES 100-116

.. code-block:: Python



    # Create pytorch dataloaders
    train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=16, num_workers=16)
    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=16)

    # Define pytorch loss function as criterion 
    criterion = nn.CrossEntropyLoss()

    # Define pytorch optimizer for training
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

    # GPU acceleration with cuda
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")









.. GENERATED FROM PYTHON SOURCE LINES 117-118

A quick training using hpe_train. The resulted model parameters will be saved into "train_{num_epochs}.pth".

.. GENERATED FROM PYTHON SOURCE LINES 120-121

In[12]:

.. GENERATED FROM PYTHON SOURCE LINES 121-128

.. code-block:: Python



    # Pysensing training interface
    from pysensing.mmwave.PC.inference.hpe import hpe_train
    # hpe_train(model, train_loader, num_epochs=1, optimizer=optimizer, criterion=criterion, device=device)









.. GENERATED FROM PYTHON SOURCE LINES 129-131

Model inference
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 133-135

Load the pretrained model, e.g. from  https://pysensing.oss-ap-southeast-1.aliyuncs.com/pretrain/mmwave_pc/hpe/MetaFi_PointTransformer.pth 
, and perform human pose estimation!

.. GENERATED FROM PYTHON SOURCE LINES 137-138

In[13]:

.. GENERATED FROM PYTHON SOURCE LINES 138-145

.. code-block:: Python


    # load pretrained model
    from pysensing.mmwave.PC.inference import load_pretrain
    model = load_pretrain(model, "MetaFi", "PointTransformer").to(device)
    model.eval()






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Use pretrained model!

    PointTransformerReg(
      (backbone): Backbone(
        (fc1): Sequential(
          (0): Linear(in_features=5, out_features=32, bias=True)
          (1): ReLU()
          (2): Linear(in_features=32, out_features=32, bias=True)
        )
        (transformer1): TransformerBlock(
          (fc1): Linear(in_features=32, out_features=128, bias=True)
          (fc2): Linear(in_features=128, out_features=32, bias=True)
          (fc_delta): Sequential(
            (0): Linear(in_features=3, out_features=128, bias=True)
            (1): ReLU()
            (2): Linear(in_features=128, out_features=128, bias=True)
          )
          (fc_gamma): Sequential(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): ReLU()
            (2): Linear(in_features=128, out_features=128, bias=True)
          )
          (w_qs): Linear(in_features=128, out_features=128, bias=False)
          (w_ks): Linear(in_features=128, out_features=128, bias=False)
          (w_vs): Linear(in_features=128, out_features=128, bias=False)
        )
        (transition_downs): ModuleList(
          (0): TransitionDown(
            (conv1): Sequential(
              (0): Conv1d(35, 64, kernel_size=(1,), stride=(1,))
              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU()
            )
            (conv2): Sequential(
              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))
              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU()
            )
          )
          (1): TransitionDown(
            (conv1): Sequential(
              (0): Conv1d(67, 128, kernel_size=(1,), stride=(1,))
              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU()
            )
            (conv2): Sequential(
              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU()
            )
          )
          (2): TransitionDown(
            (conv1): Sequential(
              (0): Conv1d(131, 256, kernel_size=(1,), stride=(1,))
              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU()
            )
            (conv2): Sequential(
              (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU()
            )
          )
          (3): TransitionDown(
            (conv1): Sequential(
              (0): Conv1d(259, 512, kernel_size=(1,), stride=(1,))
              (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU()
            )
            (conv2): Sequential(
              (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
              (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU()
            )
          )
        )
        (transformers): ModuleList(
          (0): TransformerBlock(
            (fc1): Linear(in_features=64, out_features=128, bias=True)
            (fc2): Linear(in_features=128, out_features=64, bias=True)
            (fc_delta): Sequential(
              (0): Linear(in_features=3, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=128, bias=True)
            )
            (fc_gamma): Sequential(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=128, bias=True)
            )
            (w_qs): Linear(in_features=128, out_features=128, bias=False)
            (w_ks): Linear(in_features=128, out_features=128, bias=False)
            (w_vs): Linear(in_features=128, out_features=128, bias=False)
          )
          (1): TransformerBlock(
            (fc1): Linear(in_features=128, out_features=128, bias=True)
            (fc2): Linear(in_features=128, out_features=128, bias=True)
            (fc_delta): Sequential(
              (0): Linear(in_features=3, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=128, bias=True)
            )
            (fc_gamma): Sequential(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=128, bias=True)
            )
            (w_qs): Linear(in_features=128, out_features=128, bias=False)
            (w_ks): Linear(in_features=128, out_features=128, bias=False)
            (w_vs): Linear(in_features=128, out_features=128, bias=False)
          )
          (2): TransformerBlock(
            (fc1): Linear(in_features=256, out_features=128, bias=True)
            (fc2): Linear(in_features=128, out_features=256, bias=True)
            (fc_delta): Sequential(
              (0): Linear(in_features=3, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=128, bias=True)
            )
            (fc_gamma): Sequential(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=128, bias=True)
            )
            (w_qs): Linear(in_features=128, out_features=128, bias=False)
            (w_ks): Linear(in_features=128, out_features=128, bias=False)
            (w_vs): Linear(in_features=128, out_features=128, bias=False)
          )
          (3): TransformerBlock(
            (fc1): Linear(in_features=512, out_features=128, bias=True)
            (fc2): Linear(in_features=128, out_features=512, bias=True)
            (fc_delta): Sequential(
              (0): Linear(in_features=3, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=128, bias=True)
            )
            (fc_gamma): Sequential(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=128, bias=True)
            )
            (w_qs): Linear(in_features=128, out_features=128, bias=False)
            (w_ks): Linear(in_features=128, out_features=128, bias=False)
            (w_vs): Linear(in_features=128, out_features=128, bias=False)
          )
        )
      )
      (transformer): Transformer(
        (layers): ModuleList(
          (0-4): 5 x ModuleList(
            (0): Residual(
              (fn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Attention(
                  (to_k): Linear(in_features=512, out_features=512, bias=False)
                  (to_v): Linear(in_features=512, out_features=512, bias=False)
                  (to_q): Linear(in_features=512, out_features=512, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=512, out_features=512, bias=True)
                    (1): GELU(approximate='none')
                    (2): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): Residual(
              (fn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=512, out_features=256, bias=True)
                    (1): GELU(approximate='none')
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=256, out_features=512, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (fc2): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Dropout(p=0.0, inplace=False)
        (2): ReLU()
        (3): Linear(in_features=256, out_features=32, bias=True)
      )
      (fc3): Sequential(
        (0): ReLU()
        (1): Linear(in_features=32, out_features=64, bias=True)
        (2): Dropout(p=0.0, inplace=False)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=3, bias=True)
      )
    )



.. GENERATED FROM PYTHON SOURCE LINES 146-147

Test the model on testing dataset.

.. GENERATED FROM PYTHON SOURCE LINES 149-150

In[14]:

.. GENERATED FROM PYTHON SOURCE LINES 150-153

.. code-block:: Python

    from pysensing.mmwave.PC.inference.hpe import hpe_test
    # hpe_test(model, test_loader, criterion=criterion, device=device)








.. GENERATED FROM PYTHON SOURCE LINES 154-155

Model inference on sample and deep feature embedding of input modality in HPE task.

.. GENERATED FROM PYTHON SOURCE LINES 157-158

In[15]:

.. GENERATED FROM PYTHON SOURCE LINES 158-173

.. code-block:: Python


    # Model inference
    idx = 5
    points, pose= test_dataset.__getitem__(idx)
    points = torch.tensor(points).unsqueeze(0).float().to(device)
    predicted_result = model(points)
    print("The predicted pose is {}, while the ground truth is {}".format(predicted_result.cpu(),pose))

    # Deep feature embedding
    from pysensing.mmwave.PC.inference.embedding import embedding
    emb = embedding(input = points, model=model, dataset_name = "MetaFi", model_name = "PointTransformer", device=device)
    print("The shape of feature embedding is: ", emb.shape)







.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    The predicted pose is tensor([[[-0.1174,  0.0153,  3.0502],
             [-0.2116,  0.0201,  3.0562],
             [-0.2254,  0.3597,  3.0622],
             [-0.2422,  0.7359,  3.0764],
             [ 0.0274, -0.0367,  3.0689],
             [ 0.0280,  0.3391,  3.0851],
             [ 0.0784,  0.7612,  3.1083],
             [-0.1010, -0.2694,  3.0435],
             [-0.0911, -0.5694,  3.0298],
             [-0.0989, -0.6771,  3.0037],
             [-0.1014, -0.7289,  3.0263],
             [ 0.0349, -0.5211,  3.0320],
             [ 0.2234, -0.4633,  2.9763],
             [ 0.1240, -0.5344,  2.8526],
             [-0.1987, -0.5264,  3.0986],
             [-0.3519, -0.4576,  3.0666],
             [-0.2247, -0.4779,  2.9469]]], grad_fn=<ToCopyBackward0>), while the ground truth is tensor([[-0.0626, -0.0378,  3.3111],
            [-0.1724, -0.0395,  3.3111],
            [-0.1786,  0.3689,  3.3083],
            [-0.2026,  0.7605,  3.3111],
            [ 0.0473, -0.0362,  3.3111],
            [ 0.0633,  0.3689,  3.3111],
            [ 0.0673,  0.7605,  3.3111],
            [-0.0685, -0.3322,  3.3016],
            [-0.0744, -0.6267,  3.2920],
            [-0.0642, -0.7458,  3.2512],
            [-0.0653, -0.8049,  3.2850],
            [ 0.0914, -0.5671,  3.3118],
            [ 0.3367, -0.5248,  3.3104],
            [ 0.2930, -0.5677,  3.0678],
            [-0.2505, -0.5671,  3.3131],
            [-0.5012, -0.5671,  3.3116],
            [-0.4514, -0.5674,  3.0686]])
    The shape of feature embedding is:  torch.Size([1, 17, 32])




.. GENERATED FROM PYTHON SOURCE LINES 174-176

mmDiff: diffusion model for mmWave radar HPE
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 178-185

Load Diffusion Runner with model initialized. This process will define the setting for model and dataset. Currently two settings are implemented: 
1. "mmBody + P4Transformer": 
    Phase 1: Input [b, 4, 5000, 6]; Output: [b, 17, 3] and [b, 17, 64]. 
    Phase 2: GRC, LRC, TMC, SLC
2. "MetaFi + PointTransformer": 
    Phase 1: Input [b, 5, 150, 5]; Output: [b, 17, 3] and [b, 17, 32]. 
    Phase 2: GRC, TMC, SLC

.. GENERATED FROM PYTHON SOURCE LINES 187-188

In[16]:

.. GENERATED FROM PYTHON SOURCE LINES 188-192

.. code-block:: Python

    from pysensing.mmwave.PC.model.hpe.mmDiff.load_mmDiff import load_mmDiff
    mmDiffRunner = load_mmDiff("MetaFi")






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Self.model_feat vadility passes.
    MMdiff using PointTransformer as feature extractor.




.. GENERATED FROM PYTHON SOURCE LINES 193-196

Phase 1 Training: Can train phase 1 from scratch (is_train = True) or load pretrained phase 1 model (is_train = False).
 Set is_save = True to facilitate phase 2 training acceleration.
If phase 1 features are saved, set is_save = False.

.. GENERATED FROM PYTHON SOURCE LINES 198-199

In[17]:

.. GENERATED FROM PYTHON SOURCE LINES 199-202

.. code-block:: Python


    mmDiffRunner.phase1_train(train_dataset, test_dataset, is_train=False, is_save=False)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Phase 1 use pretrained model!




.. GENERATED FROM PYTHON SOURCE LINES 203-204

Phase 1 can also receive self defined model and the model should follow the setting defined above. The Self-defined model should output coarse joints and coarse joint features.

.. GENERATED FROM PYTHON SOURCE LINES 206-207

In[18]:

.. GENERATED FROM PYTHON SOURCE LINES 207-218

.. code-block:: Python


    # Self defined model should output coarse joints and coarse joint features
    from pysensing.mmwave.PC.model.hpe.pointTrans import PointTransformerReg_feat
    model = PointTransformerReg_feat(
                        input_dim = 5,
                        nblocks = 5,
                        n_p = 17
                    )
    print(model)
    mmDiffRunner.phase1_train(train_dataset, test_dataset, model_self=model, is_train=False, is_save=False)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    PointTransformerReg_feat(
      (backbone): Backbone(
        (fc1): Sequential(
          (0): Linear(in_features=5, out_features=32, bias=True)
          (1): ReLU()
          (2): Linear(in_features=32, out_features=32, bias=True)
        )
        (transformer1): TransformerBlock(
          (fc1): Linear(in_features=32, out_features=128, bias=True)
          (fc2): Linear(in_features=128, out_features=32, bias=True)
          (fc_delta): Sequential(
            (0): Linear(in_features=3, out_features=128, bias=True)
            (1): ReLU()
            (2): Linear(in_features=128, out_features=128, bias=True)
          )
          (fc_gamma): Sequential(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): ReLU()
            (2): Linear(in_features=128, out_features=128, bias=True)
          )
          (w_qs): Linear(in_features=128, out_features=128, bias=False)
          (w_ks): Linear(in_features=128, out_features=128, bias=False)
          (w_vs): Linear(in_features=128, out_features=128, bias=False)
        )
        (transition_downs): ModuleList(
          (0): TransitionDown(
            (conv1): Sequential(
              (0): Conv1d(35, 64, kernel_size=(1,), stride=(1,))
              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU()
            )
            (conv2): Sequential(
              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))
              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU()
            )
          )
          (1): TransitionDown(
            (conv1): Sequential(
              (0): Conv1d(67, 128, kernel_size=(1,), stride=(1,))
              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU()
            )
            (conv2): Sequential(
              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU()
            )
          )
          (2): TransitionDown(
            (conv1): Sequential(
              (0): Conv1d(131, 256, kernel_size=(1,), stride=(1,))
              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU()
            )
            (conv2): Sequential(
              (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU()
            )
          )
          (3): TransitionDown(
            (conv1): Sequential(
              (0): Conv1d(259, 512, kernel_size=(1,), stride=(1,))
              (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU()
            )
            (conv2): Sequential(
              (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
              (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU()
            )
          )
        )
        (transformers): ModuleList(
          (0): TransformerBlock(
            (fc1): Linear(in_features=64, out_features=128, bias=True)
            (fc2): Linear(in_features=128, out_features=64, bias=True)
            (fc_delta): Sequential(
              (0): Linear(in_features=3, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=128, bias=True)
            )
            (fc_gamma): Sequential(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=128, bias=True)
            )
            (w_qs): Linear(in_features=128, out_features=128, bias=False)
            (w_ks): Linear(in_features=128, out_features=128, bias=False)
            (w_vs): Linear(in_features=128, out_features=128, bias=False)
          )
          (1): TransformerBlock(
            (fc1): Linear(in_features=128, out_features=128, bias=True)
            (fc2): Linear(in_features=128, out_features=128, bias=True)
            (fc_delta): Sequential(
              (0): Linear(in_features=3, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=128, bias=True)
            )
            (fc_gamma): Sequential(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=128, bias=True)
            )
            (w_qs): Linear(in_features=128, out_features=128, bias=False)
            (w_ks): Linear(in_features=128, out_features=128, bias=False)
            (w_vs): Linear(in_features=128, out_features=128, bias=False)
          )
          (2): TransformerBlock(
            (fc1): Linear(in_features=256, out_features=128, bias=True)
            (fc2): Linear(in_features=128, out_features=256, bias=True)
            (fc_delta): Sequential(
              (0): Linear(in_features=3, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=128, bias=True)
            )
            (fc_gamma): Sequential(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=128, bias=True)
            )
            (w_qs): Linear(in_features=128, out_features=128, bias=False)
            (w_ks): Linear(in_features=128, out_features=128, bias=False)
            (w_vs): Linear(in_features=128, out_features=128, bias=False)
          )
          (3): TransformerBlock(
            (fc1): Linear(in_features=512, out_features=128, bias=True)
            (fc2): Linear(in_features=128, out_features=512, bias=True)
            (fc_delta): Sequential(
              (0): Linear(in_features=3, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=128, bias=True)
            )
            (fc_gamma): Sequential(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=128, bias=True)
            )
            (w_qs): Linear(in_features=128, out_features=128, bias=False)
            (w_ks): Linear(in_features=128, out_features=128, bias=False)
            (w_vs): Linear(in_features=128, out_features=128, bias=False)
          )
        )
      )
      (transformer): Transformer(
        (layers): ModuleList(
          (0-4): 5 x ModuleList(
            (0): Residual(
              (fn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Attention(
                  (to_k): Linear(in_features=512, out_features=512, bias=False)
                  (to_v): Linear(in_features=512, out_features=512, bias=False)
                  (to_q): Linear(in_features=512, out_features=512, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=512, out_features=512, bias=True)
                    (1): GELU(approximate='none')
                    (2): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): Residual(
              (fn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=512, out_features=256, bias=True)
                    (1): GELU(approximate='none')
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=256, out_features=512, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (fc2): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Dropout(p=0.0, inplace=False)
        (2): ReLU()
        (3): Linear(in_features=256, out_features=32, bias=True)
      )
      (fc3): Sequential(
        (0): ReLU()
        (1): Linear(in_features=32, out_features=64, bias=True)
        (2): Dropout(p=0.0, inplace=False)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=3, bias=True)
      )
    )
    Self.model_feat vadility passes.
    Phase 1 use self defined model!




.. GENERATED FROM PYTHON SOURCE LINES 219-220

Phase 2 Training: Can train from scratch (is_train = True) or load pretrained phase 2 model (is_train = False).

.. GENERATED FROM PYTHON SOURCE LINES 222-223

In[19]:

.. GENERATED FROM PYTHON SOURCE LINES 223-226

.. code-block:: Python


    mmDiffRunner.phase2_train(train_loader = None, is_train = False)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Phase 2 use pretrained model!




.. GENERATED FROM PYTHON SOURCE LINES 227-228

Testing mmDiff

.. GENERATED FROM PYTHON SOURCE LINES 230-231

In[20]:

.. GENERATED FROM PYTHON SOURCE LINES 231-233

.. code-block:: Python


    #mmDiffRunner.test()








.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (2 minutes 39.438 seconds)


.. _sphx_glr_download_mmwave_PC_mmwave_PC_hpe_tutorial.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: mmwave_PC_hpe_tutorial.ipynb <mmwave_PC_hpe_tutorial.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: mmwave_PC_hpe_tutorial.py <mmwave_PC_hpe_tutorial.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
