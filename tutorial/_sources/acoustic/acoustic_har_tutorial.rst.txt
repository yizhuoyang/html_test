
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "acoustic/acoustic_har_tutorial.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_acoustic_acoustic_har_tutorial.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_acoustic_acoustic_har_tutorial.py:


Acoustic Human Activity Recognition Tutorial
==============================================================

.. GENERATED FROM PYTHON SOURCE LINES 7-8

!pip install pysensing

.. GENERATED FROM PYTHON SOURCE LINES 10-12

In this tutorial, we will be implementing codes for acoustic Human activity recognition


.. GENERATED FROM PYTHON SOURCE LINES 12-27

.. code-block:: Python

    import torch
    torch.backends.cudnn.benchmark = True
    import matplotlib.pyplot as plt
    import numpy as np
    import pysensing.acoustic.datasets.har as har_datasets
    import pysensing.acoustic.models.har as har_models
    import pysensing.acoustic.models.get_model as acoustic_models
    import pysensing.acoustic.inference.embedding as embedding
    seed = 42
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')








.. GENERATED FROM PYTHON SOURCE LINES 28-35

SAMoSA: Sensoring Activities with Motion abd Subsampled Audio
-----------------------------------
SAMSoSA dataset is designed to use audio and IMU data collected by a watch to predict the actions of the users.

There are totally 27 actions in the dataset. 

In the library, we provide a dataloader to use only audio data to predict these actions. 

.. GENERATED FROM PYTHON SOURCE LINES 37-40

Load the data
------------------------
Method 1: Use get_dataloader

.. GENERATED FROM PYTHON SOURCE LINES 40-62

.. code-block:: Python

    from pysensing.acoustic.datasets.get_dataloader import *
    train_loader,test_loader = load_har_dataset(
        root='./data',
        dataset='samosa',
        download=True)

    # Method 2: Manually setup the dataloader
    root = './data' # The path contains the samosa dataset
    samosa_traindataset = har_datasets.SAMoSA(root,'train')
    samosa_testdataset = har_datasets.SAMoSA(root,'test')
    # Define the dataloader
    samosa_trainloader = DataLoader(samosa_traindataset,batch_size=64,shuffle=True,drop_last=True)
    samosa_testloader = DataLoader(samosa_testdataset,batch_size=64,shuffle=True,drop_last=True)
    dataclass = samosa_traindataset.class_dict
    datalist  = samosa_traindataset.audio_data
    # Example of the samples in the dataset
    index = 50  # Randomly select an index
    spectrogram,activity= samosa_traindataset.__getitem__(index)
    plt.figure(figsize=(10,5))
    plt.imshow(spectrogram.numpy()[0])
    plt.title("Spectrogram for activity: {}".format(activity))
    plt.show()



.. image-sg:: /acoustic/images/sphx_glr_acoustic_har_tutorial_001.png
   :alt: Spectrogram for activity: 0
   :srcset: /acoustic/images/sphx_glr_acoustic_har_tutorial_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    using dataset: SAMoSA




.. GENERATED FROM PYTHON SOURCE LINES 63-66

Load the model
------------------------
Method 1:

.. GENERATED FROM PYTHON SOURCE LINES 66-71

.. code-block:: Python

    samosa_model = har_models.HAR_SAMCNN(dropout=0.6).to(device)
    # Method 2:
    samosa_model = acoustic_models.load_har_model('samcnn',pretrained=True).to(device)









.. GENERATED FROM PYTHON SOURCE LINES 72-74

Model training and testing
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 74-84

.. code-block:: Python

    from pysensing.acoustic.inference.training.har_train import *
    # Model training
    epoch = 1
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(samosa_model.parameters(), 0.0001)
    har_train_val(samosa_model,samosa_trainloader,samosa_testloader, epoch, optimizer, criterion, device, save_dir = './data',save = True)

    # Model testing
    test_loss = har_test(samosa_model,samosa_testloader,criterion,device)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Train round0/1:   0%|          | 0/78 [00:00<?, ?batch/s]    Train round0/1:   1%|▏         | 1/78 [00:00<00:20,  3.76batch/s]    Train round0/1:   3%|▎         | 2/78 [00:00<00:12,  5.87batch/s]    Train round0/1:   4%|▍         | 3/78 [00:00<00:10,  7.22batch/s]    Train round0/1:   5%|▌         | 4/78 [00:00<00:09,  7.67batch/s]    Train round0/1:   8%|▊         | 6/78 [00:00<00:08,  8.98batch/s]    Train round0/1:  10%|█         | 8/78 [00:00<00:07,  9.52batch/s]    Train round0/1:  12%|█▏        | 9/78 [00:01<00:07,  9.57batch/s]    Train round0/1:  14%|█▍        | 11/78 [00:01<00:06,  9.87batch/s]    Train round0/1:  17%|█▋        | 13/78 [00:01<00:06, 10.19batch/s]    Train round0/1:  19%|█▉        | 15/78 [00:01<00:06, 10.13batch/s]    Train round0/1:  22%|██▏       | 17/78 [00:01<00:05, 10.87batch/s]    Train round0/1:  24%|██▍       | 19/78 [00:01<00:05, 10.74batch/s]    Train round0/1:  27%|██▋       | 21/78 [00:02<00:05, 10.36batch/s]    Train round0/1:  29%|██▉       | 23/78 [00:02<00:05, 10.37batch/s]    Train round0/1:  32%|███▏      | 25/78 [00:02<00:05, 10.43batch/s]    Train round0/1:  35%|███▍      | 27/78 [00:02<00:04, 10.33batch/s]    Train round0/1:  37%|███▋      | 29/78 [00:02<00:04, 10.78batch/s]    Train round0/1:  40%|███▉      | 31/78 [00:03<00:04, 10.82batch/s]    Train round0/1:  42%|████▏     | 33/78 [00:03<00:04, 10.68batch/s]    Train round0/1:  45%|████▍     | 35/78 [00:03<00:04, 10.67batch/s]    Train round0/1:  47%|████▋     | 37/78 [00:03<00:03, 10.76batch/s]    Train round0/1:  50%|█████     | 39/78 [00:03<00:03, 10.47batch/s]    Train round0/1:  53%|█████▎    | 41/78 [00:04<00:03, 10.56batch/s]    Train round0/1:  55%|█████▌    | 43/78 [00:04<00:03, 10.41batch/s]    Train round0/1:  58%|█████▊    | 45/78 [00:04<00:03, 10.45batch/s]    Train round0/1:  60%|██████    | 47/78 [00:04<00:03, 10.13batch/s]    Train round0/1:  63%|██████▎   | 49/78 [00:04<00:02, 10.03batch/s]    Train round0/1:  65%|██████▌   | 51/78 [00:05<00:02, 10.58batch/s]    Train round0/1:  68%|██████▊   | 53/78 [00:05<00:02, 11.08batch/s]    Train round0/1:  71%|███████   | 55/78 [00:05<00:01, 11.71batch/s]    Train round0/1:  73%|███████▎  | 57/78 [00:05<00:01, 12.21batch/s]    Train round0/1:  76%|███████▌  | 59/78 [00:05<00:01, 11.85batch/s]    Train round0/1:  78%|███████▊  | 61/78 [00:05<00:01, 11.22batch/s]    Train round0/1:  81%|████████  | 63/78 [00:06<00:01, 11.14batch/s]    Train round0/1:  83%|████████▎ | 65/78 [00:06<00:01, 11.13batch/s]    Train round0/1:  86%|████████▌ | 67/78 [00:06<00:01, 10.82batch/s]    Train round0/1:  88%|████████▊ | 69/78 [00:06<00:00, 10.94batch/s]    Train round0/1:  91%|█████████ | 71/78 [00:06<00:00, 10.86batch/s]    Train round0/1:  94%|█████████▎| 73/78 [00:07<00:00, 10.66batch/s]    Train round0/1:  96%|█████████▌| 75/78 [00:07<00:00, 10.65batch/s]    Train round0/1:  99%|█████████▊| 77/78 [00:07<00:00, 10.62batch/s]                                                                      Epoch:1, Accuracy:0.7590,Loss:0.735932323
    Test round:   0%|          | 0/30 [00:00<?, ?batch/s]    Test round:   3%|▎         | 1/30 [00:00<00:03,  9.51batch/s]    Test round:  10%|█         | 3/30 [00:00<00:02, 11.42batch/s]    Test round:  17%|█▋        | 5/30 [00:00<00:02, 11.17batch/s]    Test round:  23%|██▎       | 7/30 [00:00<00:02, 10.89batch/s]    Test round:  30%|███       | 9/30 [00:00<00:01, 11.28batch/s]    Test round:  37%|███▋      | 11/30 [00:01<00:01, 10.89batch/s]    Test round:  43%|████▎     | 13/30 [00:01<00:01, 10.86batch/s]    Test round:  50%|█████     | 15/30 [00:01<00:01, 11.13batch/s]    Test round:  57%|█████▋    | 17/30 [00:01<00:01, 10.99batch/s]    Test round:  63%|██████▎   | 19/30 [00:01<00:01, 10.82batch/s]    Test round:  70%|███████   | 21/30 [00:01<00:00, 11.09batch/s]    Test round:  77%|███████▋  | 23/30 [00:02<00:00, 10.88batch/s]    Test round:  83%|████████▎ | 25/30 [00:02<00:00, 11.01batch/s]    Test round:  90%|█████████ | 27/30 [00:02<00:00, 11.44batch/s]    Test round:  97%|█████████▋| 29/30 [00:02<00:00, 11.35batch/s]                                                                  test accuracy:0.6594, loss:1.07134
    Save model at ./data/train_0.pth...
    Test round:   0%|          | 0/30 [00:00<?, ?batch/s]    Test round:  30%|███       | 9/30 [00:00<00:00, 84.41batch/s]    Test round:  67%|██████▋   | 20/30 [00:00<00:00, 94.29batch/s]                                                                  test accuracy:0.6599, loss:1.07058




.. GENERATED FROM PYTHON SOURCE LINES 85-90

Modle inference for single sample
------------------------
Method 1
You may aslo load your own trained model by setting the path
samosa_model.load_state_dict(torch.load('path_to_model')) # the path for the model

.. GENERATED FROM PYTHON SOURCE LINES 90-104

.. code-block:: Python

    spectrogram,activity= samosa_testdataset.__getitem__(3)
    samosa_model.eval()
    #Direct use the model for sample inference
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    predicted_result = samosa_model(spectrogram.unsqueeze(0).float().to(device))
    print("The ground truth is {}, while the predicted activity is {}".format(activity,torch.argmax(predicted_result).cpu()))

    # Method 2
    # Use inference.predict
    from pysensing.acoustic.inference.predict import *
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    predicted_result  = har_predict(spectrogram,'SAMoSA',samosa_model, device)
    print("The ground truth is {}, while the predicted activity is {}".format(activity,torch.argmax(predicted_result).cpu()))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    The ground truth is 18, while the predicted activity is 18
    The ground truth is 18, while the predicted activity is 18




.. GENERATED FROM PYTHON SOURCE LINES 105-107

Modle inference for single sample
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 107-111

.. code-block:: Python

    from pysensing.acoustic.inference.embedding import *
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    sample_embedding = har_embedding(spectrogram,'SAMoSA',samosa_model, device=device)








.. GENERATED FROM PYTHON SOURCE LINES 112-117

Implementation of "AudioIMU: Enhancing Inertial Sensing-Based Activity Recognition with Acoustic Models"
-----------------------------------
This dataset is designed to use audio and IMU data collected by a watch to predict the actions of the users, 23 different activities are collected in the dataset.

But different from the orginal paper, the reimplemeted paper only takes the audio data for human activity recognition. Subjects 01, 02, 03, 04 are used for testing while the other are used for training.

.. GENERATED FROM PYTHON SOURCE LINES 119-122

Load the data
------------------------
Method 1: Use get_dataloader

.. GENERATED FROM PYTHON SOURCE LINES 122-146

.. code-block:: Python

    from pysensing.acoustic.datasets.get_dataloader import *
    train_loader,test_loader = load_har_dataset(
        root='./data',
        dataset='audioimu',
        download=True)

    # Method2
    root = './data' # The path contains the audioimu dataset
    audioimu_traindataset = har_datasets.AudioIMU(root,'train')
    audioimu_testdataset = har_datasets.AudioIMU(root,'test')
    # Define the Dataloader
    audioimu_trainloader = DataLoader(audioimu_traindataset,batch_size=64,shuffle=False,drop_last=True)
    audioimu_testloader = DataLoader(audioimu_testdataset,batch_size=64,shuffle=False,drop_last=True)
    #List the activity classes in the dataset
    dataclass = audioimu_traindataset.classlist
    # Example of the samples in the dataset
    index = 0  # Randomly select an index
    spectrogram,activity= audioimu_testdataset.__getitem__(index)
    print(spectrogram.shape)
    plt.figure(figsize=(18,6))
    plt.imshow(spectrogram.numpy()[0])
    plt.title("Spectrogram for activity: {}".format(activity))
    plt.show()




.. image-sg:: /acoustic/images/sphx_glr_acoustic_har_tutorial_002.png
   :alt: Spectrogram for activity: 15
   :srcset: /acoustic/images/sphx_glr_acoustic_har_tutorial_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    using dataset: AudioImu
    torch.Size([1, 64, 690])




.. GENERATED FROM PYTHON SOURCE LINES 147-150

Load the model
------------------------
Method 1

.. GENERATED FROM PYTHON SOURCE LINES 150-155

.. code-block:: Python

    audio_model = har_models.HAR_AUDIOCNN().to(device)
    # Method2
    audio_model = acoustic_models.load_har_model('audiocnn',pretrained=True).to(device)









.. GENERATED FROM PYTHON SOURCE LINES 156-158

Model training and testing
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 158-168

.. code-block:: Python

    from pysensing.acoustic.inference.training.har_train import *
    epoch = 1
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(audio_model.parameters(), 0.0001)
    loss = har_train_val(audio_model,audioimu_trainloader,audioimu_testloader, epoch, optimizer, criterion, device, save_dir='./data',save = False)

    # Model testing
    test_loss = har_test(audio_model,audioimu_testloader,criterion,device)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Train round0/1:   0%|          | 0/40 [00:00<?, ?batch/s]    Train round0/1:   2%|▎         | 1/40 [00:03<02:21,  3.63s/batch]    Train round0/1:   5%|▌         | 2/40 [00:04<01:11,  1.88s/batch]    Train round0/1:   8%|▊         | 3/40 [00:04<00:48,  1.31s/batch]    Train round0/1:  10%|█         | 4/40 [00:05<00:37,  1.05s/batch]    Train round0/1:  12%|█▎        | 5/40 [00:06<00:31,  1.10batch/s]    Train round0/1:  15%|█▌        | 6/40 [00:06<00:27,  1.22batch/s]    Train round0/1:  18%|█▊        | 7/40 [00:07<00:25,  1.30batch/s]    Train round0/1:  20%|██        | 8/40 [00:08<00:23,  1.38batch/s]    Train round0/1:  22%|██▎       | 9/40 [00:08<00:21,  1.41batch/s]    Train round0/1:  25%|██▌       | 10/40 [00:09<00:20,  1.45batch/s]    Train round0/1:  28%|██▊       | 11/40 [00:10<00:19,  1.49batch/s]    Train round0/1:  30%|███       | 12/40 [00:10<00:18,  1.50batch/s]    Train round0/1:  32%|███▎      | 13/40 [00:11<00:17,  1.51batch/s]    Train round0/1:  35%|███▌      | 14/40 [00:12<00:17,  1.51batch/s]    Train round0/1:  38%|███▊      | 15/40 [00:12<00:16,  1.53batch/s]    Train round0/1:  40%|████      | 16/40 [00:13<00:15,  1.52batch/s]    Train round0/1:  42%|████▎     | 17/40 [00:14<00:15,  1.52batch/s]    Train round0/1:  45%|████▌     | 18/40 [00:14<00:14,  1.51batch/s]    Train round0/1:  48%|████▊     | 19/40 [00:15<00:13,  1.52batch/s]    Train round0/1:  50%|█████     | 20/40 [00:16<00:13,  1.52batch/s]    Train round0/1:  52%|█████▎    | 21/40 [00:16<00:12,  1.53batch/s]    Train round0/1:  55%|█████▌    | 22/40 [00:17<00:11,  1.53batch/s]    Train round0/1:  57%|█████▊    | 23/40 [00:17<00:11,  1.53batch/s]    Train round0/1:  60%|██████    | 24/40 [00:18<00:10,  1.54batch/s]    Train round0/1:  62%|██████▎   | 25/40 [00:19<00:09,  1.54batch/s]    Train round0/1:  65%|██████▌   | 26/40 [00:19<00:09,  1.53batch/s]    Train round0/1:  68%|██████▊   | 27/40 [00:20<00:08,  1.54batch/s]    Train round0/1:  70%|███████   | 28/40 [00:21<00:07,  1.54batch/s]    Train round0/1:  72%|███████▎  | 29/40 [00:21<00:07,  1.54batch/s]    Train round0/1:  75%|███████▌  | 30/40 [00:22<00:06,  1.53batch/s]    Train round0/1:  78%|███████▊  | 31/40 [00:23<00:05,  1.54batch/s]    Train round0/1:  80%|████████  | 32/40 [00:23<00:05,  1.53batch/s]    Train round0/1:  82%|████████▎ | 33/40 [00:24<00:04,  1.53batch/s]    Train round0/1:  85%|████████▌ | 34/40 [00:25<00:03,  1.54batch/s]    Train round0/1:  88%|████████▊ | 35/40 [00:25<00:03,  1.53batch/s]    Train round0/1:  90%|█████████ | 36/40 [00:26<00:02,  1.53batch/s]    Train round0/1:  92%|█████████▎| 37/40 [00:27<00:01,  1.54batch/s]    Train round0/1:  95%|█████████▌| 38/40 [00:27<00:01,  1.54batch/s]    Train round0/1:  98%|█████████▊| 39/40 [00:28<00:00,  1.54batch/s]    Train round0/1: 100%|██████████| 40/40 [00:29<00:00,  1.53batch/s]                                                                      Epoch:1, Accuracy:0.9660,Loss:0.096005203
    Test round:   0%|          | 0/15 [00:00<?, ?batch/s]    Test round:   7%|▋         | 1/15 [00:00<00:07,  1.82batch/s]    Test round:  13%|█▎        | 2/15 [00:01<00:07,  1.83batch/s]    Test round:  20%|██        | 3/15 [00:01<00:06,  1.84batch/s]    Test round:  27%|██▋       | 4/15 [00:02<00:05,  1.84batch/s]    Test round:  33%|███▎      | 5/15 [00:02<00:05,  1.84batch/s]    Test round:  40%|████      | 6/15 [00:03<00:04,  1.84batch/s]    Test round:  47%|████▋     | 7/15 [00:03<00:04,  1.84batch/s]    Test round:  53%|█████▎    | 8/15 [00:04<00:03,  1.84batch/s]    Test round:  60%|██████    | 9/15 [00:04<00:03,  1.82batch/s]    Test round:  67%|██████▋   | 10/15 [00:05<00:02,  1.83batch/s]    Test round:  73%|███████▎  | 11/15 [00:05<00:02,  1.84batch/s]    Test round:  80%|████████  | 12/15 [00:06<00:01,  1.84batch/s]    Test round:  87%|████████▋ | 13/15 [00:07<00:01,  1.85batch/s]    Test round:  93%|█████████▎| 14/15 [00:07<00:00,  1.85batch/s]    Test round: 100%|██████████| 15/15 [00:08<00:00,  1.84batch/s]                                                                  test accuracy:0.5781, loss:3.59506




.. GENERATED FROM PYTHON SOURCE LINES 169-173

Model inference
------------------------
You may aslo load your own trained model by setting the path
audio_model.load_state_dict(torch.load('path_to_model')) # the path for the model

.. GENERATED FROM PYTHON SOURCE LINES 173-186

.. code-block:: Python

    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    spectrogram,activity= audioimu_testdataset.__getitem__(6)
    audio_model.eval()
    predicted_result = audio_model(spectrogram.unsqueeze(0).float().to(device))
    print("The ground truth is {}, while the predicted activity is {}".format(activity,torch.argmax(predicted_result).cpu()))

    #Method 2
    #Use inference.predict
    from pysensing.acoustic.inference.predict import *
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    predicted_result  = har_predict(spectrogram,'AudioIMU',audio_model, device)
    print("The ground truth is {}, while the predicted activity is {}".format(activity,torch.argmax(predicted_result).cpu()))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    The ground truth is 21, while the predicted activity is 21
    The ground truth is 21, while the predicted activity is 21




.. GENERATED FROM PYTHON SOURCE LINES 187-189

Model embedding
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 189-193

.. code-block:: Python

    from pysensing.acoustic.inference.embedding import *
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    sample_embedding = har_embedding(spectrogram,'AudioIMU',audio_model, device=device)








.. GENERATED FROM PYTHON SOURCE LINES 194-195

And that's it. We're done with our acoustic humna activity recognition tutorials. Thanks for reading.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 49.397 seconds)


.. _sphx_glr_download_acoustic_acoustic_har_tutorial.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: acoustic_har_tutorial.ipynb <acoustic_har_tutorial.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: acoustic_har_tutorial.py <acoustic_har_tutorial.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
