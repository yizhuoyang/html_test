
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "acoustic/acoustic_hpe_tutorial.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_acoustic_acoustic_hpe_tutorial.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_acoustic_acoustic_hpe_tutorial.py:


Acoustic Human Pose estimation Tutorial
==============================================================

.. GENERATED FROM PYTHON SOURCE LINES 7-8

!pip install pysensing

.. GENERATED FROM PYTHON SOURCE LINES 10-12

In this tutorial, we will be implementing codes for acoustic Human pose estimation


.. GENERATED FROM PYTHON SOURCE LINES 12-17

.. code-block:: Python


    from pysensing.acoustic.datasets.utils.hpe_vis import *
    from pysensing.acoustic.models.hpe import Speech2pose,Wipose_LSTM
    from pysensing.acoustic.models.get_model import load_hpe_model
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')







.. GENERATED FROM PYTHON SOURCE LINES 18-25

Listening Human Behavior: 3D Human Pose Estimation with Acoustic Signals
 ----------------------------------------------------------------------------------
Implementation of "Listening Human Behavior: 3D Human Pose Estimation with Acoustic Signals".

This dataset contains the audio reflected by human to estimate the 3D human pose with the acoustic signals.

Reference: https://github.com/YutoShibata07/AcousticPose_Public

.. GENERATED FROM PYTHON SOURCE LINES 27-29

Load the data
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 29-48

.. code-block:: Python


    # Method 1: Use get_dataloader
    from pysensing.acoustic.datasets.get_dataloader import *
    train_loader,val_loader,test_loader = load_hpe_dataset(
        root='./data',
        dataset_name='pose_regression_timeseries_subject_1',
        download=True)

    # Method 2
    csv = './data/hpe_dataset/csv/pose_regression_timeseries_subject_1/test.csv' # The path contains the samosa dataset
    data_dir = './data'
    hpe_testdataset = SoundPose2DDataset(csv,sound_length=2400,input_feature='logmel',
                                         mean=np.array(get_mean()).astype("float32")[:4],
                                         std=np.array(get_std()).astype("float32")[:4],
                                         )
    index = 10 # Randomly select an index
    sample= hpe_testdataset.__getitem__(index)
    print(sample['targets'].shape)
    print(sample['sound'].shape)




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    torch.Size([12, 63])
    torch.Size([4, 12, 128])




.. GENERATED FROM PYTHON SOURCE LINES 49-52

Load Speech2pose model
------------------------
Method 1

.. GENERATED FROM PYTHON SOURCE LINES 52-59

.. code-block:: Python

    hpe_model = Speech2pose(out_cha=63).to(device)
    # model_path = 'path to pretrian weights'
    # state_dict = torch.load(model_path,weights_only=True)
    # hpe_model.load_state_dict(state_dict)

    # Method 2
    hpe_model = load_hpe_model('speech2pose',pretrained=True,task='subject8').to(device)







.. GENERATED FROM PYTHON SOURCE LINES 60-62

Modle Inference
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 62-81

.. code-block:: Python


    #Method 1
    sample= hpe_testdataset.__getitem__(index)
    hpe_model.eval()
    predicted_result = hpe_model(sample['sound'].unsqueeze(0).float().to(device))
    vis_images = make_images(sample['targets'].numpy(),predicted_result.cpu().detach().numpy().squeeze(0))

    #Method 2
    from pysensing.acoustic.inference.predict import *
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    predicted_result  = hpe_predict(sample['sound'],'SoundPose2DDataset',hpe_model, device=device)
    vis_images = make_images(sample['targets'].numpy(),predicted_result.cpu().detach().numpy().squeeze(0))

    seq_num = 0
    fig = plt.figure(figsize=(12, 12))
    plt.imshow(vis_images[seq_num]['img'])
    plt.axis('off')
    plt.show()




.. image-sg:: /acoustic/images/sphx_glr_acoustic_hpe_tutorial_001.png
   :alt: acoustic hpe tutorial
   :srcset: /acoustic/images/sphx_glr_acoustic_hpe_tutorial_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/kemove/anaconda3/envs/il/lib/python3.9/site-packages/torch/nn/modules/conv.py:306: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1008.)
      return F.conv1d(input, weight, bias, self.stride,




.. GENERATED FROM PYTHON SOURCE LINES 82-84

Modle Embedding
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 84-88

.. code-block:: Python

    from pysensing.acoustic.inference.embedding import *
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    sample_embedding = hpe_embedding(sample['sound'],'SoundPose2DDataset',hpe_model, device=device)








.. GENERATED FROM PYTHON SOURCE LINES 89-91

Modle Training
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 91-140

.. code-block:: Python

    from pysensing.acoustic.inference.training.AcousticPose_utils.hpe_train import train_model,generate_configs

    args = {
        "root_dir": "./data/hpe_dataset/testing_result",
        "save_name": "seq1",
        "input_feature": "logmel",
        "batchsize": 64,
        "max_epoch": 50,
        "csv_path": "./data/hpe_dataset/csv",
        "dataset_name": "pose_regression_timeseries_subject_1",
        "model": "speech2pose",
        "sound_length": 2400,
        "learning_rate": 0.01,
    }
    config_path = args["root_dir"]+'/'+args["save_name"]+"/config.yaml"
    generate_configs(args)
    resume_training = False
    random_seed = 0

    train_model(
        config_path=config_path,
        resume=resume_training,
        seed=random_seed,
    )

    # Modle Training
    # ------------------------
    from pysensing.acoustic.inference.training.AcousticPose_utils.hpe_test import evaluate_model
    args = {
        "root_dir": "./data/hpe_dataset/testing_result",
        "save_name": "seq1",
        "batchsize": 64,
        "max_epoch": 20,
        "csv_path": "./data/hpe_dataset/csv",
        "dataset_name": "pose_regression_timeseries_subject_1",
        "model": "speech2pose",
        "sound_length": 2400,
        "learning_rate": 0.01,
    }
    config_path = args["root_dir"]+'/'+args["save_name"]+"/config.yaml"
    evaluation_mode = "test"
    model_path = None

    evaluate_model(
        config_path=config_path,
        mode=evaluation_mode,
        model_path=model_path)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    {'csv_path': './data/hpe_dataset/csv', 'model': 'speech2pose', 'pretrained': True, 'use_class_weight': True, 'batch_size': 32, 'width': 224, 'height': 224, 'num_workers': 8, 'max_epoch': 50, 'learning_rate': 0.01, 'sound_length': 2400, 'dataset_name': 'pose_regression_timeseries_subject_1', 'input_feature': 'logmel', 'topk': (1, 3), 'smooth_loss': False, 'ratio': 0.0, 'gan': 'none', 'finetune': 0, 'aug': 'none'}
    Finished making configuration files.
    /home/kemove/anaconda3/envs/il/lib/python3.9/site-packages/pysensing/acoustic/inference/training/AcousticPose_utils/libs/dataset.py:195: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.
      self.df[self.df["sound_length"] == sound_length]
    /home/kemove/anaconda3/envs/il/lib/python3.9/site-packages/pysensing/acoustic/inference/training/AcousticPose_utils/libs/dataset.py:195: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.
      self.df[self.df["sound_length"] == sound_length]
    epoch: 0        epoch time[sec]: 4      train loss: 38.3569     val loss: 122.5143      val_rmse: 11.06862      val_mae: 7.60888
    epoch: 1        epoch time[sec]: 1      train loss: 3.5271      val loss: 5.5197        val_rmse: 2.34941       val_mae: 1.41563
    epoch: 2        epoch time[sec]: 1      train loss: 1.7630      val loss: 2.2134        val_rmse: 1.48776       val_mae: 0.93121
    epoch: 3        epoch time[sec]: 1      train loss: 1.3789      val loss: 2.1969        val_rmse: 1.48221       val_mae: 0.91003
    epoch: 4        epoch time[sec]: 1      train loss: 1.2648      val loss: 2.7483        val_rmse: 1.65781       val_mae: 1.08007
    epoch: 5        epoch time[sec]: 1      train loss: 1.1762      val loss: 3.5439        val_rmse: 1.88253       val_mae: 1.15230
    epoch: 6        epoch time[sec]: 1      train loss: 1.2064      val loss: 1.4791        val_rmse: 1.21617       val_mae: 0.72835
    epoch: 7        epoch time[sec]: 1      train loss: 1.0214      val loss: 3.6852        val_rmse: 1.91968       val_mae: 1.18562
    epoch: 8        epoch time[sec]: 1      train loss: 1.2381      val loss: 9.0887        val_rmse: 3.01475       val_mae: 1.92678
    epoch: 9        epoch time[sec]: 1      train loss: 0.9140      val loss: 2.7270        val_rmse: 1.65135       val_mae: 1.01521
    epoch: 10       epoch time[sec]: 1      train loss: 0.7656      val loss: 4.5527        val_rmse: 2.13371       val_mae: 1.27259
    epoch: 11       epoch time[sec]: 1      train loss: 0.6205      val loss: 1.7849        val_rmse: 1.33601       val_mae: 0.75739
    epoch: 12       epoch time[sec]: 1      train loss: 0.6154      val loss: 1.6265        val_rmse: 1.27533       val_mae: 0.74103
    epoch: 13       epoch time[sec]: 1      train loss: 0.5134      val loss: 0.9743        val_rmse: 0.98705       val_mae: 0.55418
    epoch: 14       epoch time[sec]: 1      train loss: 0.5370      val loss: 0.9392        val_rmse: 0.96910       val_mae: 0.54693
    epoch: 15       epoch time[sec]: 1      train loss: 0.4042      val loss: 0.9194        val_rmse: 0.95886       val_mae: 0.51252
    epoch: 16       epoch time[sec]: 1      train loss: 0.3712      val loss: 0.8734        val_rmse: 0.93454       val_mae: 0.48468
    epoch: 17       epoch time[sec]: 1      train loss: 0.3812      val loss: 1.2397        val_rmse: 1.11342       val_mae: 0.59881
    epoch: 18       epoch time[sec]: 1      train loss: 0.4479      val loss: 7.4993        val_rmse: 2.73849       val_mae: 1.73351
    epoch: 19       epoch time[sec]: 1      train loss: 0.5249      val loss: 0.9717        val_rmse: 0.98574       val_mae: 0.56446
    epoch: 20       epoch time[sec]: 1      train loss: 0.4537      val loss: 2.7692        val_rmse: 1.66409       val_mae: 0.93542
    epoch: 21       epoch time[sec]: 1      train loss: 0.3601      val loss: 3.2854        val_rmse: 1.81256       val_mae: 1.01124
    epoch: 22       epoch time[sec]: 1      train loss: 0.3717      val loss: 1.2158        val_rmse: 1.10262       val_mae: 0.63135
    epoch: 23       epoch time[sec]: 1      train loss: 0.3641      val loss: 1.7205        val_rmse: 1.31166       val_mae: 0.66832
    epoch: 24       epoch time[sec]: 1      train loss: 0.4600      val loss: 3.1732        val_rmse: 1.78136       val_mae: 1.01172
    epoch: 25       epoch time[sec]: 1      train loss: 0.4198      val loss: 0.7186        val_rmse: 0.84772       val_mae: 0.40647
    epoch: 26       epoch time[sec]: 1      train loss: 0.3826      val loss: 0.6256        val_rmse: 0.79096       val_mae: 0.45698
    epoch: 27       epoch time[sec]: 1      train loss: 0.4344      val loss: 1.9526        val_rmse: 1.39737       val_mae: 0.74666
    epoch: 28       epoch time[sec]: 1      train loss: 0.3857      val loss: 2.0649        val_rmse: 1.43699       val_mae: 0.88084
    epoch: 29       epoch time[sec]: 1      train loss: 0.4024      val loss: 2.1361        val_rmse: 1.46154       val_mae: 0.74190
    epoch: 30       epoch time[sec]: 1      train loss: 0.3204      val loss: 1.1463        val_rmse: 1.07066       val_mae: 0.60133
    epoch: 31       epoch time[sec]: 1      train loss: 0.2975      val loss: 0.3898        val_rmse: 0.62431       val_mae: 0.32956
    epoch: 32       epoch time[sec]: 1      train loss: 0.3220      val loss: 1.8097        val_rmse: 1.34526       val_mae: 0.68335
    epoch: 33       epoch time[sec]: 1      train loss: 0.3112      val loss: 0.5565        val_rmse: 0.74597       val_mae: 0.38793
    epoch: 34       epoch time[sec]: 1      train loss: 0.2543      val loss: 0.5031        val_rmse: 0.70931       val_mae: 0.39188
    epoch: 35       epoch time[sec]: 1      train loss: 0.2639      val loss: 0.4450        val_rmse: 0.66710       val_mae: 0.35181
    epoch: 36       epoch time[sec]: 1      train loss: 0.3003      val loss: 0.3797        val_rmse: 0.61624       val_mae: 0.33479
    epoch: 37       epoch time[sec]: 1      train loss: 0.2557      val loss: 0.4548        val_rmse: 0.67439       val_mae: 0.36248
    epoch: 38       epoch time[sec]: 1      train loss: 0.2340      val loss: 0.4082        val_rmse: 0.63889       val_mae: 0.33224
    epoch: 39       epoch time[sec]: 1      train loss: 0.2577      val loss: 0.4151        val_rmse: 0.64427       val_mae: 0.32982
    epoch: 40       epoch time[sec]: 1      train loss: 0.2413      val loss: 0.4459        val_rmse: 0.66773       val_mae: 0.37332
    epoch: 41       epoch time[sec]: 1      train loss: 0.2762      val loss: 0.4307        val_rmse: 0.65624       val_mae: 0.33005
    epoch: 42       epoch time[sec]: 1      train loss: 0.2130      val loss: 0.3840        val_rmse: 0.61968       val_mae: 0.32186
    epoch: 43       epoch time[sec]: 1      train loss: 0.2585      val loss: 0.4669        val_rmse: 0.68330       val_mae: 0.31312
    epoch: 44       epoch time[sec]: 1      train loss: 0.2674      val loss: 0.4117        val_rmse: 0.64164       val_mae: 0.33041
    epoch: 45       epoch time[sec]: 1      train loss: 0.2774      val loss: 0.4377        val_rmse: 0.66156       val_mae: 0.32949
    epoch: 46       epoch time[sec]: 1      train loss: 0.2645      val loss: 0.5535        val_rmse: 0.74399       val_mae: 0.38078
    epoch: 47       epoch time[sec]: 1      train loss: 0.2412      val loss: 0.4701        val_rmse: 0.68563       val_mae: 0.41444
    epoch: 48       epoch time[sec]: 1      train loss: 0.3006      val loss: 0.4420        val_rmse: 0.66485       val_mae: 0.35141
    epoch: 49       epoch time[sec]: 1      train loss: 0.2479      val loss: 0.3813        val_rmse: 0.61753       val_mae: 0.31207
    /home/kemove/anaconda3/envs/il/lib/python3.9/site-packages/pysensing/acoustic/inference/training/AcousticPose_utils/libs/dataset.py:195: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.
      self.df[self.df["sound_length"] == sound_length]
    ---------- Start evaluation for test data ----------
    loss: 0.37975   RMSE: 0.62      MAE: 0.33       Acc: 0.83, 0.96, 0.98, 0.99
    arm RMSE: 0.78  arm MAE: 0.46   Acc: 0.74, 0.94, 0.98, 0.99
    leg RMSE: 0.48  leg MAE: 0.27   Acc: 0.91, 0.97, 0.99, 0.99
    body RMSE: 0.51 body MAE: 0.23  Acc: 0.93, 0.99, 1.00, 1.00
    Done.




.. GENERATED FROM PYTHON SOURCE LINES 141-144

Load the Wipose_LSTM model
------------------------
# Method 1

.. GENERATED FROM PYTHON SOURCE LINES 144-151

.. code-block:: Python

    hpe_model = Wipose_LSTM(in_cha=4,out_cha=63).to(device)
    # model_path = 'path to trained model'
    # state_dict = torch.load(model_path,weights_only=True)

    # Method 2
    hpe_model = load_hpe_model('wipose',pretrained=True,task='subject8').to(device)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/kemove/anaconda3/envs/il/lib/python3.9/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
      warnings.warn("dropout option adds dropout after all but last "




.. GENERATED FROM PYTHON SOURCE LINES 152-154

Load the data
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 154-159

.. code-block:: Python

    csv = './data/hpe_dataset/csv/pose_regression_timeseries_subject_8/test.csv' # The path contains the samosa dataset
    hpe_testdataset = SoundPoseLSTMDataset(csv,sound_length=2400,input_feature='raw',mean=np.array(get_raw_mean()).astype("float32"),std=np.array(get_raw_std()).astype("float32"))
    index = 0 # Randomly select an index
    sample= hpe_testdataset.__getitem__(index)








.. GENERATED FROM PYTHON SOURCE LINES 160-163

Model inference
------------------------
Method 1

.. GENERATED FROM PYTHON SOURCE LINES 163-179

.. code-block:: Python

    hpe_model.eval()
    predicted_result = hpe_model(sample['sound'].unsqueeze(0).float().to(device))
    vis_images = make_images(sample['targets'],predicted_result.cpu().detach().squeeze(0))

    #Method 2
    from pysensing.acoustic.inference.predict import *
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    predicted_result  = hpe_predict(sample['sound'],'SoundPoseLSTMDataset',hpe_model, device=device)
    vis_images = make_images(sample['targets'].numpy(),predicted_result.cpu().detach().numpy().squeeze(0))

    seq_num = 2
    fig = plt.figure(figsize=(12, 12))
    plt.imshow(vis_images[seq_num]['img'])
    plt.axis('off')
    plt.show()




.. image-sg:: /acoustic/images/sphx_glr_acoustic_hpe_tutorial_002.png
   :alt: acoustic hpe tutorial
   :srcset: /acoustic/images/sphx_glr_acoustic_hpe_tutorial_002.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 180-182

Model embedding
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 182-186

.. code-block:: Python

    from pysensing.acoustic.inference.embedding import *
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    sample_embedding = hpe_embedding(sample['sound'],'SoundPoseLSTMDataset',hpe_model, device=device)








.. GENERATED FROM PYTHON SOURCE LINES 187-188

And that's it. We're done with our acoustic humna pose estimation tutorials. Thanks for reading.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (3 minutes 4.931 seconds)


.. _sphx_glr_download_acoustic_acoustic_hpe_tutorial.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: acoustic_hpe_tutorial.ipynb <acoustic_hpe_tutorial.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: acoustic_hpe_tutorial.py <acoustic_hpe_tutorial.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
