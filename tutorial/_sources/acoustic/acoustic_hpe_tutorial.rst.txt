
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "acoustic/acoustic_hpe_tutorial.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_acoustic_acoustic_hpe_tutorial.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_acoustic_acoustic_hpe_tutorial.py:


Acoustic Human Pose estimation Tutorial
==============================================================

.. GENERATED FROM PYTHON SOURCE LINES 7-8

!pip install pysensing

.. GENERATED FROM PYTHON SOURCE LINES 10-12

In this tutorial, we will be implementing codes for acoustic Human pose estimation


.. GENERATED FROM PYTHON SOURCE LINES 12-17

.. code-block:: Python


    from pysensing.acoustic.datasets.utils.hpe_vis import *
    from pysensing.acoustic.models.hpe import Speech2pose,Wipose_LSTM
    from pysensing.acoustic.models.get_model import load_hpe_model
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')







.. GENERATED FROM PYTHON SOURCE LINES 18-25

Listening Human Behavior: 3D Human Pose Estimation with Acoustic Signals
 ----------------------------------------------------------------------------------
Implementation of "Listening Human Behavior: 3D Human Pose Estimation with Acoustic Signals".

This dataset contains the audio reflected by human to estimate the 3D human pose with the acoustic signals.

Reference: https://github.com/YutoShibata07/AcousticPose_Public

.. GENERATED FROM PYTHON SOURCE LINES 27-29

Load the data
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 29-48

.. code-block:: Python


    # Method 1: Use get_dataloader
    from pysensing.acoustic.datasets.get_dataloader import *
    train_loader,val_loader,test_loader = load_hpe_dataset(
        root='./data',
        dataset_name='pose_regression_timeseries_subject_1',
        download=True)

    # Method 2
    csv = './data/hpe_dataset/csv/pose_regression_timeseries_subject_1/test.csv' # The path contains the samosa dataset
    data_dir = './data'
    hpe_testdataset = SoundPose2DDataset(csv,sound_length=2400,input_feature='logmel',
                                         mean=np.array(get_mean()).astype("float32")[:4],
                                         std=np.array(get_std()).astype("float32")[:4],
                                         )
    index = 10 # Randomly select an index
    sample= hpe_testdataset.__getitem__(index)
    print(sample['targets'].shape)
    print(sample['sound'].shape)




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    torch.Size([12, 63])
    torch.Size([4, 12, 128])




.. GENERATED FROM PYTHON SOURCE LINES 49-51

Load Speech2pose model
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 51-60

.. code-block:: Python


    # Method 1
    hpe_model = Speech2pose(out_cha=63).to(device)
    # model_path = 'path to pretrian weights'
    # state_dict = torch.load(model_path,weights_only=True)
    # hpe_model.load_state_dict(state_dict)

    # Method 2
    hpe_model = load_hpe_model('speech2pose',pretrained=True,task='subject8').to(device)







.. GENERATED FROM PYTHON SOURCE LINES 61-63

Modle Inference
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 63-82

.. code-block:: Python


    #Method 1
    sample= hpe_testdataset.__getitem__(index)
    hpe_model.eval()
    predicted_result = hpe_model(sample['sound'].unsqueeze(0).float().to(device))
    vis_images = make_images(sample['targets'].numpy(),predicted_result.cpu().detach().numpy().squeeze(0))

    #Method 2
    from pysensing.acoustic.inference.predict import *
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    predicted_result  = hpe_predict(sample['sound'],'SoundPose2DDataset',hpe_model, device=device)
    vis_images = make_images(sample['targets'].numpy(),predicted_result.cpu().detach().numpy().squeeze(0))

    seq_num = 0
    fig = plt.figure(figsize=(12, 12))
    plt.imshow(vis_images[seq_num]['img'])
    plt.axis('off')
    plt.show()




.. image-sg:: /acoustic/images/sphx_glr_acoustic_hpe_tutorial_001.png
   :alt: acoustic hpe tutorial
   :srcset: /acoustic/images/sphx_glr_acoustic_hpe_tutorial_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/kemove/anaconda3/envs/il/lib/python3.9/site-packages/torch/nn/modules/conv.py:306: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1008.)
      return F.conv1d(input, weight, bias, self.stride,




.. GENERATED FROM PYTHON SOURCE LINES 83-85

Modle Embedding
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 85-89

.. code-block:: Python

    from pysensing.acoustic.inference.embedding import *
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    sample_embedding = hpe_embedding(sample['sound'],'SoundPose2DDataset',hpe_model, device=device)








.. GENERATED FROM PYTHON SOURCE LINES 90-92

Modle Training
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 92-141

.. code-block:: Python

    from pysensing.acoustic.inference.training.AcousticPose_utils.hpe_train import train_model,generate_configs

    args = {
        "root_dir": "./data/hpe_dataset/testing_result",
        "save_name": "seq1",
        "input_feature": "logmel",
        "batchsize": 64,
        "max_epoch": 50,
        "csv_path": "./data/hpe_dataset/csv",
        "dataset_name": "pose_regression_timeseries_subject_1",
        "model": "speech2pose",
        "sound_length": 2400,
        "learning_rate": 0.01,
    }
    config_path = args["root_dir"]+'/'+args["save_name"]+"/config.yaml"
    generate_configs(args)
    resume_training = False
    random_seed = 0

    train_model(
        config_path=config_path,
        resume=resume_training,
        seed=random_seed,
    )

    # Modle Training
    # ------------------------
    from pysensing.acoustic.inference.training.AcousticPose_utils.hpe_test import evaluate_model
    args = {
        "root_dir": "./data/hpe_dataset/testing_result",
        "save_name": "seq1",
        "batchsize": 64,
        "max_epoch": 20,
        "csv_path": "./data/hpe_dataset/csv",
        "dataset_name": "pose_regression_timeseries_subject_1",
        "model": "speech2pose",
        "sound_length": 2400,
        "learning_rate": 0.01,
    }
    config_path = args["root_dir"]+'/'+args["save_name"]+"/config.yaml"
    evaluation_mode = "test"
    model_path = None

    evaluate_model(
        config_path=config_path,
        mode=evaluation_mode,
        model_path=model_path)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    {'csv_path': './data/hpe_dataset/csv', 'model': 'speech2pose', 'pretrained': True, 'use_class_weight': True, 'batch_size': 32, 'width': 224, 'height': 224, 'num_workers': 8, 'max_epoch': 50, 'learning_rate': 0.01, 'sound_length': 2400, 'dataset_name': 'pose_regression_timeseries_subject_1', 'input_feature': 'logmel', 'topk': (1, 3), 'smooth_loss': False, 'ratio': 0.0, 'gan': 'none', 'finetune': 0, 'aug': 'none'}
    Finished making configuration files.
    /home/kemove/anaconda3/envs/il/lib/python3.9/site-packages/pysensing/acoustic/inference/training/AcousticPose_utils/libs/dataset.py:195: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.
      self.df[self.df["sound_length"] == sound_length]
    /home/kemove/anaconda3/envs/il/lib/python3.9/site-packages/pysensing/acoustic/inference/training/AcousticPose_utils/libs/dataset.py:195: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.
      self.df[self.df["sound_length"] == sound_length]
    epoch: 0        epoch time[sec]: 4      train loss: 35.7390     val loss: 5554.8646     val_rmse: 74.53097      val_mae: 51.34270
    epoch: 1        epoch time[sec]: 1      train loss: 2.9727      val loss: 25.6959       val_rmse: 5.06912       val_mae: 3.25518
    epoch: 2        epoch time[sec]: 1      train loss: 1.4077      val loss: 2.6241        val_rmse: 1.61991       val_mae: 0.95808
    epoch: 3        epoch time[sec]: 1      train loss: 1.1356      val loss: 3.5195        val_rmse: 1.87604       val_mae: 1.04522
    epoch: 4        epoch time[sec]: 1      train loss: 0.9243      val loss: 1.5310        val_rmse: 1.23734       val_mae: 0.71184
    epoch: 5        epoch time[sec]: 1      train loss: 0.7955      val loss: 3.5594        val_rmse: 1.88662       val_mae: 1.16877
    epoch: 6        epoch time[sec]: 1      train loss: 0.8497      val loss: 4.6753        val_rmse: 2.16226       val_mae: 1.17781
    epoch: 7        epoch time[sec]: 1      train loss: 0.7119      val loss: 1.2523        val_rmse: 1.11907       val_mae: 0.63488
    epoch: 8        epoch time[sec]: 1      train loss: 0.8146      val loss: 1.8044        val_rmse: 1.34328       val_mae: 0.73590
    epoch: 9        epoch time[sec]: 1      train loss: 0.6545      val loss: 3.3827        val_rmse: 1.83922       val_mae: 1.10017
    epoch: 10       epoch time[sec]: 1      train loss: 0.5552      val loss: 1.1688        val_rmse: 1.08112       val_mae: 0.57759
    epoch: 11       epoch time[sec]: 1      train loss: 0.4592      val loss: 2.0147        val_rmse: 1.41939       val_mae: 0.74739
    epoch: 12       epoch time[sec]: 1      train loss: 0.4994      val loss: 0.9686        val_rmse: 0.98416       val_mae: 0.55964
    epoch: 13       epoch time[sec]: 1      train loss: 0.3855      val loss: 1.2417        val_rmse: 1.11432       val_mae: 0.62025
    epoch: 14       epoch time[sec]: 1      train loss: 0.4807      val loss: 12.4831       val_rmse: 3.53314       val_mae: 2.36653
    epoch: 15       epoch time[sec]: 1      train loss: 0.4118      val loss: 1.3485        val_rmse: 1.16123       val_mae: 0.66796
    epoch: 16       epoch time[sec]: 1      train loss: 0.3761      val loss: 0.9973        val_rmse: 0.99863       val_mae: 0.53796
    epoch: 17       epoch time[sec]: 1      train loss: 0.4024      val loss: 0.8047        val_rmse: 0.89707       val_mae: 0.49792
    epoch: 18       epoch time[sec]: 1      train loss: 0.3684      val loss: 0.9007        val_rmse: 0.94903       val_mae: 0.52189
    epoch: 19       epoch time[sec]: 1      train loss: 0.4444      val loss: 0.5412        val_rmse: 0.73564       val_mae: 0.38856
    epoch: 20       epoch time[sec]: 1      train loss: 0.4993      val loss: 3.9111        val_rmse: 1.97765       val_mae: 1.08059
    epoch: 21       epoch time[sec]: 1      train loss: 0.5329      val loss: 2.8935        val_rmse: 1.70104       val_mae: 1.00245
    epoch: 22       epoch time[sec]: 1      train loss: 0.5511      val loss: 5.5930        val_rmse: 2.36494       val_mae: 1.52599
    epoch: 23       epoch time[sec]: 1      train loss: 0.4637      val loss: 0.5622        val_rmse: 0.74982       val_mae: 0.43827
    epoch: 24       epoch time[sec]: 1      train loss: 0.3852      val loss: 0.5459        val_rmse: 0.73882       val_mae: 0.41697
    epoch: 25       epoch time[sec]: 1      train loss: 0.3494      val loss: 0.5928        val_rmse: 0.76992       val_mae: 0.43026
    epoch: 26       epoch time[sec]: 1      train loss: 0.3270      val loss: 1.0129        val_rmse: 1.00643       val_mae: 0.59031
    epoch: 27       epoch time[sec]: 1      train loss: 0.3271      val loss: 1.1278        val_rmse: 1.06199       val_mae: 0.58608
    epoch: 28       epoch time[sec]: 1      train loss: 0.3561      val loss: 0.8824        val_rmse: 0.93934       val_mae: 0.53670
    epoch: 29       epoch time[sec]: 1      train loss: 0.4345      val loss: 2.2521        val_rmse: 1.50071       val_mae: 0.77469
    epoch: 30       epoch time[sec]: 1      train loss: 0.3261      val loss: 0.5565        val_rmse: 0.74597       val_mae: 0.42798
    epoch: 31       epoch time[sec]: 1      train loss: 0.2789      val loss: 0.2856        val_rmse: 0.53439       val_mae: 0.29199
    epoch: 32       epoch time[sec]: 1      train loss: 0.2797      val loss: 0.8026        val_rmse: 0.89586       val_mae: 0.49548
    epoch: 33       epoch time[sec]: 1      train loss: 0.2679      val loss: 0.3465        val_rmse: 0.58865       val_mae: 0.31378
    epoch: 34       epoch time[sec]: 1      train loss: 0.2232      val loss: 0.3359        val_rmse: 0.57957       val_mae: 0.35535
    epoch: 35       epoch time[sec]: 1      train loss: 0.2356      val loss: 0.4082        val_rmse: 0.63887       val_mae: 0.36426
    epoch: 36       epoch time[sec]: 1      train loss: 0.2611      val loss: 0.5367        val_rmse: 0.73261       val_mae: 0.40534
    epoch: 37       epoch time[sec]: 1      train loss: 0.2392      val loss: 0.6087        val_rmse: 0.78019       val_mae: 0.44737
    epoch: 38       epoch time[sec]: 1      train loss: 0.2305      val loss: 0.4172        val_rmse: 0.64594       val_mae: 0.32282
    epoch: 39       epoch time[sec]: 1      train loss: 0.2442      val loss: 0.2554        val_rmse: 0.50535       val_mae: 0.28782
    epoch: 40       epoch time[sec]: 1      train loss: 0.2222      val loss: 0.2548        val_rmse: 0.50480       val_mae: 0.30691
    epoch: 41       epoch time[sec]: 1      train loss: 0.2488      val loss: 0.5324        val_rmse: 0.72965       val_mae: 0.37258
    epoch: 42       epoch time[sec]: 1      train loss: 0.1904      val loss: 0.2356        val_rmse: 0.48537       val_mae: 0.26606
    epoch: 43       epoch time[sec]: 1      train loss: 0.2384      val loss: 0.3041        val_rmse: 0.55143       val_mae: 0.30847
    epoch: 44       epoch time[sec]: 1      train loss: 0.3045      val loss: 1.0510        val_rmse: 1.02519       val_mae: 0.52256
    epoch: 45       epoch time[sec]: 1      train loss: 0.3366      val loss: 0.6302        val_rmse: 0.79388       val_mae: 0.45900
    epoch: 46       epoch time[sec]: 1      train loss: 0.3774      val loss: 1.3712        val_rmse: 1.17100       val_mae: 0.64678
    epoch: 47       epoch time[sec]: 1      train loss: 0.3146      val loss: 2.0887        val_rmse: 1.44523       val_mae: 0.88037
    epoch: 48       epoch time[sec]: 1      train loss: 0.5820      val loss: 5.6748        val_rmse: 2.38218       val_mae: 1.50417
    epoch: 49       epoch time[sec]: 1      train loss: 0.5284      val loss: 1.1686        val_rmse: 1.08104       val_mae: 0.58750
    /home/kemove/anaconda3/envs/il/lib/python3.9/site-packages/pysensing/acoustic/inference/training/AcousticPose_utils/libs/dataset.py:195: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.
      self.df[self.df["sound_length"] == sound_length]
    ---------- Start evaluation for test data ----------
    loss: 0.23558   RMSE: 0.49      MAE: 0.27       Acc: 0.87, 0.97, 0.99, 0.99
    arm RMSE: 0.66  arm MAE: 0.39   Acc: 0.80, 0.95, 0.98, 0.99
    leg RMSE: 0.35  leg MAE: 0.20   Acc: 0.95, 0.99, 0.99, 1.00
    body RMSE: 0.32 body MAE: 0.17  Acc: 0.94, 1.00, 1.00, 1.00
    Done.




.. GENERATED FROM PYTHON SOURCE LINES 142-144

Load the Wipose_LSTM model
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 144-153

.. code-block:: Python


    # Method 1
    hpe_model = Wipose_LSTM(in_cha=4,out_cha=63).to(device)
    # model_path = 'path to trained model'
    # state_dict = torch.load(model_path,weights_only=True)

    # Method 2
    hpe_model = load_hpe_model('wipose',pretrained=True,task='subject8').to(device)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/kemove/anaconda3/envs/il/lib/python3.9/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
      warnings.warn("dropout option adds dropout after all but last "




.. GENERATED FROM PYTHON SOURCE LINES 154-156

Load the data
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 156-161

.. code-block:: Python

    csv = './data/hpe_dataset/csv/pose_regression_timeseries_subject_8/test.csv' # The path contains the samosa dataset
    hpe_testdataset = SoundPoseLSTMDataset(csv,sound_length=2400,input_feature='raw',mean=np.array(get_raw_mean()).astype("float32"),std=np.array(get_raw_std()).astype("float32"))
    index = 0 # Randomly select an index
    sample= hpe_testdataset.__getitem__(index)








.. GENERATED FROM PYTHON SOURCE LINES 162-164

Model inference
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 164-182

.. code-block:: Python


    # Method 1
    hpe_model.eval()
    predicted_result = hpe_model(sample['sound'].unsqueeze(0).float().to(device))
    vis_images = make_images(sample['targets'],predicted_result.cpu().detach().squeeze(0))

    #Method 2
    from pysensing.acoustic.inference.predict import *
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    predicted_result  = hpe_predict(sample['sound'],'SoundPoseLSTMDataset',hpe_model, device=device)
    vis_images = make_images(sample['targets'].numpy(),predicted_result.cpu().detach().numpy().squeeze(0))

    seq_num = 2
    fig = plt.figure(figsize=(12, 12))
    plt.imshow(vis_images[seq_num]['img'])
    plt.axis('off')
    plt.show()




.. image-sg:: /acoustic/images/sphx_glr_acoustic_hpe_tutorial_002.png
   :alt: acoustic hpe tutorial
   :srcset: /acoustic/images/sphx_glr_acoustic_hpe_tutorial_002.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 183-185

Model embedding
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 185-189

.. code-block:: Python

    from pysensing.acoustic.inference.embedding import *
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    sample_embedding = hpe_embedding(sample['sound'],'SoundPoseLSTMDataset',hpe_model, device=device)








.. GENERATED FROM PYTHON SOURCE LINES 190-191

And that's it. We're done with our acoustic humna pose estimation tutorials. Thanks for reading.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (3 minutes 13.995 seconds)


.. _sphx_glr_download_acoustic_acoustic_hpe_tutorial.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: acoustic_hpe_tutorial.ipynb <acoustic_hpe_tutorial.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: acoustic_hpe_tutorial.py <acoustic_hpe_tutorial.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
