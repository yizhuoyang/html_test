
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "acoustic/acoustic_hpe_tutorial.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_acoustic_acoustic_hpe_tutorial.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_acoustic_acoustic_hpe_tutorial.py:


Acoustic Human Pose estimation Tutorial
==============================================================

.. GENERATED FROM PYTHON SOURCE LINES 7-8

!pip install pysensing

.. GENERATED FROM PYTHON SOURCE LINES 10-12

In this tutorial, we will be implementing codes for acoustic Human pose estimation


.. GENERATED FROM PYTHON SOURCE LINES 12-17

.. code-block:: Python


    from pysensing.acoustic.datasets.utils.hpe_vis import *
    from pysensing.acoustic.models.hpe import Speech2pose,Wipose_LSTM
    from pysensing.acoustic.models.get_model import load_hpe_model
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')







.. GENERATED FROM PYTHON SOURCE LINES 18-25

Listening Human Behavior: 3D Human Pose Estimation with Acoustic Signals
 ----------------------------------------------------------------------------------
Implementation of "Listening Human Behavior: 3D Human Pose Estimation with Acoustic Signals".

This dataset contains the audio reflected by human to estimate the 3D human pose with the acoustic signals.

Reference: https://github.com/YutoShibata07/AcousticPose_Public

.. GENERATED FROM PYTHON SOURCE LINES 27-29

Load the data
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 29-48

.. code-block:: Python


    # Method 1: Use get_dataloader
    from pysensing.acoustic.datasets.get_dataloader import *
    train_loader,val_loader,test_loader = load_hpe_dataset(
        root='./data',
        dataset_name='pose_regression_timeseries_subject_1',
        download=True)

    # Method 2
    csv = './data/hpe_dataset/csv/pose_regression_timeseries_subject_1/test.csv' # The path contains the samosa dataset
    data_dir = './data'
    hpe_testdataset = SoundPose2DDataset(csv,sound_length=2400,input_feature='logmel',
                                         mean=np.array(get_mean()).astype("float32")[:4],
                                         std=np.array(get_std()).astype("float32")[:4],
                                         )
    index = 10 # Randomly select an index
    sample= hpe_testdataset.__getitem__(index)
    print(sample['targets'].shape)
    print(sample['sound'].shape)




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    torch.Size([12, 63])
    torch.Size([4, 12, 128])




.. GENERATED FROM PYTHON SOURCE LINES 49-51

Load Speech2pose model
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 51-60

.. code-block:: Python


    # Method 1
    hpe_model = Speech2pose(out_cha=63).to(device)
    # model_path = 'path to pretrian weights'
    # state_dict = torch.load(model_path,weights_only=True)
    # hpe_model.load_state_dict(state_dict)

    # Method 2
    hpe_model = load_hpe_model('speech2pose',pretrained=True,task='subject8').to(device)







.. GENERATED FROM PYTHON SOURCE LINES 61-63

Modle Inference
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 63-82

.. code-block:: Python


    #Method 1
    sample= hpe_testdataset.__getitem__(index)
    hpe_model.eval()
    predicted_result = hpe_model(sample['sound'].unsqueeze(0).float().to(device))
    vis_images = make_images(sample['targets'].numpy(),predicted_result.cpu().detach().numpy().squeeze(0))

    #Method 2
    from pysensing.acoustic.inference.predict import *
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    predicted_result  = hpe_predict(sample['sound'],'SoundPose2DDataset',hpe_model, device=device)
    vis_images = make_images(sample['targets'].numpy(),predicted_result.cpu().detach().numpy().squeeze(0))

    seq_num = 0
    fig = plt.figure(figsize=(12, 12))
    plt.imshow(vis_images[seq_num]['img'])
    plt.axis('off')
    plt.show()




.. image-sg:: /acoustic/images/sphx_glr_acoustic_hpe_tutorial_001.png
   :alt: acoustic hpe tutorial
   :srcset: /acoustic/images/sphx_glr_acoustic_hpe_tutorial_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/kemove/anaconda3/envs/open-mmlab/lib/python3.8/site-packages/torch/nn/modules/conv.py:304: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1031.)
      return F.conv1d(input, weight, bias, self.stride,




.. GENERATED FROM PYTHON SOURCE LINES 83-85

Modle Embedding
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 85-89

.. code-block:: Python

    from pysensing.acoustic.inference.embedding import *
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    sample_embedding = hpe_embedding(sample['sound'],'SoundPose2DDataset',hpe_model, device=device)








.. GENERATED FROM PYTHON SOURCE LINES 90-92

Modle Training
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 92-141

.. code-block:: Python

    from pysensing.acoustic.inference.training.AcousticPose_utils.hpe_train import train_model,generate_configs

    args = {
        "root_dir": "./data/hpe_dataset/testing_result",
        "save_name": "seq1",
        "input_feature": "logmel",
        "batchsize": 64,
        "max_epoch": 50,
        "csv_path": "./data/hpe_dataset/csv",
        "dataset_name": "pose_regression_timeseries_subject_1",
        "model": "speech2pose",
        "sound_length": 2400,
        "learning_rate": 0.01,
    }
    config_path = args["root_dir"]+'/'+args["save_name"]+"/config.yaml"
    generate_configs(args)
    resume_training = False
    random_seed = 0

    train_model(
        config_path=config_path,
        resume=resume_training,
        seed=random_seed,
    )

    # Modle Training
    # ------------------------
    from pysensing.acoustic.inference.training.AcousticPose_utils.hpe_test import evaluate_model
    args = {
        "root_dir": "./data/hpe_dataset/testing_result",
        "save_name": "seq1",
        "batchsize": 64,
        "max_epoch": 20,
        "csv_path": "./data/hpe_dataset/csv",
        "dataset_name": "pose_regression_timeseries_subject_1",
        "model": "speech2pose",
        "sound_length": 2400,
        "learning_rate": 0.01,
    }
    config_path = args["root_dir"]+'/'+args["save_name"]+"/config.yaml"
    evaluation_mode = "test"
    model_path = None

    evaluate_model(
        config_path=config_path,
        mode=evaluation_mode,
        model_path=model_path)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    {'csv_path': './data/hpe_dataset/csv', 'model': 'speech2pose', 'pretrained': True, 'use_class_weight': True, 'batch_size': 32, 'width': 224, 'height': 224, 'num_workers': 8, 'max_epoch': 50, 'learning_rate': 0.01, 'sound_length': 2400, 'dataset_name': 'pose_regression_timeseries_subject_1', 'input_feature': 'logmel', 'topk': (1, 3), 'smooth_loss': False, 'ratio': 0.0, 'gan': 'none', 'finetune': 0, 'aug': 'none'}
    Finished making configuration files.
    INFO - 2024-11-20 11:28:14,093 - seed - Finished setting up seed.
    INFO - 2024-11-20 11:28:14,094 - config - Experiment Configuration
    {'aug': 'none',
     'batch_size': 32,
     'csv_path': './data/hpe_dataset/csv',
     'dataset_name': 'pose_regression_timeseries_subject_1',
     'finetune': 0,
     'gan': 'none',
     'height': 224,
     'input_feature': 'logmel',
     'learning_rate': 0.01,
     'max_epoch': 50,
     'model': 'speech2pose',
     'num_workers': 8,
     'pretrained': True,
     'ratio': 0.0,
     'smooth_loss': False,
     'sound_length': 2400,
     'topk': (1,
              3),
     'use_class_weight': True,
     'width': 224}
    INFO - 2024-11-20 11:28:14,094 - config - successfully loaded configuration.
    INFO - 2024-11-20 11:28:14,094 - dataset - Dataset: pose_regression_timeseries_subject_1        Split: train    Batch size: 32.
    INFO - 2024-11-20 11:28:14,094 - mean_std - mean value: [-23.539913, -25.119139, -26.711174, -22.011652, 0.0044820304, 0.00080580305, -0.040803947]
    INFO - 2024-11-20 11:28:14,094 - mean_std - std value: [12.967929, 10.571223, 10.273731, 11.8457985, 0.022749536, 0.019968906, 0.037085325]
    INFO - 2024-11-20 11:28:14,918 - dataset - the number of samples: 9708
    INFO - 2024-11-20 11:28:14,918 - dataset - Dataset: pose_regression_timeseries_subject_1        Split: val      Batch size: 1.
    INFO - 2024-11-20 11:28:14,918 - mean_std - mean value: [-23.539913, -25.119139, -26.711174, -22.011652, 0.0044820304, 0.00080580305, -0.040803947]
    INFO - 2024-11-20 11:28:14,918 - mean_std - std value: [12.967929, 10.571223, 10.273731, 11.8457985, 0.022749536, 0.019968906, 0.037085325]
    INFO - 2024-11-20 11:28:15,124 - dataset - the number of samples: 2424
    INFO - 2024-11-20 11:28:15,125 - __init__ - speech2pose will be used as a model.
    INFO - 2024-11-20 11:28:15,148 - hpe_train - Start training.
    INFO - 2024-11-20 11:28:20,287 - logger - epoch: 0      epoch time[sec]: 4      lr: 0.01        train loss: 35.9078     val loss: 49.4077       val_rmse: 7.02906       val_mae: 4.15302
    epoch: 0        epoch time[sec]: 4      train loss: 35.9078     val loss: 49.4077       val_rmse: 7.02906       val_mae: 4.15302
    INFO - 2024-11-20 11:28:22,737 - logger - epoch: 1      epoch time[sec]: 1      lr: 0.01        train loss: 3.2186      val loss: 5.1915        val_rmse: 2.27848       val_mae: 1.30127
    epoch: 1        epoch time[sec]: 1      train loss: 3.2186      val loss: 5.1915        val_rmse: 2.27848       val_mae: 1.30127
    INFO - 2024-11-20 11:28:25,317 - logger - epoch: 2      epoch time[sec]: 1      lr: 0.01        train loss: 1.7873      val loss: 2.0800        val_rmse: 1.44220       val_mae: 0.88617
    epoch: 2        epoch time[sec]: 1      train loss: 1.7873      val loss: 2.0800        val_rmse: 1.44220       val_mae: 0.88617
    INFO - 2024-11-20 11:28:27,934 - logger - epoch: 3      epoch time[sec]: 1      lr: 0.01        train loss: 1.3931      val loss: 2.1673        val_rmse: 1.47219       val_mae: 0.89062
    epoch: 3        epoch time[sec]: 1      train loss: 1.3931      val loss: 2.1673        val_rmse: 1.47219       val_mae: 0.89062
    INFO - 2024-11-20 11:28:30,402 - logger - epoch: 4      epoch time[sec]: 1      lr: 0.01        train loss: 1.3545      val loss: 2.2021        val_rmse: 1.48395       val_mae: 0.89224
    epoch: 4        epoch time[sec]: 1      train loss: 1.3545      val loss: 2.2021        val_rmse: 1.48395       val_mae: 0.89224
    INFO - 2024-11-20 11:28:33,262 - logger - epoch: 5      epoch time[sec]: 1      lr: 0.01        train loss: 1.2792      val loss: 1.3940        val_rmse: 1.18067       val_mae: 0.75314
    epoch: 5        epoch time[sec]: 1      train loss: 1.2792      val loss: 1.3940        val_rmse: 1.18067       val_mae: 0.75314
    INFO - 2024-11-20 11:28:37,502 - logger - epoch: 6      epoch time[sec]: 1      lr: 0.01        train loss: 1.3827      val loss: 1.4903        val_rmse: 1.22078       val_mae: 0.70531
    epoch: 6        epoch time[sec]: 1      train loss: 1.3827      val loss: 1.4903        val_rmse: 1.22078       val_mae: 0.70531
    INFO - 2024-11-20 11:28:41,280 - logger - epoch: 7      epoch time[sec]: 1      lr: 0.01        train loss: 1.1380      val loss: 1.3884        val_rmse: 1.17830       val_mae: 0.75178
    epoch: 7        epoch time[sec]: 1      train loss: 1.1380      val loss: 1.3884        val_rmse: 1.17830       val_mae: 0.75178
    INFO - 2024-11-20 11:28:43,999 - logger - epoch: 8      epoch time[sec]: 1      lr: 0.01        train loss: 1.2759      val loss: 3.4145        val_rmse: 1.84783       val_mae: 0.99689
    epoch: 8        epoch time[sec]: 1      train loss: 1.2759      val loss: 3.4145        val_rmse: 1.84783       val_mae: 0.99689
    INFO - 2024-11-20 11:28:46,769 - logger - epoch: 9      epoch time[sec]: 1      lr: 0.01        train loss: 1.2372      val loss: 7.3385        val_rmse: 2.70897       val_mae: 1.73976
    epoch: 9        epoch time[sec]: 1      train loss: 1.2372      val loss: 7.3385        val_rmse: 2.70897       val_mae: 1.73976
    INFO - 2024-11-20 11:28:49,315 - logger - epoch: 10     epoch time[sec]: 1      lr: 0.01        train loss: 1.1067      val loss: 2.0695        val_rmse: 1.43859       val_mae: 0.87368
    epoch: 10       epoch time[sec]: 1      train loss: 1.1067      val loss: 2.0695        val_rmse: 1.43859       val_mae: 0.87368
    INFO - 2024-11-20 11:28:51,860 - logger - epoch: 11     epoch time[sec]: 1      lr: 0.01        train loss: 1.0034      val loss: 1.5623        val_rmse: 1.24992       val_mae: 0.71251
    epoch: 11       epoch time[sec]: 1      train loss: 1.0034      val loss: 1.5623        val_rmse: 1.24992       val_mae: 0.71251
    INFO - 2024-11-20 11:28:54,337 - logger - epoch: 12     epoch time[sec]: 1      lr: 0.01        train loss: 0.9604      val loss: 1.7297        val_rmse: 1.31517       val_mae: 0.73883
    epoch: 12       epoch time[sec]: 1      train loss: 0.9604      val loss: 1.7297        val_rmse: 1.31517       val_mae: 0.73883
    INFO - 2024-11-20 11:28:56,880 - logger - epoch: 13     epoch time[sec]: 1      lr: 0.01        train loss: 0.7869      val loss: 1.7395        val_rmse: 1.31890       val_mae: 0.71431
    epoch: 13       epoch time[sec]: 1      train loss: 0.7869      val loss: 1.7395        val_rmse: 1.31890       val_mae: 0.71431
    INFO - 2024-11-20 11:28:59,431 - logger - epoch: 14     epoch time[sec]: 1      lr: 0.01        train loss: 0.8848      val loss: 3.3268        val_rmse: 1.82395       val_mae: 1.01679
    epoch: 14       epoch time[sec]: 1      train loss: 0.8848      val loss: 3.3268        val_rmse: 1.82395       val_mae: 1.01679
    INFO - 2024-11-20 11:29:01,905 - logger - epoch: 15     epoch time[sec]: 1      lr: 0.01        train loss: 0.7021      val loss: 1.3354        val_rmse: 1.15559       val_mae: 0.66640
    epoch: 15       epoch time[sec]: 1      train loss: 0.7021      val loss: 1.3354        val_rmse: 1.15559       val_mae: 0.66640
    INFO - 2024-11-20 11:29:04,488 - logger - epoch: 16     epoch time[sec]: 1      lr: 0.01        train loss: 0.6145      val loss: 2.7139        val_rmse: 1.64740       val_mae: 0.94755
    epoch: 16       epoch time[sec]: 1      train loss: 0.6145      val loss: 2.7139        val_rmse: 1.64740       val_mae: 0.94755
    INFO - 2024-11-20 11:29:07,013 - logger - epoch: 17     epoch time[sec]: 1      lr: 0.01        train loss: 0.6034      val loss: 0.9290        val_rmse: 0.96386       val_mae: 0.54778
    epoch: 17       epoch time[sec]: 1      train loss: 0.6034      val loss: 0.9290        val_rmse: 0.96386       val_mae: 0.54778
    INFO - 2024-11-20 11:29:09,586 - logger - epoch: 18     epoch time[sec]: 1      lr: 0.01        train loss: 0.5561      val loss: 1.1131        val_rmse: 1.05501       val_mae: 0.62210
    epoch: 18       epoch time[sec]: 1      train loss: 0.5561      val loss: 1.1131        val_rmse: 1.05501       val_mae: 0.62210
    INFO - 2024-11-20 11:29:12,070 - logger - epoch: 19     epoch time[sec]: 1      lr: 0.01        train loss: 0.6654      val loss: 3.0080        val_rmse: 1.73435       val_mae: 1.06267
    epoch: 19       epoch time[sec]: 1      train loss: 0.6654      val loss: 3.0080        val_rmse: 1.73435       val_mae: 1.06267
    INFO - 2024-11-20 11:29:15,016 - logger - epoch: 20     epoch time[sec]: 1      lr: 0.01        train loss: 0.5328      val loss: 2.1986        val_rmse: 1.48275       val_mae: 0.90172
    epoch: 20       epoch time[sec]: 1      train loss: 0.5328      val loss: 2.1986        val_rmse: 1.48275       val_mae: 0.90172
    INFO - 2024-11-20 11:29:17,868 - logger - epoch: 21     epoch time[sec]: 1      lr: 0.01        train loss: 0.4290      val loss: 1.2125        val_rmse: 1.10113       val_mae: 0.63939
    epoch: 21       epoch time[sec]: 1      train loss: 0.4290      val loss: 1.2125        val_rmse: 1.10113       val_mae: 0.63939
    INFO - 2024-11-20 11:29:21,281 - logger - epoch: 22     epoch time[sec]: 1      lr: 0.01        train loss: 0.4415      val loss: 0.8623        val_rmse: 0.92863       val_mae: 0.48014
    epoch: 22       epoch time[sec]: 1      train loss: 0.4415      val loss: 0.8623        val_rmse: 0.92863       val_mae: 0.48014
    INFO - 2024-11-20 11:29:24,821 - logger - epoch: 23     epoch time[sec]: 1      lr: 0.01        train loss: 0.4289      val loss: 1.4516        val_rmse: 1.20482       val_mae: 0.68310
    epoch: 23       epoch time[sec]: 1      train loss: 0.4289      val loss: 1.4516        val_rmse: 1.20482       val_mae: 0.68310
    INFO - 2024-11-20 11:29:28,180 - logger - epoch: 24     epoch time[sec]: 1      lr: 0.01        train loss: 0.4061      val loss: 1.3787        val_rmse: 1.17419       val_mae: 0.68952
    epoch: 24       epoch time[sec]: 1      train loss: 0.4061      val loss: 1.3787        val_rmse: 1.17419       val_mae: 0.68952
    INFO - 2024-11-20 11:29:31,329 - logger - epoch: 25     epoch time[sec]: 1      lr: 0.01        train loss: 0.3865      val loss: 0.7943        val_rmse: 0.89126       val_mae: 0.45662
    epoch: 25       epoch time[sec]: 1      train loss: 0.3865      val loss: 0.7943        val_rmse: 0.89126       val_mae: 0.45662
    INFO - 2024-11-20 11:29:33,920 - logger - epoch: 26     epoch time[sec]: 1      lr: 0.01        train loss: 0.3832      val loss: 1.1497        val_rmse: 1.07224       val_mae: 0.55443
    epoch: 26       epoch time[sec]: 1      train loss: 0.3832      val loss: 1.1497        val_rmse: 1.07224       val_mae: 0.55443
    INFO - 2024-11-20 11:29:36,411 - logger - epoch: 27     epoch time[sec]: 1      lr: 0.01        train loss: 0.3956      val loss: 0.9122        val_rmse: 0.95508       val_mae: 0.51790
    epoch: 27       epoch time[sec]: 1      train loss: 0.3956      val loss: 0.9122        val_rmse: 0.95508       val_mae: 0.51790
    INFO - 2024-11-20 11:29:39,873 - logger - epoch: 28     epoch time[sec]: 1      lr: 0.01        train loss: 0.3577      val loss: 1.4741        val_rmse: 1.21414       val_mae: 0.62630
    epoch: 28       epoch time[sec]: 1      train loss: 0.3577      val loss: 1.4741        val_rmse: 1.21414       val_mae: 0.62630
    INFO - 2024-11-20 11:29:42,881 - logger - epoch: 29     epoch time[sec]: 1      lr: 0.01        train loss: 0.4601      val loss: 4.6237        val_rmse: 2.15028       val_mae: 1.28956
    epoch: 29       epoch time[sec]: 1      train loss: 0.4601      val loss: 4.6237        val_rmse: 2.15028       val_mae: 1.28956
    INFO - 2024-11-20 11:29:46,064 - logger - epoch: 30     epoch time[sec]: 1      lr: 0.01        train loss: 0.4339      val loss: 3.5612        val_rmse: 1.88711       val_mae: 1.08993
    epoch: 30       epoch time[sec]: 1      train loss: 0.4339      val loss: 3.5612        val_rmse: 1.88711       val_mae: 1.08993
    INFO - 2024-11-20 11:29:48,939 - logger - epoch: 31     epoch time[sec]: 1      lr: 0.01        train loss: 0.4145      val loss: 1.6004        val_rmse: 1.26508       val_mae: 0.68883
    epoch: 31       epoch time[sec]: 1      train loss: 0.4145      val loss: 1.6004        val_rmse: 1.26508       val_mae: 0.68883
    INFO - 2024-11-20 11:29:51,428 - logger - epoch: 32     epoch time[sec]: 1      lr: 0.01        train loss: 0.3959      val loss: 0.8364        val_rmse: 0.91455       val_mae: 0.50837
    epoch: 32       epoch time[sec]: 1      train loss: 0.3959      val loss: 0.8364        val_rmse: 0.91455       val_mae: 0.50837
    INFO - 2024-11-20 11:29:53,992 - logger - epoch: 33     epoch time[sec]: 1      lr: 0.01        train loss: 0.3402      val loss: 0.8699        val_rmse: 0.93267       val_mae: 0.49092
    epoch: 33       epoch time[sec]: 1      train loss: 0.3402      val loss: 0.8699        val_rmse: 0.93267       val_mae: 0.49092
    INFO - 2024-11-20 11:29:56,591 - logger - epoch: 34     epoch time[sec]: 1      lr: 0.01        train loss: 0.2769      val loss: 0.4296        val_rmse: 0.65545       val_mae: 0.38494
    epoch: 34       epoch time[sec]: 1      train loss: 0.2769      val loss: 0.4296        val_rmse: 0.65545       val_mae: 0.38494
    INFO - 2024-11-20 11:30:00,179 - logger - epoch: 35     epoch time[sec]: 1      lr: 0.01        train loss: 0.2707      val loss: 0.3649        val_rmse: 0.60406       val_mae: 0.33845
    epoch: 35       epoch time[sec]: 1      train loss: 0.2707      val loss: 0.3649        val_rmse: 0.60406       val_mae: 0.33845
    INFO - 2024-11-20 11:30:04,307 - logger - epoch: 36     epoch time[sec]: 1      lr: 0.01        train loss: 0.2979      val loss: 1.6681        val_rmse: 1.29155       val_mae: 0.71848
    epoch: 36       epoch time[sec]: 1      train loss: 0.2979      val loss: 1.6681        val_rmse: 1.29155       val_mae: 0.71848
    INFO - 2024-11-20 11:30:06,846 - logger - epoch: 37     epoch time[sec]: 1      lr: 0.01        train loss: 0.2614      val loss: 0.3439        val_rmse: 0.58643       val_mae: 0.33869
    epoch: 37       epoch time[sec]: 1      train loss: 0.2614      val loss: 0.3439        val_rmse: 0.58643       val_mae: 0.33869
    INFO - 2024-11-20 11:30:09,459 - logger - epoch: 38     epoch time[sec]: 1      lr: 0.01        train loss: 0.2332      val loss: 0.5017        val_rmse: 0.70829       val_mae: 0.39683
    epoch: 38       epoch time[sec]: 1      train loss: 0.2332      val loss: 0.5017        val_rmse: 0.70829       val_mae: 0.39683
    INFO - 2024-11-20 11:30:11,893 - logger - epoch: 39     epoch time[sec]: 1      lr: 0.01        train loss: 0.2718      val loss: 0.5848        val_rmse: 0.76472       val_mae: 0.40973
    epoch: 39       epoch time[sec]: 1      train loss: 0.2718      val loss: 0.5848        val_rmse: 0.76472       val_mae: 0.40973
    INFO - 2024-11-20 11:30:14,413 - logger - epoch: 40     epoch time[sec]: 1      lr: 0.01        train loss: 0.2550      val loss: 0.2871        val_rmse: 0.53581       val_mae: 0.31498
    epoch: 40       epoch time[sec]: 1      train loss: 0.2550      val loss: 0.2871        val_rmse: 0.53581       val_mae: 0.31498
    INFO - 2024-11-20 11:30:17,000 - logger - epoch: 41     epoch time[sec]: 1      lr: 0.01        train loss: 0.2608      val loss: 0.5654        val_rmse: 0.75194       val_mae: 0.40433
    epoch: 41       epoch time[sec]: 1      train loss: 0.2608      val loss: 0.5654        val_rmse: 0.75194       val_mae: 0.40433
    INFO - 2024-11-20 11:30:19,745 - logger - epoch: 42     epoch time[sec]: 1      lr: 0.01        train loss: 0.2088      val loss: 0.2874        val_rmse: 0.53612       val_mae: 0.28228
    epoch: 42       epoch time[sec]: 1      train loss: 0.2088      val loss: 0.2874        val_rmse: 0.53612       val_mae: 0.28228
    INFO - 2024-11-20 11:30:23,103 - logger - epoch: 43     epoch time[sec]: 1      lr: 0.01        train loss: 0.2524      val loss: 0.3326        val_rmse: 0.57668       val_mae: 0.31789
    epoch: 43       epoch time[sec]: 1      train loss: 0.2524      val loss: 0.3326        val_rmse: 0.57668       val_mae: 0.31789
    INFO - 2024-11-20 11:30:26,108 - logger - epoch: 44     epoch time[sec]: 1      lr: 0.01        train loss: 0.2489      val loss: 0.3601        val_rmse: 0.60012       val_mae: 0.33366
    epoch: 44       epoch time[sec]: 1      train loss: 0.2489      val loss: 0.3601        val_rmse: 0.60012       val_mae: 0.33366
    INFO - 2024-11-20 11:30:28,611 - logger - epoch: 45     epoch time[sec]: 1      lr: 0.01        train loss: 0.2576      val loss: 0.7245        val_rmse: 0.85120       val_mae: 0.46228
    epoch: 45       epoch time[sec]: 1      train loss: 0.2576      val loss: 0.7245        val_rmse: 0.85120       val_mae: 0.46228
    INFO - 2024-11-20 11:30:31,099 - logger - epoch: 46     epoch time[sec]: 1      lr: 0.01        train loss: 0.2921      val loss: 0.7315        val_rmse: 0.85530       val_mae: 0.45723
    epoch: 46       epoch time[sec]: 1      train loss: 0.2921      val loss: 0.7315        val_rmse: 0.85530       val_mae: 0.45723
    INFO - 2024-11-20 11:30:33,641 - logger - epoch: 47     epoch time[sec]: 1      lr: 0.01        train loss: 0.2708      val loss: 0.6067        val_rmse: 0.77891       val_mae: 0.45281
    epoch: 47       epoch time[sec]: 1      train loss: 0.2708      val loss: 0.6067        val_rmse: 0.77891       val_mae: 0.45281
    INFO - 2024-11-20 11:30:36,147 - logger - epoch: 48     epoch time[sec]: 1      lr: 0.01        train loss: 0.3275      val loss: 0.6822        val_rmse: 0.82595       val_mae: 0.46309
    epoch: 48       epoch time[sec]: 1      train loss: 0.3275      val loss: 0.6822        val_rmse: 0.82595       val_mae: 0.46309
    INFO - 2024-11-20 11:30:38,607 - logger - epoch: 49     epoch time[sec]: 1      lr: 0.01        train loss: 0.2724      val loss: 0.6006        val_rmse: 0.77498       val_mae: 0.41831
    epoch: 49       epoch time[sec]: 1      train loss: 0.2724      val loss: 0.6006        val_rmse: 0.77498       val_mae: 0.41831
    INFO - 2024-11-20 11:30:40,483 - hpe_train - Done
    INFO - 2024-11-20 11:30:40,487 - config - Experiment Configuration
    {'aug': 'none',
     'batch_size': 32,
     'csv_path': './data/hpe_dataset/csv',
     'dataset_name': 'pose_regression_timeseries_subject_1',
     'finetune': 0,
     'gan': 'none',
     'height': 224,
     'input_feature': 'logmel',
     'learning_rate': 0.01,
     'max_epoch': 50,
     'model': 'speech2pose',
     'num_workers': 8,
     'pretrained': True,
     'ratio': 0.0,
     'smooth_loss': False,
     'sound_length': 2400,
     'topk': (1,
              3),
     'use_class_weight': True,
     'width': 224}
    INFO - 2024-11-20 11:30:40,487 - config - successfully loaded configuration.
    INFO - 2024-11-20 11:30:40,487 - dataset - Dataset: pose_regression_timeseries_subject_1        Split: test     Batch size: 1.
    INFO - 2024-11-20 11:30:40,487 - mean_std - mean value: [-23.539913, -25.119139, -26.711174, -22.011652, 0.0044820304, 0.00080580305, -0.040803947]
    INFO - 2024-11-20 11:30:40,488 - mean_std - std value: [12.967929, 10.571223, 10.273731, 11.8457985, 0.022749536, 0.019968906, 0.037085325]
    INFO - 2024-11-20 11:30:40,696 - dataset - the number of samples: 2424
    INFO - 2024-11-20 11:30:40,696 - __init__ - speech2pose will be used as a model.
    /home/kemove/anaconda3/envs/open-mmlab/lib/python3.8/site-packages/pysensing/acoustic/inference/training/AcousticPose_utils/hpe_test.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
      state_dict = torch.load(os.path.join(result_path, "best_model.prm"))
    ---------- Start evaluation for test data ----------
    loss: 0.28709   RMSE: 0.54      MAE: 0.31       Acc: 0.79, 0.94, 0.98, 0.99
    arm RMSE: 0.71  arm MAE: 0.45   Acc: 0.70, 0.92, 0.97, 0.98
    leg RMSE: 0.38  leg MAE: 0.23   Acc: 0.90, 0.97, 0.99, 1.00
    body RMSE: 0.41 body MAE: 0.22  Acc: 0.86, 0.95, 0.98, 0.99
    Done.




.. GENERATED FROM PYTHON SOURCE LINES 142-144

Load the Wipose_LSTM model
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 144-153

.. code-block:: Python


    # Method 1
    hpe_model = Wipose_LSTM(in_cha=4,out_cha=63).to(device)
    # model_path = 'path to trained model'
    # state_dict = torch.load(model_path,weights_only=True)

    # Method 2
    hpe_model = load_hpe_model('wipose',pretrained=True,task='subject8').to(device)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/kemove/anaconda3/envs/open-mmlab/lib/python3.8/site-packages/torch/nn/modules/rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
      warnings.warn("dropout option adds dropout after all but last "




.. GENERATED FROM PYTHON SOURCE LINES 154-156

Load the data
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 156-161

.. code-block:: Python

    csv = './data/hpe_dataset/csv/pose_regression_timeseries_subject_8/test.csv' # The path contains the samosa dataset
    hpe_testdataset = SoundPoseLSTMDataset(csv,sound_length=2400,input_feature='raw',mean=np.array(get_raw_mean()).astype("float32"),std=np.array(get_raw_std()).astype("float32"))
    index = 0 # Randomly select an index
    sample= hpe_testdataset.__getitem__(index)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    INFO - 2024-11-20 11:30:56,592 - mean_std - mean value: [-1.9971873e-05, -1.60138708e-08, -5.86972669e-08, -1.24919584e-07]
    INFO - 2024-11-20 11:30:56,592 - mean_std - std value: [0.00657787, 0.00519175, 0.00472543, 0.00824625]
    INFO - 2024-11-20 11:30:56,989 - hpe - the number of samples: 2940




.. GENERATED FROM PYTHON SOURCE LINES 162-164

Model inference
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 164-182

.. code-block:: Python


    # Method 1
    hpe_model.eval()
    predicted_result = hpe_model(sample['sound'].unsqueeze(0).float().to(device))
    vis_images = make_images(sample['targets'],predicted_result.cpu().detach().squeeze(0))

    #Method 2
    from pysensing.acoustic.inference.predict import *
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    predicted_result  = hpe_predict(sample['sound'],'SoundPoseLSTMDataset',hpe_model, device=device)
    vis_images = make_images(sample['targets'].numpy(),predicted_result.cpu().detach().numpy().squeeze(0))

    seq_num = 2
    fig = plt.figure(figsize=(12, 12))
    plt.imshow(vis_images[seq_num]['img'])
    plt.axis('off')
    plt.show()




.. image-sg:: /acoustic/images/sphx_glr_acoustic_hpe_tutorial_002.png
   :alt: acoustic hpe tutorial
   :srcset: /acoustic/images/sphx_glr_acoustic_hpe_tutorial_002.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 183-185

Model embedding
------------------------

.. GENERATED FROM PYTHON SOURCE LINES 185-189

.. code-block:: Python

    from pysensing.acoustic.inference.embedding import *
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    sample_embedding = hpe_embedding(sample['sound'],'SoundPoseLSTMDataset',hpe_model, device=device)








.. GENERATED FROM PYTHON SOURCE LINES 190-191

And that's it. We're done with our acoustic humna pose estimation tutorials. Thanks for reading.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (2 minutes 46.766 seconds)


.. _sphx_glr_download_acoustic_acoustic_hpe_tutorial.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: acoustic_hpe_tutorial.ipynb <acoustic_hpe_tutorial.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: acoustic_hpe_tutorial.py <acoustic_hpe_tutorial.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
