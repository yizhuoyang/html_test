


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Acoustic Human Pose estimation Tutorial &mdash; Pysensing Tutorials  documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-rendered-html.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Acoustic Pedestrian Detection Tutorial" href="acoustic_ped_det_tutorial.html" />
  <!-- Google Analytics -->
  
  <!-- End Google Analytics -->
  

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://github.com/pysensing/pysensing" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/pysensing/pysensing">Get Started</a>
          </li>

          <!-- <li class="active"> -->
          <li>
            <a href="https://github.com/pysensing/pysensing">Tutorials</a>
          </li>

          <!-- <li> -->
          <li>
            <a href="https://yizhuoyang.github.io/html_test/">Doc</a>
          </li>

          <li>
            <a href="https://github.com/pysensing/pysensing">About Us</a>
          </li>

          <li>
            <a href="https://github.com/pysensing/pysensing">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Tutorials" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Acoustic</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="acoustic_filter_tutorial.html">Acoustic Preprocessing.Filter Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="acoustic_transform_tutorial.html">Acoustic Preprocssing.Transfrom Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="acoustic_sig_aug_tutorial.html">Acoustic Augmentation.Signal_aug Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="acoustic_spectrogram_aug_tutorial.html">Acoustic augmentation.spectrogram_aug Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="acoustic_har_tutorial.html">Acoustic Human Activity Recognition Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="acoustic_hgr_tutorial.html">Acoustic Hand Gesture Recognition Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="acoustic_ped_det_tutorial.html">Acoustic Pedestrian Detection Tutorial</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Acoustic Human Pose estimation Tutorial</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Tutorials
          
        </a> &gt;
      </li>

        
      <li>Acoustic Human Pose estimation Tutorial</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/acoustic/acoustic_hpe_tutorial.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <div class="pytorch-call-to-action-links">
            <div id="tutorial-type">acoustic/acoustic_hpe_tutorial</div>

            <div id="google-colab-link">
              <img class="call-to-action-img" src="../_static/images/pytorch-colab.svg"/>
              <div class="call-to-action-desktop-view">Run in Google Colab</div>
              <div class="call-to-action-mobile-view">Colab</div>
            </div>
            <div id="download-notebook-link">
              <img class="call-to-action-notebook-img" src="../_static/images/pytorch-download.svg"/>
              <div class="call-to-action-desktop-view">Download Notebook</div>
              <div class="call-to-action-mobile-view">Notebook</div>
            </div>
            <div id="github-view-link">
              <img class="call-to-action-img" src="../_static/images/pytorch-github.svg"/>
              <div class="call-to-action-desktop-view">View on GitHub</div>
              <div class="call-to-action-mobile-view">GitHub</div>
            </div>
          </div>

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-acoustic-acoustic-hpe-tutorial-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="acoustic-human-pose-estimation-tutorial">
<span id="sphx-glr-acoustic-acoustic-hpe-tutorial-py"></span><h1>Acoustic Human Pose estimation Tutorial<a class="headerlink" href="#acoustic-human-pose-estimation-tutorial" title="Permalink to this heading">¶</a></h1>
<p>!pip install pysensing</p>
<p>In this tutorial, we will be implementing codes for acoustic Human pose estimation</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pysensing.acoustic.datasets.utils.hpe_vis</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pysensing.acoustic.models.hpe</span> <span class="kn">import</span> <span class="n">Speech2pose</span><span class="p">,</span><span class="n">Wipose_LSTM</span>
<span class="kn">from</span> <span class="nn">pysensing.acoustic.models.get_model</span> <span class="kn">import</span> <span class="n">load_hpe_model</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="simple">
<dt>Listening Human Behavior: 3D Human Pose Estimation with Acoustic Signals</dt><dd></dd>
</dl>
<p>Implementation of “Listening Human Behavior: 3D Human Pose Estimation with Acoustic Signals”.</p>
<p>This dataset contains the audio reflected by human to estimate the 3D human pose with the acoustic signals.</p>
<p>Reference: <a class="reference external" href="https://github.com/YutoShibata07/AcousticPose_Public">https://github.com/YutoShibata07/AcousticPose_Public</a></p>
<section id="load-the-data">
<h2>Load the data<a class="headerlink" href="#load-the-data" title="Permalink to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Method 1: Use get_dataloader</span>
<span class="kn">from</span> <span class="nn">pysensing.acoustic.datasets.get_dataloader</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">train_loader</span><span class="p">,</span><span class="n">val_loader</span><span class="p">,</span><span class="n">test_loader</span> <span class="o">=</span> <span class="n">load_hpe_dataset</span><span class="p">(</span>
    <span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span>
    <span class="n">dataset_name</span><span class="o">=</span><span class="s1">&#39;pose_regression_timeseries_subject_1&#39;</span><span class="p">,</span>
    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Method 2</span>
<span class="n">csv</span> <span class="o">=</span> <span class="s1">&#39;./data/hpe_dataset/csv/pose_regression_timeseries_subject_1/test.csv&#39;</span> <span class="c1"># The path contains the samosa dataset</span>
<span class="n">data_dir</span> <span class="o">=</span> <span class="s1">&#39;./data&#39;</span>
<span class="n">hpe_testdataset</span> <span class="o">=</span> <span class="n">SoundPose2DDataset</span><span class="p">(</span><span class="n">csv</span><span class="p">,</span><span class="n">sound_length</span><span class="o">=</span><span class="mi">2400</span><span class="p">,</span><span class="n">input_feature</span><span class="o">=</span><span class="s1">&#39;logmel&#39;</span><span class="p">,</span>
                                     <span class="n">mean</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">get_mean</span><span class="p">())</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)[:</span><span class="mi">4</span><span class="p">],</span>
                                     <span class="n">std</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">get_std</span><span class="p">())</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)[:</span><span class="mi">4</span><span class="p">],</span>
                                     <span class="p">)</span>
<span class="n">index</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># Randomly select an index</span>
<span class="n">sample</span><span class="o">=</span> <span class="n">hpe_testdataset</span><span class="o">.</span><span class="fm">__getitem__</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;targets&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;sound&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Size([12, 63])
torch.Size([4, 12, 128])
</pre></div>
</div>
</section>
<section id="load-speech2pose-model">
<h2>Load Speech2pose model<a class="headerlink" href="#load-speech2pose-model" title="Permalink to this heading">¶</a></h2>
<p>Method 1</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">hpe_model</span> <span class="o">=</span> <span class="n">Speech2pose</span><span class="p">(</span><span class="n">out_cha</span><span class="o">=</span><span class="mi">63</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># model_path = &#39;path to pretrian weights&#39;</span>
<span class="c1"># state_dict = torch.load(model_path,weights_only=True)</span>
<span class="c1"># hpe_model.load_state_dict(state_dict)</span>

<span class="c1"># Method 2</span>
<span class="n">hpe_model</span> <span class="o">=</span> <span class="n">load_hpe_model</span><span class="p">(</span><span class="s1">&#39;speech2pose&#39;</span><span class="p">,</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">task</span><span class="o">=</span><span class="s1">&#39;subject8&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="modle-inference">
<h2>Modle Inference<a class="headerlink" href="#modle-inference" title="Permalink to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1">#Method 1</span>
<span class="n">sample</span><span class="o">=</span> <span class="n">hpe_testdataset</span><span class="o">.</span><span class="fm">__getitem__</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
<span class="n">hpe_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">predicted_result</span> <span class="o">=</span> <span class="n">hpe_model</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;sound&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="n">vis_images</span> <span class="o">=</span> <span class="n">make_images</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;targets&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span><span class="n">predicted_result</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="c1">#Method 2</span>
<span class="kn">from</span> <span class="nn">pysensing.acoustic.inference.predict</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">predicted_result</span>  <span class="o">=</span> <span class="n">hpe_predict</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;sound&#39;</span><span class="p">],</span><span class="s1">&#39;SoundPose2DDataset&#39;</span><span class="p">,</span><span class="n">hpe_model</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">vis_images</span> <span class="o">=</span> <span class="n">make_images</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;targets&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span><span class="n">predicted_result</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="n">seq_num</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">vis_images</span><span class="p">[</span><span class="n">seq_num</span><span class="p">][</span><span class="s1">&#39;img&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_acoustic_hpe_tutorial_001.png" srcset="../_images/sphx_glr_acoustic_hpe_tutorial_001.png" alt="acoustic hpe tutorial" class = "sphx-glr-single-img"/><div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>/home/kemove/anaconda3/envs/il/lib/python3.9/site-packages/torch/nn/modules/conv.py:306: UserWarning: Using padding=&#39;same&#39; with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1008.)
  return F.conv1d(input, weight, bias, self.stride,
</pre></div>
</div>
</section>
<section id="modle-embedding">
<h2>Modle Embedding<a class="headerlink" href="#modle-embedding" title="Permalink to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pysensing.acoustic.inference.embedding</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">sample_embedding</span> <span class="o">=</span> <span class="n">hpe_embedding</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;sound&#39;</span><span class="p">],</span><span class="s1">&#39;SoundPose2DDataset&#39;</span><span class="p">,</span><span class="n">hpe_model</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="modle-training">
<h2>Modle Training<a class="headerlink" href="#modle-training" title="Permalink to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pysensing.acoustic.inference.training.AcousticPose_utils.hpe_train</span> <span class="kn">import</span> <span class="n">train_model</span><span class="p">,</span><span class="n">generate_configs</span>

<span class="n">args</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;root_dir&quot;</span><span class="p">:</span> <span class="s2">&quot;./data/hpe_dataset/testing_result&quot;</span><span class="p">,</span>
    <span class="s2">&quot;save_name&quot;</span><span class="p">:</span> <span class="s2">&quot;seq1&quot;</span><span class="p">,</span>
    <span class="s2">&quot;input_feature&quot;</span><span class="p">:</span> <span class="s2">&quot;logmel&quot;</span><span class="p">,</span>
    <span class="s2">&quot;batchsize&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
    <span class="s2">&quot;max_epoch&quot;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
    <span class="s2">&quot;csv_path&quot;</span><span class="p">:</span> <span class="s2">&quot;./data/hpe_dataset/csv&quot;</span><span class="p">,</span>
    <span class="s2">&quot;dataset_name&quot;</span><span class="p">:</span> <span class="s2">&quot;pose_regression_timeseries_subject_1&quot;</span><span class="p">,</span>
    <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;speech2pose&quot;</span><span class="p">,</span>
    <span class="s2">&quot;sound_length&quot;</span><span class="p">:</span> <span class="mi">2400</span><span class="p">,</span>
    <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">config_path</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;root_dir&quot;</span><span class="p">]</span><span class="o">+</span><span class="s1">&#39;/&#39;</span><span class="o">+</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;save_name&quot;</span><span class="p">]</span><span class="o">+</span><span class="s2">&quot;/config.yaml&quot;</span>
<span class="n">generate_configs</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
<span class="n">resume_training</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">random_seed</span> <span class="o">=</span> <span class="mi">0</span>

<span class="n">train_model</span><span class="p">(</span>
    <span class="n">config_path</span><span class="o">=</span><span class="n">config_path</span><span class="p">,</span>
    <span class="n">resume</span><span class="o">=</span><span class="n">resume_training</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="n">random_seed</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Modle Training</span>
<span class="c1"># ------------------------</span>
<span class="kn">from</span> <span class="nn">pysensing.acoustic.inference.training.AcousticPose_utils.hpe_test</span> <span class="kn">import</span> <span class="n">evaluate_model</span>
<span class="n">args</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;root_dir&quot;</span><span class="p">:</span> <span class="s2">&quot;./data/hpe_dataset/testing_result&quot;</span><span class="p">,</span>
    <span class="s2">&quot;save_name&quot;</span><span class="p">:</span> <span class="s2">&quot;seq1&quot;</span><span class="p">,</span>
    <span class="s2">&quot;batchsize&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
    <span class="s2">&quot;max_epoch&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
    <span class="s2">&quot;csv_path&quot;</span><span class="p">:</span> <span class="s2">&quot;./data/hpe_dataset/csv&quot;</span><span class="p">,</span>
    <span class="s2">&quot;dataset_name&quot;</span><span class="p">:</span> <span class="s2">&quot;pose_regression_timeseries_subject_1&quot;</span><span class="p">,</span>
    <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;speech2pose&quot;</span><span class="p">,</span>
    <span class="s2">&quot;sound_length&quot;</span><span class="p">:</span> <span class="mi">2400</span><span class="p">,</span>
    <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">config_path</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;root_dir&quot;</span><span class="p">]</span><span class="o">+</span><span class="s1">&#39;/&#39;</span><span class="o">+</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;save_name&quot;</span><span class="p">]</span><span class="o">+</span><span class="s2">&quot;/config.yaml&quot;</span>
<span class="n">evaluation_mode</span> <span class="o">=</span> <span class="s2">&quot;test&quot;</span>
<span class="n">model_path</span> <span class="o">=</span> <span class="kc">None</span>

<span class="n">evaluate_model</span><span class="p">(</span>
    <span class="n">config_path</span><span class="o">=</span><span class="n">config_path</span><span class="p">,</span>
    <span class="n">mode</span><span class="o">=</span><span class="n">evaluation_mode</span><span class="p">,</span>
    <span class="n">model_path</span><span class="o">=</span><span class="n">model_path</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>{&#39;csv_path&#39;: &#39;./data/hpe_dataset/csv&#39;, &#39;model&#39;: &#39;speech2pose&#39;, &#39;pretrained&#39;: True, &#39;use_class_weight&#39;: True, &#39;batch_size&#39;: 32, &#39;width&#39;: 224, &#39;height&#39;: 224, &#39;num_workers&#39;: 8, &#39;max_epoch&#39;: 50, &#39;learning_rate&#39;: 0.01, &#39;sound_length&#39;: 2400, &#39;dataset_name&#39;: &#39;pose_regression_timeseries_subject_1&#39;, &#39;input_feature&#39;: &#39;logmel&#39;, &#39;topk&#39;: (1, 3), &#39;smooth_loss&#39;: False, &#39;ratio&#39;: 0.0, &#39;gan&#39;: &#39;none&#39;, &#39;finetune&#39;: 0, &#39;aug&#39;: &#39;none&#39;}
Finished making configuration files.
/home/kemove/anaconda3/envs/il/lib/python3.9/site-packages/pysensing/acoustic/inference/training/AcousticPose_utils/libs/dataset.py:195: FutureWarning: DataFrame.fillna with &#39;method&#39; is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.
  self.df[self.df[&quot;sound_length&quot;] == sound_length]
/home/kemove/anaconda3/envs/il/lib/python3.9/site-packages/pysensing/acoustic/inference/training/AcousticPose_utils/libs/dataset.py:195: FutureWarning: DataFrame.fillna with &#39;method&#39; is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.
  self.df[self.df[&quot;sound_length&quot;] == sound_length]
epoch: 0        epoch time[sec]: 4      train loss: 38.3569     val loss: 122.5143      val_rmse: 11.06862      val_mae: 7.60888
epoch: 1        epoch time[sec]: 1      train loss: 3.5271      val loss: 5.5197        val_rmse: 2.34941       val_mae: 1.41563
epoch: 2        epoch time[sec]: 1      train loss: 1.7630      val loss: 2.2134        val_rmse: 1.48776       val_mae: 0.93121
epoch: 3        epoch time[sec]: 1      train loss: 1.3789      val loss: 2.1969        val_rmse: 1.48221       val_mae: 0.91003
epoch: 4        epoch time[sec]: 1      train loss: 1.2648      val loss: 2.7483        val_rmse: 1.65781       val_mae: 1.08007
epoch: 5        epoch time[sec]: 1      train loss: 1.1762      val loss: 3.5439        val_rmse: 1.88253       val_mae: 1.15230
epoch: 6        epoch time[sec]: 1      train loss: 1.2064      val loss: 1.4791        val_rmse: 1.21617       val_mae: 0.72835
epoch: 7        epoch time[sec]: 1      train loss: 1.0214      val loss: 3.6852        val_rmse: 1.91968       val_mae: 1.18562
epoch: 8        epoch time[sec]: 1      train loss: 1.2381      val loss: 9.0887        val_rmse: 3.01475       val_mae: 1.92678
epoch: 9        epoch time[sec]: 1      train loss: 0.9140      val loss: 2.7270        val_rmse: 1.65135       val_mae: 1.01521
epoch: 10       epoch time[sec]: 1      train loss: 0.7656      val loss: 4.5527        val_rmse: 2.13371       val_mae: 1.27259
epoch: 11       epoch time[sec]: 1      train loss: 0.6205      val loss: 1.7849        val_rmse: 1.33601       val_mae: 0.75739
epoch: 12       epoch time[sec]: 1      train loss: 0.6154      val loss: 1.6265        val_rmse: 1.27533       val_mae: 0.74103
epoch: 13       epoch time[sec]: 1      train loss: 0.5134      val loss: 0.9743        val_rmse: 0.98705       val_mae: 0.55418
epoch: 14       epoch time[sec]: 1      train loss: 0.5370      val loss: 0.9392        val_rmse: 0.96910       val_mae: 0.54693
epoch: 15       epoch time[sec]: 1      train loss: 0.4042      val loss: 0.9194        val_rmse: 0.95886       val_mae: 0.51252
epoch: 16       epoch time[sec]: 1      train loss: 0.3712      val loss: 0.8734        val_rmse: 0.93454       val_mae: 0.48468
epoch: 17       epoch time[sec]: 1      train loss: 0.3812      val loss: 1.2397        val_rmse: 1.11342       val_mae: 0.59881
epoch: 18       epoch time[sec]: 1      train loss: 0.4479      val loss: 7.4993        val_rmse: 2.73849       val_mae: 1.73351
epoch: 19       epoch time[sec]: 1      train loss: 0.5249      val loss: 0.9717        val_rmse: 0.98574       val_mae: 0.56446
epoch: 20       epoch time[sec]: 1      train loss: 0.4537      val loss: 2.7692        val_rmse: 1.66409       val_mae: 0.93542
epoch: 21       epoch time[sec]: 1      train loss: 0.3601      val loss: 3.2854        val_rmse: 1.81256       val_mae: 1.01124
epoch: 22       epoch time[sec]: 1      train loss: 0.3717      val loss: 1.2158        val_rmse: 1.10262       val_mae: 0.63135
epoch: 23       epoch time[sec]: 1      train loss: 0.3641      val loss: 1.7205        val_rmse: 1.31166       val_mae: 0.66832
epoch: 24       epoch time[sec]: 1      train loss: 0.4600      val loss: 3.1732        val_rmse: 1.78136       val_mae: 1.01172
epoch: 25       epoch time[sec]: 1      train loss: 0.4198      val loss: 0.7186        val_rmse: 0.84772       val_mae: 0.40647
epoch: 26       epoch time[sec]: 1      train loss: 0.3826      val loss: 0.6256        val_rmse: 0.79096       val_mae: 0.45698
epoch: 27       epoch time[sec]: 1      train loss: 0.4344      val loss: 1.9526        val_rmse: 1.39737       val_mae: 0.74666
epoch: 28       epoch time[sec]: 1      train loss: 0.3857      val loss: 2.0649        val_rmse: 1.43699       val_mae: 0.88084
epoch: 29       epoch time[sec]: 1      train loss: 0.4024      val loss: 2.1361        val_rmse: 1.46154       val_mae: 0.74190
epoch: 30       epoch time[sec]: 1      train loss: 0.3204      val loss: 1.1463        val_rmse: 1.07066       val_mae: 0.60133
epoch: 31       epoch time[sec]: 1      train loss: 0.2975      val loss: 0.3898        val_rmse: 0.62431       val_mae: 0.32956
epoch: 32       epoch time[sec]: 1      train loss: 0.3220      val loss: 1.8097        val_rmse: 1.34526       val_mae: 0.68335
epoch: 33       epoch time[sec]: 1      train loss: 0.3112      val loss: 0.5565        val_rmse: 0.74597       val_mae: 0.38793
epoch: 34       epoch time[sec]: 1      train loss: 0.2543      val loss: 0.5031        val_rmse: 0.70931       val_mae: 0.39188
epoch: 35       epoch time[sec]: 1      train loss: 0.2639      val loss: 0.4450        val_rmse: 0.66710       val_mae: 0.35181
epoch: 36       epoch time[sec]: 1      train loss: 0.3003      val loss: 0.3797        val_rmse: 0.61624       val_mae: 0.33479
epoch: 37       epoch time[sec]: 1      train loss: 0.2557      val loss: 0.4548        val_rmse: 0.67439       val_mae: 0.36248
epoch: 38       epoch time[sec]: 1      train loss: 0.2340      val loss: 0.4082        val_rmse: 0.63889       val_mae: 0.33224
epoch: 39       epoch time[sec]: 1      train loss: 0.2577      val loss: 0.4151        val_rmse: 0.64427       val_mae: 0.32982
epoch: 40       epoch time[sec]: 1      train loss: 0.2413      val loss: 0.4459        val_rmse: 0.66773       val_mae: 0.37332
epoch: 41       epoch time[sec]: 1      train loss: 0.2762      val loss: 0.4307        val_rmse: 0.65624       val_mae: 0.33005
epoch: 42       epoch time[sec]: 1      train loss: 0.2130      val loss: 0.3840        val_rmse: 0.61968       val_mae: 0.32186
epoch: 43       epoch time[sec]: 1      train loss: 0.2585      val loss: 0.4669        val_rmse: 0.68330       val_mae: 0.31312
epoch: 44       epoch time[sec]: 1      train loss: 0.2674      val loss: 0.4117        val_rmse: 0.64164       val_mae: 0.33041
epoch: 45       epoch time[sec]: 1      train loss: 0.2774      val loss: 0.4377        val_rmse: 0.66156       val_mae: 0.32949
epoch: 46       epoch time[sec]: 1      train loss: 0.2645      val loss: 0.5535        val_rmse: 0.74399       val_mae: 0.38078
epoch: 47       epoch time[sec]: 1      train loss: 0.2412      val loss: 0.4701        val_rmse: 0.68563       val_mae: 0.41444
epoch: 48       epoch time[sec]: 1      train loss: 0.3006      val loss: 0.4420        val_rmse: 0.66485       val_mae: 0.35141
epoch: 49       epoch time[sec]: 1      train loss: 0.2479      val loss: 0.3813        val_rmse: 0.61753       val_mae: 0.31207
/home/kemove/anaconda3/envs/il/lib/python3.9/site-packages/pysensing/acoustic/inference/training/AcousticPose_utils/libs/dataset.py:195: FutureWarning: DataFrame.fillna with &#39;method&#39; is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.
  self.df[self.df[&quot;sound_length&quot;] == sound_length]
---------- Start evaluation for test data ----------
loss: 0.37975   RMSE: 0.62      MAE: 0.33       Acc: 0.83, 0.96, 0.98, 0.99
arm RMSE: 0.78  arm MAE: 0.46   Acc: 0.74, 0.94, 0.98, 0.99
leg RMSE: 0.48  leg MAE: 0.27   Acc: 0.91, 0.97, 0.99, 0.99
body RMSE: 0.51 body MAE: 0.23  Acc: 0.93, 0.99, 1.00, 1.00
Done.
</pre></div>
</div>
</section>
<section id="load-the-wipose-lstm-model">
<h2>Load the Wipose_LSTM model<a class="headerlink" href="#load-the-wipose-lstm-model" title="Permalink to this heading">¶</a></h2>
<p># Method 1</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">hpe_model</span> <span class="o">=</span> <span class="n">Wipose_LSTM</span><span class="p">(</span><span class="n">in_cha</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span><span class="n">out_cha</span><span class="o">=</span><span class="mi">63</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># model_path = &#39;path to trained model&#39;</span>
<span class="c1"># state_dict = torch.load(model_path,weights_only=True)</span>

<span class="c1"># Method 2</span>
<span class="n">hpe_model</span> <span class="o">=</span> <span class="n">load_hpe_model</span><span class="p">(</span><span class="s1">&#39;wipose&#39;</span><span class="p">,</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">task</span><span class="o">=</span><span class="s1">&#39;subject8&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>/home/kemove/anaconda3/envs/il/lib/python3.9/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn(&quot;dropout option adds dropout after all but last &quot;
</pre></div>
</div>
</section>
<section id="id1">
<h2>Load the data<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">csv</span> <span class="o">=</span> <span class="s1">&#39;./data/hpe_dataset/csv/pose_regression_timeseries_subject_8/test.csv&#39;</span> <span class="c1"># The path contains the samosa dataset</span>
<span class="n">hpe_testdataset</span> <span class="o">=</span> <span class="n">SoundPoseLSTMDataset</span><span class="p">(</span><span class="n">csv</span><span class="p">,</span><span class="n">sound_length</span><span class="o">=</span><span class="mi">2400</span><span class="p">,</span><span class="n">input_feature</span><span class="o">=</span><span class="s1">&#39;raw&#39;</span><span class="p">,</span><span class="n">mean</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">get_raw_mean</span><span class="p">())</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">),</span><span class="n">std</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">get_raw_std</span><span class="p">())</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">))</span>
<span class="n">index</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># Randomly select an index</span>
<span class="n">sample</span><span class="o">=</span> <span class="n">hpe_testdataset</span><span class="o">.</span><span class="fm">__getitem__</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="model-inference">
<h2>Model inference<a class="headerlink" href="#model-inference" title="Permalink to this heading">¶</a></h2>
<p>Method 1</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">hpe_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">predicted_result</span> <span class="o">=</span> <span class="n">hpe_model</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;sound&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="n">vis_images</span> <span class="o">=</span> <span class="n">make_images</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;targets&#39;</span><span class="p">],</span><span class="n">predicted_result</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="c1">#Method 2</span>
<span class="kn">from</span> <span class="nn">pysensing.acoustic.inference.predict</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">predicted_result</span>  <span class="o">=</span> <span class="n">hpe_predict</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;sound&#39;</span><span class="p">],</span><span class="s1">&#39;SoundPoseLSTMDataset&#39;</span><span class="p">,</span><span class="n">hpe_model</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">vis_images</span> <span class="o">=</span> <span class="n">make_images</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;targets&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span><span class="n">predicted_result</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="n">seq_num</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">vis_images</span><span class="p">[</span><span class="n">seq_num</span><span class="p">][</span><span class="s1">&#39;img&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_acoustic_hpe_tutorial_002.png" srcset="../_images/sphx_glr_acoustic_hpe_tutorial_002.png" alt="acoustic hpe tutorial" class = "sphx-glr-single-img"/></section>
<section id="model-embedding">
<h2>Model embedding<a class="headerlink" href="#model-embedding" title="Permalink to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pysensing.acoustic.inference.embedding</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">sample_embedding</span> <span class="o">=</span> <span class="n">hpe_embedding</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;sound&#39;</span><span class="p">],</span><span class="s1">&#39;SoundPoseLSTMDataset&#39;</span><span class="p">,</span><span class="n">hpe_model</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>And that’s it. We’re done with our acoustic humna pose estimation tutorials. Thanks for reading.</p>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (3 minutes 4.931 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-acoustic-acoustic-hpe-tutorial-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/7a6f102b877e68e9956fa06c9728eff5/acoustic_hpe_tutorial.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">acoustic_hpe_tutorial.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/3f90f032a2f8d490ea6cc694e6ba4690/acoustic_hpe_tutorial.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">acoustic_hpe_tutorial.py</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="acoustic_ped_det_tutorial.html" class="btn btn-neutral" title="Acoustic Pedestrian Detection Tutorial" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr class="rating-hr hr-top">
      <div class="rating-container">
        <div class="rating-prompt">Rate this Tutorial</div>
        <div class="stars-outer">
          <i class="far fa-star" title="1 Star" data-behavior="tutorial-rating" data-count="1"></i>
          <i class="far fa-star" title="2 Stars" data-behavior="tutorial-rating" data-count="2"></i>
          <i class="far fa-star" title="3 Stars" data-behavior="tutorial-rating" data-count="3"></i>
          <i class="far fa-star" title="4 Stars" data-behavior="tutorial-rating" data-count="4"></i>
          <i class="far fa-star" title="5 Stars" data-behavior="tutorial-rating" data-count="5"></i>
        </div>
      </div>
    <hr class="rating-hr hr-bottom"/>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2024, Pysensing contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>
<!--        <div class="pytorch-content-right" id="pytorch-content-right">-->
<!--          <div class="pytorch-right-menu" id="pytorch-right-menu">-->
<!--              <a class="twitter-timeline" data-width="100%" href="https://twitter.com/pypose_org?ref_src=twsrc%5Etfw">News from Twitter @pypose_rog<br>You need VPN if you see this.</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>-->
<!--          </div>-->
<!--        </div>-->
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../_static/doctools.js"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access documentation for Pysensing</p>
          <a class="with-right-arrow" href="https://yizhuoyang.github.io/html_test/">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get started with tutorials and examples</p>
          <a class="with-right-arrow" href="https://github.com/pysensing/pysensing">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Get Started</h2>
          <p>Find resources and how to start using pysensing</p>
          <a class="with-right-arrow" href="https://github.com/pysensing/pysensing">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://github.com/pysensing/pysensing" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://github.com/pysensing/pysensing">Pysensing</a></li>
            <li><a href="https://github.com/pysensing/pysensing">Get Started</a></li>
            <li><a href="https://github.com/pysensing/pysensing">Features</a></li>
            <li><a href="https://github.com/pysensing/pysensing">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://github.com/pysensing/pysensing">Resources</a></li>
            <li><a href="https://github.com/pysensing/pysensing">Tutorials</a></li>
            <li><a href="https://yizhuoyang.github.io/html_test/">Docs</a></li>
            <li><a href="https://github.com/pysensing/pysensing" target="_blank">Github Issues</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://github.com/pysensing/pysensing" target="_blank">Twitter</a></li>
            <li><a href="https://github.com/pysensing/pysensing" target="_blank">GitHub</a></li>

          </ul>  
          </div>
        </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://github.com/pysensing/pysensing" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/pysensing/pysensing">Get Started</a>
          </li>
          <li>
            <a href="https://github.com/pysensing/pysensing">Tutorials</a>
          </li>
          <li>
            <a href="https://yizhuoyang.github.io/html_test/">Docs</a>
          </li>
          <li>
            <a href="https://github.com/pysensing/pysensing">About Us</a>
          </li>
          <li>
            <a href="https://github.com/pysensing/pysensing">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>