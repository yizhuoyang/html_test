


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Acoustic Human Pose estimation Tutorial &mdash; Pysensing Tutorials  documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-rendered-html.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Bluetooth Localization Tutorial" href="../bluetooth/bluetooth_localization_tutorial.html" />
    <link rel="prev" title="Acoustic Pedestrian Detection Tutorial" href="acoustic_ped_det_tutorial.html" />
  <!-- Google Analytics -->
  
  <!-- End Google Analytics -->
  

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://github.com/pysensing/pysensing" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/pysensing/pysensing">Get Started</a>
          </li>

          <!-- <li class="active"> -->
          <li>
            <a href="https://github.com/pysensing/pysensing">Tutorials</a>
          </li>

          <!-- <li> -->
          <li>
            <a href="https://github.com/pysensing/pysensing">Doc</a>
          </li>

          <li>
            <a href="https://github.com/pysensing/pysensing">About Us</a>
          </li>

          <li>
            <a href="https://github.com/pysensing/pysensing">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Tutorials" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Acoustic</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="acoustic_filter_tutorial.html">Acoustic Preprocessing.Filter Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="acoustic_transform_tutorial.html">Acoustic Preprocssing.Transfrom Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="acoustic_sig_aug_tutorial.html">Acoustic Augmentation.Signal_aug Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="acoustic_spectrogram_aug_tutorial.html">Acoustic augmentation.spectrogram_aug Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="acoustic_har_tutorial.html">Acoustic Human Activity Recognition Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="acoustic_hgr_tutorial.html">Acoustic Hand Gesture Recognition Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="acoustic_ped_det_tutorial.html">Acoustic Pedestrian Detection Tutorial</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Acoustic Human Pose estimation Tutorial</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Bluetooth</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../bluetooth/bluetooth_localization_tutorial.html">Bluetooth Localization Tutorial</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">CSI</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../csi/add_noise_tutorial.html">CSI Augmentation.adding_noise Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../csi/deformation_tutorial.html">CSI Preprocessing.transform Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../csi/denoising_tutorial.html">CSI Preprocessing.denoising Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../csi/HAR_HID_tutorial.html">CSI Augmentation.deformation Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../csi/HFD_tutorial.html">CSI classification tasks Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../csi/HPE_tutorial.html">CSI human pose estimation Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../csi/Reconstruction_tutorial.html">CSI human reconstruction Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../csi/transform_tutorial.html">CSI human fall detection Tutorial</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">intraoral_scan</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../intraoral_scan/tooth_alignment.html">Tutorial for tooth alignment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intraoral_scan/tooth_segmentation.html">Tutorial for tooth segmentation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">mmwave_raw</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../mmwave_raw/mmwave_raw_tutorial.html">Data loading and prepocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mmwave_raw/mmwave_raw_tutorial.html#model-loading-and-inference">model loading and inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">mmwave_PC</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../mmwave_PC/mmwave_PC_har_tutorial.html">Tutorial for Human Activity Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mmwave_PC/mmwave_PC_hgr_tutorial.html">Tutorial for Human Gesture Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mmwave_PC/mmwave_PC_hpe_tutorial.html">Tutorial for Human Pose Estimation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">rssi</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../rssi/rssi_localization_fingerprinting_tutorial.html">RSSI Localization Fingerprinting Tutorial</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">uwb</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../uwb/uwb_preprocessing_tutorial.html">Tutorial for UWB Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../uwb/uwb_augmentation_tutorial.html">Tutorial for UWB Data Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../uwb/uwb_har_tutorial.html">Tutorial for UWB Human Activity Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../uwb/uwb_localization_tutorial.html">Tutorial for UWB Localization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../uwb/uwb_occupancy_detection_tutorial.html">Tutorial for UWB Occupancy Detection</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Tutorials
          
        </a> &gt;
      </li>

        
      <li>Acoustic Human Pose estimation Tutorial</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/acoustic/acoustic_hpe_tutorial.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <div class="pytorch-call-to-action-links">
            <div id="tutorial-type">acoustic/acoustic_hpe_tutorial</div>

            <div id="google-colab-link">
              <img class="call-to-action-img" src="../_static/images/pytorch-colab.svg"/>
              <div class="call-to-action-desktop-view">Run in Google Colab</div>
              <div class="call-to-action-mobile-view">Colab</div>
            </div>
            <div id="download-notebook-link">
              <img class="call-to-action-notebook-img" src="../_static/images/pytorch-download.svg"/>
              <div class="call-to-action-desktop-view">Download Notebook</div>
              <div class="call-to-action-mobile-view">Notebook</div>
            </div>
            <div id="github-view-link">
              <img class="call-to-action-img" src="../_static/images/pytorch-github.svg"/>
              <div class="call-to-action-desktop-view">View on GitHub</div>
              <div class="call-to-action-mobile-view">GitHub</div>
            </div>
          </div>

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-acoustic-acoustic-hpe-tutorial-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="acoustic-human-pose-estimation-tutorial">
<span id="sphx-glr-acoustic-acoustic-hpe-tutorial-py"></span><h1>Acoustic Human Pose estimation Tutorial<a class="headerlink" href="#acoustic-human-pose-estimation-tutorial" title="Permalink to this heading">¶</a></h1>
<p>!pip install pysensing</p>
<p>In this tutorial, we will be implementing codes for acoustic Human pose estimation</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pysensing.acoustic.datasets.utils.hpe_vis</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pysensing.acoustic.models.hpe</span> <span class="kn">import</span> <span class="n">Speech2pose</span><span class="p">,</span><span class="n">Wipose_LSTM</span>
<span class="kn">from</span> <span class="nn">pysensing.acoustic.models.get_model</span> <span class="kn">import</span> <span class="n">load_hpe_model</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="simple">
<dt>Listening Human Behavior: 3D Human Pose Estimation with Acoustic Signals</dt><dd></dd>
</dl>
<p>Implementation of “Listening Human Behavior: 3D Human Pose Estimation with Acoustic Signals”.</p>
<p>This dataset contains the audio reflected by human to estimate the 3D human pose with the acoustic signals.</p>
<p>Reference: <a class="reference external" href="https://github.com/YutoShibata07/AcousticPose_Public">https://github.com/YutoShibata07/AcousticPose_Public</a></p>
<section id="load-the-data">
<h2>Load the data<a class="headerlink" href="#load-the-data" title="Permalink to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Method 1: Use get_dataloader</span>
<span class="kn">from</span> <span class="nn">pysensing.acoustic.datasets.get_dataloader</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">train_loader</span><span class="p">,</span><span class="n">val_loader</span><span class="p">,</span><span class="n">test_loader</span> <span class="o">=</span> <span class="n">load_hpe_dataset</span><span class="p">(</span>
    <span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span>
    <span class="n">dataset_name</span><span class="o">=</span><span class="s1">&#39;pose_regression_timeseries_subject_1&#39;</span><span class="p">,</span>
    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Method 2</span>
<span class="n">csv</span> <span class="o">=</span> <span class="s1">&#39;./data/hpe_dataset/csv/pose_regression_timeseries_subject_1/test.csv&#39;</span> <span class="c1"># The path contains the samosa dataset</span>
<span class="n">data_dir</span> <span class="o">=</span> <span class="s1">&#39;./data&#39;</span>
<span class="n">hpe_testdataset</span> <span class="o">=</span> <span class="n">SoundPose2DDataset</span><span class="p">(</span><span class="n">csv</span><span class="p">,</span><span class="n">sound_length</span><span class="o">=</span><span class="mi">2400</span><span class="p">,</span><span class="n">input_feature</span><span class="o">=</span><span class="s1">&#39;logmel&#39;</span><span class="p">,</span>
                                     <span class="n">mean</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">get_mean</span><span class="p">())</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)[:</span><span class="mi">4</span><span class="p">],</span>
                                     <span class="n">std</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">get_std</span><span class="p">())</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)[:</span><span class="mi">4</span><span class="p">],</span>
                                     <span class="p">)</span>
<span class="n">index</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># Randomly select an index</span>
<span class="n">sample</span><span class="o">=</span> <span class="n">hpe_testdataset</span><span class="o">.</span><span class="fm">__getitem__</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;targets&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;sound&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Size([12, 63])
torch.Size([4, 12, 128])
</pre></div>
</div>
</section>
<section id="load-speech2pose-model">
<h2>Load Speech2pose model<a class="headerlink" href="#load-speech2pose-model" title="Permalink to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Method 1</span>
<span class="n">hpe_model</span> <span class="o">=</span> <span class="n">Speech2pose</span><span class="p">(</span><span class="n">out_cha</span><span class="o">=</span><span class="mi">63</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># model_path = &#39;path to pretrian weights&#39;</span>
<span class="c1"># state_dict = torch.load(model_path,weights_only=True)</span>
<span class="c1"># hpe_model.load_state_dict(state_dict)</span>

<span class="c1"># Method 2</span>
<span class="n">hpe_model</span> <span class="o">=</span> <span class="n">load_hpe_model</span><span class="p">(</span><span class="s1">&#39;speech2pose&#39;</span><span class="p">,</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">task</span><span class="o">=</span><span class="s1">&#39;subject8&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="modle-inference">
<h2>Modle Inference<a class="headerlink" href="#modle-inference" title="Permalink to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1">#Method 1</span>
<span class="n">sample</span><span class="o">=</span> <span class="n">hpe_testdataset</span><span class="o">.</span><span class="fm">__getitem__</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
<span class="n">hpe_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">predicted_result</span> <span class="o">=</span> <span class="n">hpe_model</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;sound&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="n">vis_images</span> <span class="o">=</span> <span class="n">make_images</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;targets&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span><span class="n">predicted_result</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="c1">#Method 2</span>
<span class="kn">from</span> <span class="nn">pysensing.acoustic.inference.predict</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">predicted_result</span>  <span class="o">=</span> <span class="n">hpe_predict</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;sound&#39;</span><span class="p">],</span><span class="s1">&#39;SoundPose2DDataset&#39;</span><span class="p">,</span><span class="n">hpe_model</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">vis_images</span> <span class="o">=</span> <span class="n">make_images</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;targets&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span><span class="n">predicted_result</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="n">seq_num</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">vis_images</span><span class="p">[</span><span class="n">seq_num</span><span class="p">][</span><span class="s1">&#39;img&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_acoustic_hpe_tutorial_001.png" srcset="../_images/sphx_glr_acoustic_hpe_tutorial_001.png" alt="acoustic hpe tutorial" class = "sphx-glr-single-img"/><div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>/home/kemove/anaconda3/envs/il/lib/python3.9/site-packages/torch/nn/modules/conv.py:306: UserWarning: Using padding=&#39;same&#39; with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1008.)
  return F.conv1d(input, weight, bias, self.stride,
</pre></div>
</div>
</section>
<section id="modle-embedding">
<h2>Modle Embedding<a class="headerlink" href="#modle-embedding" title="Permalink to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pysensing.acoustic.inference.embedding</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">sample_embedding</span> <span class="o">=</span> <span class="n">hpe_embedding</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;sound&#39;</span><span class="p">],</span><span class="s1">&#39;SoundPose2DDataset&#39;</span><span class="p">,</span><span class="n">hpe_model</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="modle-training">
<h2>Modle Training<a class="headerlink" href="#modle-training" title="Permalink to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pysensing.acoustic.inference.training.AcousticPose_utils.hpe_train</span> <span class="kn">import</span> <span class="n">train_model</span><span class="p">,</span><span class="n">generate_configs</span>

<span class="n">args</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;root_dir&quot;</span><span class="p">:</span> <span class="s2">&quot;./data/hpe_dataset/testing_result&quot;</span><span class="p">,</span>
    <span class="s2">&quot;save_name&quot;</span><span class="p">:</span> <span class="s2">&quot;seq1&quot;</span><span class="p">,</span>
    <span class="s2">&quot;input_feature&quot;</span><span class="p">:</span> <span class="s2">&quot;logmel&quot;</span><span class="p">,</span>
    <span class="s2">&quot;batchsize&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
    <span class="s2">&quot;max_epoch&quot;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
    <span class="s2">&quot;csv_path&quot;</span><span class="p">:</span> <span class="s2">&quot;./data/hpe_dataset/csv&quot;</span><span class="p">,</span>
    <span class="s2">&quot;dataset_name&quot;</span><span class="p">:</span> <span class="s2">&quot;pose_regression_timeseries_subject_1&quot;</span><span class="p">,</span>
    <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;speech2pose&quot;</span><span class="p">,</span>
    <span class="s2">&quot;sound_length&quot;</span><span class="p">:</span> <span class="mi">2400</span><span class="p">,</span>
    <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">config_path</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;root_dir&quot;</span><span class="p">]</span><span class="o">+</span><span class="s1">&#39;/&#39;</span><span class="o">+</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;save_name&quot;</span><span class="p">]</span><span class="o">+</span><span class="s2">&quot;/config.yaml&quot;</span>
<span class="n">generate_configs</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
<span class="n">resume_training</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">random_seed</span> <span class="o">=</span> <span class="mi">0</span>

<span class="n">train_model</span><span class="p">(</span>
    <span class="n">config_path</span><span class="o">=</span><span class="n">config_path</span><span class="p">,</span>
    <span class="n">resume</span><span class="o">=</span><span class="n">resume_training</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="n">random_seed</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Modle Training</span>
<span class="c1"># ------------------------</span>
<span class="kn">from</span> <span class="nn">pysensing.acoustic.inference.training.AcousticPose_utils.hpe_test</span> <span class="kn">import</span> <span class="n">evaluate_model</span>
<span class="n">args</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;root_dir&quot;</span><span class="p">:</span> <span class="s2">&quot;./data/hpe_dataset/testing_result&quot;</span><span class="p">,</span>
    <span class="s2">&quot;save_name&quot;</span><span class="p">:</span> <span class="s2">&quot;seq1&quot;</span><span class="p">,</span>
    <span class="s2">&quot;batchsize&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
    <span class="s2">&quot;max_epoch&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
    <span class="s2">&quot;csv_path&quot;</span><span class="p">:</span> <span class="s2">&quot;./data/hpe_dataset/csv&quot;</span><span class="p">,</span>
    <span class="s2">&quot;dataset_name&quot;</span><span class="p">:</span> <span class="s2">&quot;pose_regression_timeseries_subject_1&quot;</span><span class="p">,</span>
    <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;speech2pose&quot;</span><span class="p">,</span>
    <span class="s2">&quot;sound_length&quot;</span><span class="p">:</span> <span class="mi">2400</span><span class="p">,</span>
    <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">config_path</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;root_dir&quot;</span><span class="p">]</span><span class="o">+</span><span class="s1">&#39;/&#39;</span><span class="o">+</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;save_name&quot;</span><span class="p">]</span><span class="o">+</span><span class="s2">&quot;/config.yaml&quot;</span>
<span class="n">evaluation_mode</span> <span class="o">=</span> <span class="s2">&quot;test&quot;</span>
<span class="n">model_path</span> <span class="o">=</span> <span class="kc">None</span>

<span class="n">evaluate_model</span><span class="p">(</span>
    <span class="n">config_path</span><span class="o">=</span><span class="n">config_path</span><span class="p">,</span>
    <span class="n">mode</span><span class="o">=</span><span class="n">evaluation_mode</span><span class="p">,</span>
    <span class="n">model_path</span><span class="o">=</span><span class="n">model_path</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>{&#39;csv_path&#39;: &#39;./data/hpe_dataset/csv&#39;, &#39;model&#39;: &#39;speech2pose&#39;, &#39;pretrained&#39;: True, &#39;use_class_weight&#39;: True, &#39;batch_size&#39;: 32, &#39;width&#39;: 224, &#39;height&#39;: 224, &#39;num_workers&#39;: 8, &#39;max_epoch&#39;: 50, &#39;learning_rate&#39;: 0.01, &#39;sound_length&#39;: 2400, &#39;dataset_name&#39;: &#39;pose_regression_timeseries_subject_1&#39;, &#39;input_feature&#39;: &#39;logmel&#39;, &#39;topk&#39;: (1, 3), &#39;smooth_loss&#39;: False, &#39;ratio&#39;: 0.0, &#39;gan&#39;: &#39;none&#39;, &#39;finetune&#39;: 0, &#39;aug&#39;: &#39;none&#39;}
Finished making configuration files.
/home/kemove/anaconda3/envs/il/lib/python3.9/site-packages/pysensing/acoustic/inference/training/AcousticPose_utils/libs/dataset.py:195: FutureWarning: DataFrame.fillna with &#39;method&#39; is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.
  self.df[self.df[&quot;sound_length&quot;] == sound_length]
/home/kemove/anaconda3/envs/il/lib/python3.9/site-packages/pysensing/acoustic/inference/training/AcousticPose_utils/libs/dataset.py:195: FutureWarning: DataFrame.fillna with &#39;method&#39; is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.
  self.df[self.df[&quot;sound_length&quot;] == sound_length]
epoch: 0        epoch time[sec]: 4      train loss: 35.7390     val loss: 5554.8646     val_rmse: 74.53097      val_mae: 51.34270
epoch: 1        epoch time[sec]: 1      train loss: 2.9727      val loss: 25.6959       val_rmse: 5.06912       val_mae: 3.25518
epoch: 2        epoch time[sec]: 1      train loss: 1.4077      val loss: 2.6241        val_rmse: 1.61991       val_mae: 0.95808
epoch: 3        epoch time[sec]: 1      train loss: 1.1356      val loss: 3.5195        val_rmse: 1.87604       val_mae: 1.04522
epoch: 4        epoch time[sec]: 1      train loss: 0.9243      val loss: 1.5310        val_rmse: 1.23734       val_mae: 0.71184
epoch: 5        epoch time[sec]: 1      train loss: 0.7955      val loss: 3.5594        val_rmse: 1.88662       val_mae: 1.16877
epoch: 6        epoch time[sec]: 1      train loss: 0.8497      val loss: 4.6753        val_rmse: 2.16226       val_mae: 1.17781
epoch: 7        epoch time[sec]: 1      train loss: 0.7119      val loss: 1.2523        val_rmse: 1.11907       val_mae: 0.63488
epoch: 8        epoch time[sec]: 1      train loss: 0.8146      val loss: 1.8044        val_rmse: 1.34328       val_mae: 0.73590
epoch: 9        epoch time[sec]: 1      train loss: 0.6545      val loss: 3.3827        val_rmse: 1.83922       val_mae: 1.10017
epoch: 10       epoch time[sec]: 1      train loss: 0.5552      val loss: 1.1688        val_rmse: 1.08112       val_mae: 0.57759
epoch: 11       epoch time[sec]: 1      train loss: 0.4592      val loss: 2.0147        val_rmse: 1.41939       val_mae: 0.74739
epoch: 12       epoch time[sec]: 1      train loss: 0.4994      val loss: 0.9686        val_rmse: 0.98416       val_mae: 0.55964
epoch: 13       epoch time[sec]: 1      train loss: 0.3855      val loss: 1.2417        val_rmse: 1.11432       val_mae: 0.62025
epoch: 14       epoch time[sec]: 1      train loss: 0.4807      val loss: 12.4831       val_rmse: 3.53314       val_mae: 2.36653
epoch: 15       epoch time[sec]: 1      train loss: 0.4118      val loss: 1.3485        val_rmse: 1.16123       val_mae: 0.66796
epoch: 16       epoch time[sec]: 1      train loss: 0.3761      val loss: 0.9973        val_rmse: 0.99863       val_mae: 0.53796
epoch: 17       epoch time[sec]: 1      train loss: 0.4024      val loss: 0.8047        val_rmse: 0.89707       val_mae: 0.49792
epoch: 18       epoch time[sec]: 1      train loss: 0.3684      val loss: 0.9007        val_rmse: 0.94903       val_mae: 0.52189
epoch: 19       epoch time[sec]: 1      train loss: 0.4444      val loss: 0.5412        val_rmse: 0.73564       val_mae: 0.38856
epoch: 20       epoch time[sec]: 1      train loss: 0.4993      val loss: 3.9111        val_rmse: 1.97765       val_mae: 1.08059
epoch: 21       epoch time[sec]: 1      train loss: 0.5329      val loss: 2.8935        val_rmse: 1.70104       val_mae: 1.00245
epoch: 22       epoch time[sec]: 1      train loss: 0.5511      val loss: 5.5930        val_rmse: 2.36494       val_mae: 1.52599
epoch: 23       epoch time[sec]: 1      train loss: 0.4637      val loss: 0.5622        val_rmse: 0.74982       val_mae: 0.43827
epoch: 24       epoch time[sec]: 1      train loss: 0.3852      val loss: 0.5459        val_rmse: 0.73882       val_mae: 0.41697
epoch: 25       epoch time[sec]: 1      train loss: 0.3494      val loss: 0.5928        val_rmse: 0.76992       val_mae: 0.43026
epoch: 26       epoch time[sec]: 1      train loss: 0.3270      val loss: 1.0129        val_rmse: 1.00643       val_mae: 0.59031
epoch: 27       epoch time[sec]: 1      train loss: 0.3271      val loss: 1.1278        val_rmse: 1.06199       val_mae: 0.58608
epoch: 28       epoch time[sec]: 1      train loss: 0.3561      val loss: 0.8824        val_rmse: 0.93934       val_mae: 0.53670
epoch: 29       epoch time[sec]: 1      train loss: 0.4345      val loss: 2.2521        val_rmse: 1.50071       val_mae: 0.77469
epoch: 30       epoch time[sec]: 1      train loss: 0.3261      val loss: 0.5565        val_rmse: 0.74597       val_mae: 0.42798
epoch: 31       epoch time[sec]: 1      train loss: 0.2789      val loss: 0.2856        val_rmse: 0.53439       val_mae: 0.29199
epoch: 32       epoch time[sec]: 1      train loss: 0.2797      val loss: 0.8026        val_rmse: 0.89586       val_mae: 0.49548
epoch: 33       epoch time[sec]: 1      train loss: 0.2679      val loss: 0.3465        val_rmse: 0.58865       val_mae: 0.31378
epoch: 34       epoch time[sec]: 1      train loss: 0.2232      val loss: 0.3359        val_rmse: 0.57957       val_mae: 0.35535
epoch: 35       epoch time[sec]: 1      train loss: 0.2356      val loss: 0.4082        val_rmse: 0.63887       val_mae: 0.36426
epoch: 36       epoch time[sec]: 1      train loss: 0.2611      val loss: 0.5367        val_rmse: 0.73261       val_mae: 0.40534
epoch: 37       epoch time[sec]: 1      train loss: 0.2392      val loss: 0.6087        val_rmse: 0.78019       val_mae: 0.44737
epoch: 38       epoch time[sec]: 1      train loss: 0.2305      val loss: 0.4172        val_rmse: 0.64594       val_mae: 0.32282
epoch: 39       epoch time[sec]: 1      train loss: 0.2442      val loss: 0.2554        val_rmse: 0.50535       val_mae: 0.28782
epoch: 40       epoch time[sec]: 1      train loss: 0.2222      val loss: 0.2548        val_rmse: 0.50480       val_mae: 0.30691
epoch: 41       epoch time[sec]: 1      train loss: 0.2488      val loss: 0.5324        val_rmse: 0.72965       val_mae: 0.37258
epoch: 42       epoch time[sec]: 1      train loss: 0.1904      val loss: 0.2356        val_rmse: 0.48537       val_mae: 0.26606
epoch: 43       epoch time[sec]: 1      train loss: 0.2384      val loss: 0.3041        val_rmse: 0.55143       val_mae: 0.30847
epoch: 44       epoch time[sec]: 1      train loss: 0.3045      val loss: 1.0510        val_rmse: 1.02519       val_mae: 0.52256
epoch: 45       epoch time[sec]: 1      train loss: 0.3366      val loss: 0.6302        val_rmse: 0.79388       val_mae: 0.45900
epoch: 46       epoch time[sec]: 1      train loss: 0.3774      val loss: 1.3712        val_rmse: 1.17100       val_mae: 0.64678
epoch: 47       epoch time[sec]: 1      train loss: 0.3146      val loss: 2.0887        val_rmse: 1.44523       val_mae: 0.88037
epoch: 48       epoch time[sec]: 1      train loss: 0.5820      val loss: 5.6748        val_rmse: 2.38218       val_mae: 1.50417
epoch: 49       epoch time[sec]: 1      train loss: 0.5284      val loss: 1.1686        val_rmse: 1.08104       val_mae: 0.58750
/home/kemove/anaconda3/envs/il/lib/python3.9/site-packages/pysensing/acoustic/inference/training/AcousticPose_utils/libs/dataset.py:195: FutureWarning: DataFrame.fillna with &#39;method&#39; is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.
  self.df[self.df[&quot;sound_length&quot;] == sound_length]
---------- Start evaluation for test data ----------
loss: 0.23558   RMSE: 0.49      MAE: 0.27       Acc: 0.87, 0.97, 0.99, 0.99
arm RMSE: 0.66  arm MAE: 0.39   Acc: 0.80, 0.95, 0.98, 0.99
leg RMSE: 0.35  leg MAE: 0.20   Acc: 0.95, 0.99, 0.99, 1.00
body RMSE: 0.32 body MAE: 0.17  Acc: 0.94, 1.00, 1.00, 1.00
Done.
</pre></div>
</div>
</section>
<section id="load-the-wipose-lstm-model">
<h2>Load the Wipose_LSTM model<a class="headerlink" href="#load-the-wipose-lstm-model" title="Permalink to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Method 1</span>
<span class="n">hpe_model</span> <span class="o">=</span> <span class="n">Wipose_LSTM</span><span class="p">(</span><span class="n">in_cha</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span><span class="n">out_cha</span><span class="o">=</span><span class="mi">63</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># model_path = &#39;path to trained model&#39;</span>
<span class="c1"># state_dict = torch.load(model_path,weights_only=True)</span>

<span class="c1"># Method 2</span>
<span class="n">hpe_model</span> <span class="o">=</span> <span class="n">load_hpe_model</span><span class="p">(</span><span class="s1">&#39;wipose&#39;</span><span class="p">,</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">task</span><span class="o">=</span><span class="s1">&#39;subject8&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>/home/kemove/anaconda3/envs/il/lib/python3.9/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn(&quot;dropout option adds dropout after all but last &quot;
</pre></div>
</div>
</section>
<section id="id1">
<h2>Load the data<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">csv</span> <span class="o">=</span> <span class="s1">&#39;./data/hpe_dataset/csv/pose_regression_timeseries_subject_8/test.csv&#39;</span> <span class="c1"># The path contains the samosa dataset</span>
<span class="n">hpe_testdataset</span> <span class="o">=</span> <span class="n">SoundPoseLSTMDataset</span><span class="p">(</span><span class="n">csv</span><span class="p">,</span><span class="n">sound_length</span><span class="o">=</span><span class="mi">2400</span><span class="p">,</span><span class="n">input_feature</span><span class="o">=</span><span class="s1">&#39;raw&#39;</span><span class="p">,</span><span class="n">mean</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">get_raw_mean</span><span class="p">())</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">),</span><span class="n">std</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">get_raw_std</span><span class="p">())</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">))</span>
<span class="n">index</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># Randomly select an index</span>
<span class="n">sample</span><span class="o">=</span> <span class="n">hpe_testdataset</span><span class="o">.</span><span class="fm">__getitem__</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="model-inference">
<h2>Model inference<a class="headerlink" href="#model-inference" title="Permalink to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Method 1</span>
<span class="n">hpe_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">predicted_result</span> <span class="o">=</span> <span class="n">hpe_model</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;sound&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="n">vis_images</span> <span class="o">=</span> <span class="n">make_images</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;targets&#39;</span><span class="p">],</span><span class="n">predicted_result</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="c1">#Method 2</span>
<span class="kn">from</span> <span class="nn">pysensing.acoustic.inference.predict</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">predicted_result</span>  <span class="o">=</span> <span class="n">hpe_predict</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;sound&#39;</span><span class="p">],</span><span class="s1">&#39;SoundPoseLSTMDataset&#39;</span><span class="p">,</span><span class="n">hpe_model</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">vis_images</span> <span class="o">=</span> <span class="n">make_images</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;targets&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span><span class="n">predicted_result</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="n">seq_num</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">vis_images</span><span class="p">[</span><span class="n">seq_num</span><span class="p">][</span><span class="s1">&#39;img&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_acoustic_hpe_tutorial_002.png" srcset="../_images/sphx_glr_acoustic_hpe_tutorial_002.png" alt="acoustic hpe tutorial" class = "sphx-glr-single-img"/></section>
<section id="model-embedding">
<h2>Model embedding<a class="headerlink" href="#model-embedding" title="Permalink to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pysensing.acoustic.inference.embedding</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">sample_embedding</span> <span class="o">=</span> <span class="n">hpe_embedding</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;sound&#39;</span><span class="p">],</span><span class="s1">&#39;SoundPoseLSTMDataset&#39;</span><span class="p">,</span><span class="n">hpe_model</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>And that’s it. We’re done with our acoustic humna pose estimation tutorials. Thanks for reading.</p>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (3 minutes 13.995 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-acoustic-acoustic-hpe-tutorial-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/7a6f102b877e68e9956fa06c9728eff5/acoustic_hpe_tutorial.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">acoustic_hpe_tutorial.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/3f90f032a2f8d490ea6cc694e6ba4690/acoustic_hpe_tutorial.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">acoustic_hpe_tutorial.py</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../bluetooth/bluetooth_localization_tutorial.html" class="btn btn-neutral float-right" title="Bluetooth Localization Tutorial" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="acoustic_ped_det_tutorial.html" class="btn btn-neutral" title="Acoustic Pedestrian Detection Tutorial" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr class="rating-hr hr-top">
      <div class="rating-container">
        <div class="rating-prompt">Rate this Tutorial</div>
        <div class="stars-outer">
          <i class="far fa-star" title="1 Star" data-behavior="tutorial-rating" data-count="1"></i>
          <i class="far fa-star" title="2 Stars" data-behavior="tutorial-rating" data-count="2"></i>
          <i class="far fa-star" title="3 Stars" data-behavior="tutorial-rating" data-count="3"></i>
          <i class="far fa-star" title="4 Stars" data-behavior="tutorial-rating" data-count="4"></i>
          <i class="far fa-star" title="5 Stars" data-behavior="tutorial-rating" data-count="5"></i>
        </div>
      </div>
    <hr class="rating-hr hr-bottom"/>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2024, Pysensing contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>
<!--        <div class="pytorch-content-right" id="pytorch-content-right">-->
<!--          <div class="pytorch-right-menu" id="pytorch-right-menu">-->
<!--              <a class="twitter-timeline" data-width="100%" href="https://twitter.com/pypose_org?ref_src=twsrc%5Etfw">News from Twitter @pypose_rog<br>You need VPN if you see this.</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>-->
<!--          </div>-->
<!--        </div>-->
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../_static/doctools.js"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access documentation for Pysensing</p>
          <a class="with-right-arrow" href="https://github.com/pysensing/pysensing">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get started with tutorials and examples</p>
          <a class="with-right-arrow" href="https://github.com/pysensing/pysensing">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Get Started</h2>
          <p>Find resources and how to start using pysensing</p>
          <a class="with-right-arrow" href="https://github.com/pysensing/pysensing">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://github.com/pysensing/pysensing" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://github.com/pysensing/pysensing">Pysensing</a></li>
            <li><a href="https://github.com/pysensing/pysensing">Get Started</a></li>
            <li><a href="https://github.com/pysensing/pysensing">Features</a></li>
            <li><a href="https://github.com/pysensing/pysensing">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://github.com/pysensing/pysensing">Resources</a></li>
            <li><a href="https://github.com/pysensing/pysensing">Tutorials</a></li>
            <li><a href="https://github.com/pysensing/pysensing">Docs</a></li>
            <li><a href="https://github.com/pysensing/pysensing" target="_blank">Github Issues</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://github.com/pysensing/pysensing" target="_blank">Twitter</a></li>
            <li><a href="https://github.com/pysensing/pysensing" target="_blank">GitHub</a></li>

          </ul>  
          </div>
        </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://github.com/pysensing/pysensing" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/pysensing/pysensing">Get Started</a>
          </li>
          <li>
            <a href="https://github.com/pysensing/pysensing">Tutorials</a>
          </li>
          <li>
            <a href="https://github.com/pysensing/pysensing">Docs</a>
          </li>
          <li>
            <a href="https://github.com/pysensing/pysensing">About Us</a>
          </li>
          <li>
            <a href="https://github.com/pysensing/pysensing">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>