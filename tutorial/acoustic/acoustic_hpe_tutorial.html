


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Acoustic Human Pose estimation Tutorial &mdash; Pysensing Tutorials  documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-rendered-html.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="CSI Augmentation.adding_noise Tutorial" href="../csi/add_noise_tutorial.html" />
    <link rel="prev" title="Acoustic Pedestrian Detection Tutorial" href="acoustic_ped_det_tutorial.html" />
  <!-- Google Analytics -->
  
  <!-- End Google Analytics -->
  

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://github.com/pysensing/pysensing" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/pysensing/pysensing">Get Started</a>
          </li>

          <!-- <li class="active"> -->
          <li>
            <a href="https://github.com/pysensing/pysensing">Tutorials</a>
          </li>

          <!-- <li> -->
          <li>
            <a href="https://yizhuoyang.github.io/html_test/docs/main/index.html">Doc</a>
          </li>

          <li>
            <a href="https://github.com/pysensing/pysensing">About Us</a>
          </li>

          <li>
            <a href="https://github.com/pysensing/pysensing">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Tutorials" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Acoustic</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="acoustic_filter_tutorial.html">Acoustic Preprocessing.Filter Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="acoustic_transform_tutorial.html">Acoustic Preprocssing.Transfrom Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="acoustic_sig_aug_tutorial.html">Acoustic Augmentation.Signal_aug Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="acoustic_spectrogram_aug_tutorial.html">Acoustic augmentation.spectrogram_aug Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="acoustic_har_tutorial.html">Acoustic Human Activity Recognition Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="acoustic_hgr_tutorial.html">Acoustic Hand Gesture Recognition Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="acoustic_ped_det_tutorial.html">Acoustic Pedestrian Detection Tutorial</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Acoustic Human Pose estimation Tutorial</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">CSI</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../csi/add_noise_tutorial.html">CSI Augmentation.adding_noise Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../csi/deformation_tutorial.html">CSI Preprocessing.transform Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../csi/denoising_tutorial.html">CSI Preprocessing.denoising Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../csi/HAR_HID_tutorial.html">CSI Augmentation.deformation Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../csi/HFD_tutorial.html">CSI classification tasks Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../csi/HPE_tutorial.html">CSI human pose estimation Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../csi/Reconstruction_tutorial.html">CSI human reconstruction Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../csi/transform_tutorial.html">CSI human fall detection Tutorial</a></li>
</ul>


            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Tutorials
          
        </a> &gt;
      </li>

        
      <li>Acoustic Human Pose estimation Tutorial</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/acoustic/acoustic_hpe_tutorial.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <div class="pytorch-call-to-action-links">
            <div id="tutorial-type">acoustic/acoustic_hpe_tutorial</div>

            <div id="google-colab-link">
              <img class="call-to-action-img" src="../_static/images/pytorch-colab.svg"/>
              <div class="call-to-action-desktop-view">Run in Google Colab</div>
              <div class="call-to-action-mobile-view">Colab</div>
            </div>
            <div id="download-notebook-link">
              <img class="call-to-action-notebook-img" src="../_static/images/pytorch-download.svg"/>
              <div class="call-to-action-desktop-view">Download Notebook</div>
              <div class="call-to-action-mobile-view">Notebook</div>
            </div>
            <div id="github-view-link">
              <img class="call-to-action-img" src="../_static/images/pytorch-github.svg"/>
              <div class="call-to-action-desktop-view">View on GitHub</div>
              <div class="call-to-action-mobile-view">GitHub</div>
            </div>
          </div>

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-acoustic-acoustic-hpe-tutorial-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="acoustic-human-pose-estimation-tutorial">
<span id="sphx-glr-acoustic-acoustic-hpe-tutorial-py"></span><h1>Acoustic Human Pose estimation Tutorial<a class="headerlink" href="#acoustic-human-pose-estimation-tutorial" title="Permalink to this heading">¶</a></h1>
<p>!pip install pysensing</p>
<p>In this tutorial, we will be implementing codes for acoustic Human pose estimation</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pysensing.acoustic.datasets.utils.hpe_vis</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pysensing.acoustic.models.hpe</span> <span class="kn">import</span> <span class="n">Speech2pose</span><span class="p">,</span><span class="n">Wipose_LSTM</span>
<span class="kn">from</span> <span class="nn">pysensing.acoustic.models.get_model</span> <span class="kn">import</span> <span class="n">load_hpe_model</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="simple">
<dt>Listening Human Behavior: 3D Human Pose Estimation with Acoustic Signals</dt><dd></dd>
</dl>
<p>Implementation of “Listening Human Behavior: 3D Human Pose Estimation with Acoustic Signals”.</p>
<p>This dataset contains the audio reflected by human to estimate the 3D human pose with the acoustic signals.</p>
<p>Reference: <a class="reference external" href="https://github.com/YutoShibata07/AcousticPose_Public">https://github.com/YutoShibata07/AcousticPose_Public</a></p>
<section id="load-the-data">
<h2>Load the data<a class="headerlink" href="#load-the-data" title="Permalink to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Method 1: Use get_dataloader</span>
<span class="kn">from</span> <span class="nn">pysensing.acoustic.datasets.get_dataloader</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">train_loader</span><span class="p">,</span><span class="n">val_loader</span><span class="p">,</span><span class="n">test_loader</span> <span class="o">=</span> <span class="n">load_hpe_dataset</span><span class="p">(</span>
    <span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span>
    <span class="n">dataset_name</span><span class="o">=</span><span class="s1">&#39;pose_regression_timeseries_subject_1&#39;</span><span class="p">,</span>
    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Method 2</span>
<span class="n">csv</span> <span class="o">=</span> <span class="s1">&#39;./data/hpe_dataset/csv/pose_regression_timeseries_subject_1/test.csv&#39;</span> <span class="c1"># The path contains the samosa dataset</span>
<span class="n">data_dir</span> <span class="o">=</span> <span class="s1">&#39;./data&#39;</span>
<span class="n">hpe_testdataset</span> <span class="o">=</span> <span class="n">SoundPose2DDataset</span><span class="p">(</span><span class="n">csv</span><span class="p">,</span><span class="n">sound_length</span><span class="o">=</span><span class="mi">2400</span><span class="p">,</span><span class="n">input_feature</span><span class="o">=</span><span class="s1">&#39;logmel&#39;</span><span class="p">,</span>
                                     <span class="n">mean</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">get_mean</span><span class="p">())</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)[:</span><span class="mi">4</span><span class="p">],</span>
                                     <span class="n">std</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">get_std</span><span class="p">())</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)[:</span><span class="mi">4</span><span class="p">],</span>
                                     <span class="p">)</span>
<span class="n">index</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># Randomly select an index</span>
<span class="n">sample</span><span class="o">=</span> <span class="n">hpe_testdataset</span><span class="o">.</span><span class="fm">__getitem__</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;targets&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;sound&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Size([12, 63])
torch.Size([4, 12, 128])
</pre></div>
</div>
</section>
<section id="load-speech2pose-model">
<h2>Load Speech2pose model<a class="headerlink" href="#load-speech2pose-model" title="Permalink to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Method 1</span>
<span class="n">hpe_model</span> <span class="o">=</span> <span class="n">Speech2pose</span><span class="p">(</span><span class="n">out_cha</span><span class="o">=</span><span class="mi">63</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># model_path = &#39;path to pretrian weights&#39;</span>
<span class="c1"># state_dict = torch.load(model_path,weights_only=True)</span>
<span class="c1"># hpe_model.load_state_dict(state_dict)</span>

<span class="c1"># Method 2</span>
<span class="n">hpe_model</span> <span class="o">=</span> <span class="n">load_hpe_model</span><span class="p">(</span><span class="s1">&#39;speech2pose&#39;</span><span class="p">,</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">task</span><span class="o">=</span><span class="s1">&#39;subject8&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="modle-inference">
<h2>Modle Inference<a class="headerlink" href="#modle-inference" title="Permalink to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1">#Method 1</span>
<span class="n">sample</span><span class="o">=</span> <span class="n">hpe_testdataset</span><span class="o">.</span><span class="fm">__getitem__</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
<span class="n">hpe_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">predicted_result</span> <span class="o">=</span> <span class="n">hpe_model</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;sound&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="n">vis_images</span> <span class="o">=</span> <span class="n">make_images</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;targets&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span><span class="n">predicted_result</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="c1">#Method 2</span>
<span class="kn">from</span> <span class="nn">pysensing.acoustic.inference.predict</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">predicted_result</span>  <span class="o">=</span> <span class="n">hpe_predict</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;sound&#39;</span><span class="p">],</span><span class="s1">&#39;SoundPose2DDataset&#39;</span><span class="p">,</span><span class="n">hpe_model</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">vis_images</span> <span class="o">=</span> <span class="n">make_images</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;targets&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span><span class="n">predicted_result</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="n">seq_num</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">vis_images</span><span class="p">[</span><span class="n">seq_num</span><span class="p">][</span><span class="s1">&#39;img&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_acoustic_hpe_tutorial_001.png" srcset="../_images/sphx_glr_acoustic_hpe_tutorial_001.png" alt="acoustic hpe tutorial" class = "sphx-glr-single-img"/><div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>/home/kemove/anaconda3/envs/open-mmlab/lib/python3.8/site-packages/torch/nn/modules/conv.py:304: UserWarning: Using padding=&#39;same&#39; with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1031.)
  return F.conv1d(input, weight, bias, self.stride,
</pre></div>
</div>
</section>
<section id="modle-embedding">
<h2>Modle Embedding<a class="headerlink" href="#modle-embedding" title="Permalink to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pysensing.acoustic.inference.embedding</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">sample_embedding</span> <span class="o">=</span> <span class="n">hpe_embedding</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;sound&#39;</span><span class="p">],</span><span class="s1">&#39;SoundPose2DDataset&#39;</span><span class="p">,</span><span class="n">hpe_model</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="modle-training">
<h2>Modle Training<a class="headerlink" href="#modle-training" title="Permalink to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pysensing.acoustic.inference.training.AcousticPose_utils.hpe_train</span> <span class="kn">import</span> <span class="n">train_model</span><span class="p">,</span><span class="n">generate_configs</span>

<span class="n">args</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;root_dir&quot;</span><span class="p">:</span> <span class="s2">&quot;./data/hpe_dataset/testing_result&quot;</span><span class="p">,</span>
    <span class="s2">&quot;save_name&quot;</span><span class="p">:</span> <span class="s2">&quot;seq1&quot;</span><span class="p">,</span>
    <span class="s2">&quot;input_feature&quot;</span><span class="p">:</span> <span class="s2">&quot;logmel&quot;</span><span class="p">,</span>
    <span class="s2">&quot;batchsize&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
    <span class="s2">&quot;max_epoch&quot;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
    <span class="s2">&quot;csv_path&quot;</span><span class="p">:</span> <span class="s2">&quot;./data/hpe_dataset/csv&quot;</span><span class="p">,</span>
    <span class="s2">&quot;dataset_name&quot;</span><span class="p">:</span> <span class="s2">&quot;pose_regression_timeseries_subject_1&quot;</span><span class="p">,</span>
    <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;speech2pose&quot;</span><span class="p">,</span>
    <span class="s2">&quot;sound_length&quot;</span><span class="p">:</span> <span class="mi">2400</span><span class="p">,</span>
    <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">config_path</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;root_dir&quot;</span><span class="p">]</span><span class="o">+</span><span class="s1">&#39;/&#39;</span><span class="o">+</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;save_name&quot;</span><span class="p">]</span><span class="o">+</span><span class="s2">&quot;/config.yaml&quot;</span>
<span class="n">generate_configs</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
<span class="n">resume_training</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">random_seed</span> <span class="o">=</span> <span class="mi">0</span>

<span class="n">train_model</span><span class="p">(</span>
    <span class="n">config_path</span><span class="o">=</span><span class="n">config_path</span><span class="p">,</span>
    <span class="n">resume</span><span class="o">=</span><span class="n">resume_training</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="n">random_seed</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Modle Training</span>
<span class="c1"># ------------------------</span>
<span class="kn">from</span> <span class="nn">pysensing.acoustic.inference.training.AcousticPose_utils.hpe_test</span> <span class="kn">import</span> <span class="n">evaluate_model</span>
<span class="n">args</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;root_dir&quot;</span><span class="p">:</span> <span class="s2">&quot;./data/hpe_dataset/testing_result&quot;</span><span class="p">,</span>
    <span class="s2">&quot;save_name&quot;</span><span class="p">:</span> <span class="s2">&quot;seq1&quot;</span><span class="p">,</span>
    <span class="s2">&quot;batchsize&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
    <span class="s2">&quot;max_epoch&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
    <span class="s2">&quot;csv_path&quot;</span><span class="p">:</span> <span class="s2">&quot;./data/hpe_dataset/csv&quot;</span><span class="p">,</span>
    <span class="s2">&quot;dataset_name&quot;</span><span class="p">:</span> <span class="s2">&quot;pose_regression_timeseries_subject_1&quot;</span><span class="p">,</span>
    <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;speech2pose&quot;</span><span class="p">,</span>
    <span class="s2">&quot;sound_length&quot;</span><span class="p">:</span> <span class="mi">2400</span><span class="p">,</span>
    <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">config_path</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;root_dir&quot;</span><span class="p">]</span><span class="o">+</span><span class="s1">&#39;/&#39;</span><span class="o">+</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;save_name&quot;</span><span class="p">]</span><span class="o">+</span><span class="s2">&quot;/config.yaml&quot;</span>
<span class="n">evaluation_mode</span> <span class="o">=</span> <span class="s2">&quot;test&quot;</span>
<span class="n">model_path</span> <span class="o">=</span> <span class="kc">None</span>

<span class="n">evaluate_model</span><span class="p">(</span>
    <span class="n">config_path</span><span class="o">=</span><span class="n">config_path</span><span class="p">,</span>
    <span class="n">mode</span><span class="o">=</span><span class="n">evaluation_mode</span><span class="p">,</span>
    <span class="n">model_path</span><span class="o">=</span><span class="n">model_path</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>{&#39;csv_path&#39;: &#39;./data/hpe_dataset/csv&#39;, &#39;model&#39;: &#39;speech2pose&#39;, &#39;pretrained&#39;: True, &#39;use_class_weight&#39;: True, &#39;batch_size&#39;: 32, &#39;width&#39;: 224, &#39;height&#39;: 224, &#39;num_workers&#39;: 8, &#39;max_epoch&#39;: 50, &#39;learning_rate&#39;: 0.01, &#39;sound_length&#39;: 2400, &#39;dataset_name&#39;: &#39;pose_regression_timeseries_subject_1&#39;, &#39;input_feature&#39;: &#39;logmel&#39;, &#39;topk&#39;: (1, 3), &#39;smooth_loss&#39;: False, &#39;ratio&#39;: 0.0, &#39;gan&#39;: &#39;none&#39;, &#39;finetune&#39;: 0, &#39;aug&#39;: &#39;none&#39;}
Finished making configuration files.
INFO - 2024-11-20 11:28:14,093 - seed - Finished setting up seed.
INFO - 2024-11-20 11:28:14,094 - config - Experiment Configuration
{&#39;aug&#39;: &#39;none&#39;,
 &#39;batch_size&#39;: 32,
 &#39;csv_path&#39;: &#39;./data/hpe_dataset/csv&#39;,
 &#39;dataset_name&#39;: &#39;pose_regression_timeseries_subject_1&#39;,
 &#39;finetune&#39;: 0,
 &#39;gan&#39;: &#39;none&#39;,
 &#39;height&#39;: 224,
 &#39;input_feature&#39;: &#39;logmel&#39;,
 &#39;learning_rate&#39;: 0.01,
 &#39;max_epoch&#39;: 50,
 &#39;model&#39;: &#39;speech2pose&#39;,
 &#39;num_workers&#39;: 8,
 &#39;pretrained&#39;: True,
 &#39;ratio&#39;: 0.0,
 &#39;smooth_loss&#39;: False,
 &#39;sound_length&#39;: 2400,
 &#39;topk&#39;: (1,
          3),
 &#39;use_class_weight&#39;: True,
 &#39;width&#39;: 224}
INFO - 2024-11-20 11:28:14,094 - config - successfully loaded configuration.
INFO - 2024-11-20 11:28:14,094 - dataset - Dataset: pose_regression_timeseries_subject_1        Split: train    Batch size: 32.
INFO - 2024-11-20 11:28:14,094 - mean_std - mean value: [-23.539913, -25.119139, -26.711174, -22.011652, 0.0044820304, 0.00080580305, -0.040803947]
INFO - 2024-11-20 11:28:14,094 - mean_std - std value: [12.967929, 10.571223, 10.273731, 11.8457985, 0.022749536, 0.019968906, 0.037085325]
INFO - 2024-11-20 11:28:14,918 - dataset - the number of samples: 9708
INFO - 2024-11-20 11:28:14,918 - dataset - Dataset: pose_regression_timeseries_subject_1        Split: val      Batch size: 1.
INFO - 2024-11-20 11:28:14,918 - mean_std - mean value: [-23.539913, -25.119139, -26.711174, -22.011652, 0.0044820304, 0.00080580305, -0.040803947]
INFO - 2024-11-20 11:28:14,918 - mean_std - std value: [12.967929, 10.571223, 10.273731, 11.8457985, 0.022749536, 0.019968906, 0.037085325]
INFO - 2024-11-20 11:28:15,124 - dataset - the number of samples: 2424
INFO - 2024-11-20 11:28:15,125 - __init__ - speech2pose will be used as a model.
INFO - 2024-11-20 11:28:15,148 - hpe_train - Start training.
INFO - 2024-11-20 11:28:20,287 - logger - epoch: 0      epoch time[sec]: 4      lr: 0.01        train loss: 35.9078     val loss: 49.4077       val_rmse: 7.02906       val_mae: 4.15302
epoch: 0        epoch time[sec]: 4      train loss: 35.9078     val loss: 49.4077       val_rmse: 7.02906       val_mae: 4.15302
INFO - 2024-11-20 11:28:22,737 - logger - epoch: 1      epoch time[sec]: 1      lr: 0.01        train loss: 3.2186      val loss: 5.1915        val_rmse: 2.27848       val_mae: 1.30127
epoch: 1        epoch time[sec]: 1      train loss: 3.2186      val loss: 5.1915        val_rmse: 2.27848       val_mae: 1.30127
INFO - 2024-11-20 11:28:25,317 - logger - epoch: 2      epoch time[sec]: 1      lr: 0.01        train loss: 1.7873      val loss: 2.0800        val_rmse: 1.44220       val_mae: 0.88617
epoch: 2        epoch time[sec]: 1      train loss: 1.7873      val loss: 2.0800        val_rmse: 1.44220       val_mae: 0.88617
INFO - 2024-11-20 11:28:27,934 - logger - epoch: 3      epoch time[sec]: 1      lr: 0.01        train loss: 1.3931      val loss: 2.1673        val_rmse: 1.47219       val_mae: 0.89062
epoch: 3        epoch time[sec]: 1      train loss: 1.3931      val loss: 2.1673        val_rmse: 1.47219       val_mae: 0.89062
INFO - 2024-11-20 11:28:30,402 - logger - epoch: 4      epoch time[sec]: 1      lr: 0.01        train loss: 1.3545      val loss: 2.2021        val_rmse: 1.48395       val_mae: 0.89224
epoch: 4        epoch time[sec]: 1      train loss: 1.3545      val loss: 2.2021        val_rmse: 1.48395       val_mae: 0.89224
INFO - 2024-11-20 11:28:33,262 - logger - epoch: 5      epoch time[sec]: 1      lr: 0.01        train loss: 1.2792      val loss: 1.3940        val_rmse: 1.18067       val_mae: 0.75314
epoch: 5        epoch time[sec]: 1      train loss: 1.2792      val loss: 1.3940        val_rmse: 1.18067       val_mae: 0.75314
INFO - 2024-11-20 11:28:37,502 - logger - epoch: 6      epoch time[sec]: 1      lr: 0.01        train loss: 1.3827      val loss: 1.4903        val_rmse: 1.22078       val_mae: 0.70531
epoch: 6        epoch time[sec]: 1      train loss: 1.3827      val loss: 1.4903        val_rmse: 1.22078       val_mae: 0.70531
INFO - 2024-11-20 11:28:41,280 - logger - epoch: 7      epoch time[sec]: 1      lr: 0.01        train loss: 1.1380      val loss: 1.3884        val_rmse: 1.17830       val_mae: 0.75178
epoch: 7        epoch time[sec]: 1      train loss: 1.1380      val loss: 1.3884        val_rmse: 1.17830       val_mae: 0.75178
INFO - 2024-11-20 11:28:43,999 - logger - epoch: 8      epoch time[sec]: 1      lr: 0.01        train loss: 1.2759      val loss: 3.4145        val_rmse: 1.84783       val_mae: 0.99689
epoch: 8        epoch time[sec]: 1      train loss: 1.2759      val loss: 3.4145        val_rmse: 1.84783       val_mae: 0.99689
INFO - 2024-11-20 11:28:46,769 - logger - epoch: 9      epoch time[sec]: 1      lr: 0.01        train loss: 1.2372      val loss: 7.3385        val_rmse: 2.70897       val_mae: 1.73976
epoch: 9        epoch time[sec]: 1      train loss: 1.2372      val loss: 7.3385        val_rmse: 2.70897       val_mae: 1.73976
INFO - 2024-11-20 11:28:49,315 - logger - epoch: 10     epoch time[sec]: 1      lr: 0.01        train loss: 1.1067      val loss: 2.0695        val_rmse: 1.43859       val_mae: 0.87368
epoch: 10       epoch time[sec]: 1      train loss: 1.1067      val loss: 2.0695        val_rmse: 1.43859       val_mae: 0.87368
INFO - 2024-11-20 11:28:51,860 - logger - epoch: 11     epoch time[sec]: 1      lr: 0.01        train loss: 1.0034      val loss: 1.5623        val_rmse: 1.24992       val_mae: 0.71251
epoch: 11       epoch time[sec]: 1      train loss: 1.0034      val loss: 1.5623        val_rmse: 1.24992       val_mae: 0.71251
INFO - 2024-11-20 11:28:54,337 - logger - epoch: 12     epoch time[sec]: 1      lr: 0.01        train loss: 0.9604      val loss: 1.7297        val_rmse: 1.31517       val_mae: 0.73883
epoch: 12       epoch time[sec]: 1      train loss: 0.9604      val loss: 1.7297        val_rmse: 1.31517       val_mae: 0.73883
INFO - 2024-11-20 11:28:56,880 - logger - epoch: 13     epoch time[sec]: 1      lr: 0.01        train loss: 0.7869      val loss: 1.7395        val_rmse: 1.31890       val_mae: 0.71431
epoch: 13       epoch time[sec]: 1      train loss: 0.7869      val loss: 1.7395        val_rmse: 1.31890       val_mae: 0.71431
INFO - 2024-11-20 11:28:59,431 - logger - epoch: 14     epoch time[sec]: 1      lr: 0.01        train loss: 0.8848      val loss: 3.3268        val_rmse: 1.82395       val_mae: 1.01679
epoch: 14       epoch time[sec]: 1      train loss: 0.8848      val loss: 3.3268        val_rmse: 1.82395       val_mae: 1.01679
INFO - 2024-11-20 11:29:01,905 - logger - epoch: 15     epoch time[sec]: 1      lr: 0.01        train loss: 0.7021      val loss: 1.3354        val_rmse: 1.15559       val_mae: 0.66640
epoch: 15       epoch time[sec]: 1      train loss: 0.7021      val loss: 1.3354        val_rmse: 1.15559       val_mae: 0.66640
INFO - 2024-11-20 11:29:04,488 - logger - epoch: 16     epoch time[sec]: 1      lr: 0.01        train loss: 0.6145      val loss: 2.7139        val_rmse: 1.64740       val_mae: 0.94755
epoch: 16       epoch time[sec]: 1      train loss: 0.6145      val loss: 2.7139        val_rmse: 1.64740       val_mae: 0.94755
INFO - 2024-11-20 11:29:07,013 - logger - epoch: 17     epoch time[sec]: 1      lr: 0.01        train loss: 0.6034      val loss: 0.9290        val_rmse: 0.96386       val_mae: 0.54778
epoch: 17       epoch time[sec]: 1      train loss: 0.6034      val loss: 0.9290        val_rmse: 0.96386       val_mae: 0.54778
INFO - 2024-11-20 11:29:09,586 - logger - epoch: 18     epoch time[sec]: 1      lr: 0.01        train loss: 0.5561      val loss: 1.1131        val_rmse: 1.05501       val_mae: 0.62210
epoch: 18       epoch time[sec]: 1      train loss: 0.5561      val loss: 1.1131        val_rmse: 1.05501       val_mae: 0.62210
INFO - 2024-11-20 11:29:12,070 - logger - epoch: 19     epoch time[sec]: 1      lr: 0.01        train loss: 0.6654      val loss: 3.0080        val_rmse: 1.73435       val_mae: 1.06267
epoch: 19       epoch time[sec]: 1      train loss: 0.6654      val loss: 3.0080        val_rmse: 1.73435       val_mae: 1.06267
INFO - 2024-11-20 11:29:15,016 - logger - epoch: 20     epoch time[sec]: 1      lr: 0.01        train loss: 0.5328      val loss: 2.1986        val_rmse: 1.48275       val_mae: 0.90172
epoch: 20       epoch time[sec]: 1      train loss: 0.5328      val loss: 2.1986        val_rmse: 1.48275       val_mae: 0.90172
INFO - 2024-11-20 11:29:17,868 - logger - epoch: 21     epoch time[sec]: 1      lr: 0.01        train loss: 0.4290      val loss: 1.2125        val_rmse: 1.10113       val_mae: 0.63939
epoch: 21       epoch time[sec]: 1      train loss: 0.4290      val loss: 1.2125        val_rmse: 1.10113       val_mae: 0.63939
INFO - 2024-11-20 11:29:21,281 - logger - epoch: 22     epoch time[sec]: 1      lr: 0.01        train loss: 0.4415      val loss: 0.8623        val_rmse: 0.92863       val_mae: 0.48014
epoch: 22       epoch time[sec]: 1      train loss: 0.4415      val loss: 0.8623        val_rmse: 0.92863       val_mae: 0.48014
INFO - 2024-11-20 11:29:24,821 - logger - epoch: 23     epoch time[sec]: 1      lr: 0.01        train loss: 0.4289      val loss: 1.4516        val_rmse: 1.20482       val_mae: 0.68310
epoch: 23       epoch time[sec]: 1      train loss: 0.4289      val loss: 1.4516        val_rmse: 1.20482       val_mae: 0.68310
INFO - 2024-11-20 11:29:28,180 - logger - epoch: 24     epoch time[sec]: 1      lr: 0.01        train loss: 0.4061      val loss: 1.3787        val_rmse: 1.17419       val_mae: 0.68952
epoch: 24       epoch time[sec]: 1      train loss: 0.4061      val loss: 1.3787        val_rmse: 1.17419       val_mae: 0.68952
INFO - 2024-11-20 11:29:31,329 - logger - epoch: 25     epoch time[sec]: 1      lr: 0.01        train loss: 0.3865      val loss: 0.7943        val_rmse: 0.89126       val_mae: 0.45662
epoch: 25       epoch time[sec]: 1      train loss: 0.3865      val loss: 0.7943        val_rmse: 0.89126       val_mae: 0.45662
INFO - 2024-11-20 11:29:33,920 - logger - epoch: 26     epoch time[sec]: 1      lr: 0.01        train loss: 0.3832      val loss: 1.1497        val_rmse: 1.07224       val_mae: 0.55443
epoch: 26       epoch time[sec]: 1      train loss: 0.3832      val loss: 1.1497        val_rmse: 1.07224       val_mae: 0.55443
INFO - 2024-11-20 11:29:36,411 - logger - epoch: 27     epoch time[sec]: 1      lr: 0.01        train loss: 0.3956      val loss: 0.9122        val_rmse: 0.95508       val_mae: 0.51790
epoch: 27       epoch time[sec]: 1      train loss: 0.3956      val loss: 0.9122        val_rmse: 0.95508       val_mae: 0.51790
INFO - 2024-11-20 11:29:39,873 - logger - epoch: 28     epoch time[sec]: 1      lr: 0.01        train loss: 0.3577      val loss: 1.4741        val_rmse: 1.21414       val_mae: 0.62630
epoch: 28       epoch time[sec]: 1      train loss: 0.3577      val loss: 1.4741        val_rmse: 1.21414       val_mae: 0.62630
INFO - 2024-11-20 11:29:42,881 - logger - epoch: 29     epoch time[sec]: 1      lr: 0.01        train loss: 0.4601      val loss: 4.6237        val_rmse: 2.15028       val_mae: 1.28956
epoch: 29       epoch time[sec]: 1      train loss: 0.4601      val loss: 4.6237        val_rmse: 2.15028       val_mae: 1.28956
INFO - 2024-11-20 11:29:46,064 - logger - epoch: 30     epoch time[sec]: 1      lr: 0.01        train loss: 0.4339      val loss: 3.5612        val_rmse: 1.88711       val_mae: 1.08993
epoch: 30       epoch time[sec]: 1      train loss: 0.4339      val loss: 3.5612        val_rmse: 1.88711       val_mae: 1.08993
INFO - 2024-11-20 11:29:48,939 - logger - epoch: 31     epoch time[sec]: 1      lr: 0.01        train loss: 0.4145      val loss: 1.6004        val_rmse: 1.26508       val_mae: 0.68883
epoch: 31       epoch time[sec]: 1      train loss: 0.4145      val loss: 1.6004        val_rmse: 1.26508       val_mae: 0.68883
INFO - 2024-11-20 11:29:51,428 - logger - epoch: 32     epoch time[sec]: 1      lr: 0.01        train loss: 0.3959      val loss: 0.8364        val_rmse: 0.91455       val_mae: 0.50837
epoch: 32       epoch time[sec]: 1      train loss: 0.3959      val loss: 0.8364        val_rmse: 0.91455       val_mae: 0.50837
INFO - 2024-11-20 11:29:53,992 - logger - epoch: 33     epoch time[sec]: 1      lr: 0.01        train loss: 0.3402      val loss: 0.8699        val_rmse: 0.93267       val_mae: 0.49092
epoch: 33       epoch time[sec]: 1      train loss: 0.3402      val loss: 0.8699        val_rmse: 0.93267       val_mae: 0.49092
INFO - 2024-11-20 11:29:56,591 - logger - epoch: 34     epoch time[sec]: 1      lr: 0.01        train loss: 0.2769      val loss: 0.4296        val_rmse: 0.65545       val_mae: 0.38494
epoch: 34       epoch time[sec]: 1      train loss: 0.2769      val loss: 0.4296        val_rmse: 0.65545       val_mae: 0.38494
INFO - 2024-11-20 11:30:00,179 - logger - epoch: 35     epoch time[sec]: 1      lr: 0.01        train loss: 0.2707      val loss: 0.3649        val_rmse: 0.60406       val_mae: 0.33845
epoch: 35       epoch time[sec]: 1      train loss: 0.2707      val loss: 0.3649        val_rmse: 0.60406       val_mae: 0.33845
INFO - 2024-11-20 11:30:04,307 - logger - epoch: 36     epoch time[sec]: 1      lr: 0.01        train loss: 0.2979      val loss: 1.6681        val_rmse: 1.29155       val_mae: 0.71848
epoch: 36       epoch time[sec]: 1      train loss: 0.2979      val loss: 1.6681        val_rmse: 1.29155       val_mae: 0.71848
INFO - 2024-11-20 11:30:06,846 - logger - epoch: 37     epoch time[sec]: 1      lr: 0.01        train loss: 0.2614      val loss: 0.3439        val_rmse: 0.58643       val_mae: 0.33869
epoch: 37       epoch time[sec]: 1      train loss: 0.2614      val loss: 0.3439        val_rmse: 0.58643       val_mae: 0.33869
INFO - 2024-11-20 11:30:09,459 - logger - epoch: 38     epoch time[sec]: 1      lr: 0.01        train loss: 0.2332      val loss: 0.5017        val_rmse: 0.70829       val_mae: 0.39683
epoch: 38       epoch time[sec]: 1      train loss: 0.2332      val loss: 0.5017        val_rmse: 0.70829       val_mae: 0.39683
INFO - 2024-11-20 11:30:11,893 - logger - epoch: 39     epoch time[sec]: 1      lr: 0.01        train loss: 0.2718      val loss: 0.5848        val_rmse: 0.76472       val_mae: 0.40973
epoch: 39       epoch time[sec]: 1      train loss: 0.2718      val loss: 0.5848        val_rmse: 0.76472       val_mae: 0.40973
INFO - 2024-11-20 11:30:14,413 - logger - epoch: 40     epoch time[sec]: 1      lr: 0.01        train loss: 0.2550      val loss: 0.2871        val_rmse: 0.53581       val_mae: 0.31498
epoch: 40       epoch time[sec]: 1      train loss: 0.2550      val loss: 0.2871        val_rmse: 0.53581       val_mae: 0.31498
INFO - 2024-11-20 11:30:17,000 - logger - epoch: 41     epoch time[sec]: 1      lr: 0.01        train loss: 0.2608      val loss: 0.5654        val_rmse: 0.75194       val_mae: 0.40433
epoch: 41       epoch time[sec]: 1      train loss: 0.2608      val loss: 0.5654        val_rmse: 0.75194       val_mae: 0.40433
INFO - 2024-11-20 11:30:19,745 - logger - epoch: 42     epoch time[sec]: 1      lr: 0.01        train loss: 0.2088      val loss: 0.2874        val_rmse: 0.53612       val_mae: 0.28228
epoch: 42       epoch time[sec]: 1      train loss: 0.2088      val loss: 0.2874        val_rmse: 0.53612       val_mae: 0.28228
INFO - 2024-11-20 11:30:23,103 - logger - epoch: 43     epoch time[sec]: 1      lr: 0.01        train loss: 0.2524      val loss: 0.3326        val_rmse: 0.57668       val_mae: 0.31789
epoch: 43       epoch time[sec]: 1      train loss: 0.2524      val loss: 0.3326        val_rmse: 0.57668       val_mae: 0.31789
INFO - 2024-11-20 11:30:26,108 - logger - epoch: 44     epoch time[sec]: 1      lr: 0.01        train loss: 0.2489      val loss: 0.3601        val_rmse: 0.60012       val_mae: 0.33366
epoch: 44       epoch time[sec]: 1      train loss: 0.2489      val loss: 0.3601        val_rmse: 0.60012       val_mae: 0.33366
INFO - 2024-11-20 11:30:28,611 - logger - epoch: 45     epoch time[sec]: 1      lr: 0.01        train loss: 0.2576      val loss: 0.7245        val_rmse: 0.85120       val_mae: 0.46228
epoch: 45       epoch time[sec]: 1      train loss: 0.2576      val loss: 0.7245        val_rmse: 0.85120       val_mae: 0.46228
INFO - 2024-11-20 11:30:31,099 - logger - epoch: 46     epoch time[sec]: 1      lr: 0.01        train loss: 0.2921      val loss: 0.7315        val_rmse: 0.85530       val_mae: 0.45723
epoch: 46       epoch time[sec]: 1      train loss: 0.2921      val loss: 0.7315        val_rmse: 0.85530       val_mae: 0.45723
INFO - 2024-11-20 11:30:33,641 - logger - epoch: 47     epoch time[sec]: 1      lr: 0.01        train loss: 0.2708      val loss: 0.6067        val_rmse: 0.77891       val_mae: 0.45281
epoch: 47       epoch time[sec]: 1      train loss: 0.2708      val loss: 0.6067        val_rmse: 0.77891       val_mae: 0.45281
INFO - 2024-11-20 11:30:36,147 - logger - epoch: 48     epoch time[sec]: 1      lr: 0.01        train loss: 0.3275      val loss: 0.6822        val_rmse: 0.82595       val_mae: 0.46309
epoch: 48       epoch time[sec]: 1      train loss: 0.3275      val loss: 0.6822        val_rmse: 0.82595       val_mae: 0.46309
INFO - 2024-11-20 11:30:38,607 - logger - epoch: 49     epoch time[sec]: 1      lr: 0.01        train loss: 0.2724      val loss: 0.6006        val_rmse: 0.77498       val_mae: 0.41831
epoch: 49       epoch time[sec]: 1      train loss: 0.2724      val loss: 0.6006        val_rmse: 0.77498       val_mae: 0.41831
INFO - 2024-11-20 11:30:40,483 - hpe_train - Done
INFO - 2024-11-20 11:30:40,487 - config - Experiment Configuration
{&#39;aug&#39;: &#39;none&#39;,
 &#39;batch_size&#39;: 32,
 &#39;csv_path&#39;: &#39;./data/hpe_dataset/csv&#39;,
 &#39;dataset_name&#39;: &#39;pose_regression_timeseries_subject_1&#39;,
 &#39;finetune&#39;: 0,
 &#39;gan&#39;: &#39;none&#39;,
 &#39;height&#39;: 224,
 &#39;input_feature&#39;: &#39;logmel&#39;,
 &#39;learning_rate&#39;: 0.01,
 &#39;max_epoch&#39;: 50,
 &#39;model&#39;: &#39;speech2pose&#39;,
 &#39;num_workers&#39;: 8,
 &#39;pretrained&#39;: True,
 &#39;ratio&#39;: 0.0,
 &#39;smooth_loss&#39;: False,
 &#39;sound_length&#39;: 2400,
 &#39;topk&#39;: (1,
          3),
 &#39;use_class_weight&#39;: True,
 &#39;width&#39;: 224}
INFO - 2024-11-20 11:30:40,487 - config - successfully loaded configuration.
INFO - 2024-11-20 11:30:40,487 - dataset - Dataset: pose_regression_timeseries_subject_1        Split: test     Batch size: 1.
INFO - 2024-11-20 11:30:40,487 - mean_std - mean value: [-23.539913, -25.119139, -26.711174, -22.011652, 0.0044820304, 0.00080580305, -0.040803947]
INFO - 2024-11-20 11:30:40,488 - mean_std - std value: [12.967929, 10.571223, 10.273731, 11.8457985, 0.022749536, 0.019968906, 0.037085325]
INFO - 2024-11-20 11:30:40,696 - dataset - the number of samples: 2424
INFO - 2024-11-20 11:30:40,696 - __init__ - speech2pose will be used as a model.
/home/kemove/anaconda3/envs/open-mmlab/lib/python3.8/site-packages/pysensing/acoustic/inference/training/AcousticPose_utils/hpe_test.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don&#39;t have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state_dict = torch.load(os.path.join(result_path, &quot;best_model.prm&quot;))
---------- Start evaluation for test data ----------
loss: 0.28709   RMSE: 0.54      MAE: 0.31       Acc: 0.79, 0.94, 0.98, 0.99
arm RMSE: 0.71  arm MAE: 0.45   Acc: 0.70, 0.92, 0.97, 0.98
leg RMSE: 0.38  leg MAE: 0.23   Acc: 0.90, 0.97, 0.99, 1.00
body RMSE: 0.41 body MAE: 0.22  Acc: 0.86, 0.95, 0.98, 0.99
Done.
</pre></div>
</div>
</section>
<section id="load-the-wipose-lstm-model">
<h2>Load the Wipose_LSTM model<a class="headerlink" href="#load-the-wipose-lstm-model" title="Permalink to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Method 1</span>
<span class="n">hpe_model</span> <span class="o">=</span> <span class="n">Wipose_LSTM</span><span class="p">(</span><span class="n">in_cha</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span><span class="n">out_cha</span><span class="o">=</span><span class="mi">63</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># model_path = &#39;path to trained model&#39;</span>
<span class="c1"># state_dict = torch.load(model_path,weights_only=True)</span>

<span class="c1"># Method 2</span>
<span class="n">hpe_model</span> <span class="o">=</span> <span class="n">load_hpe_model</span><span class="p">(</span><span class="s1">&#39;wipose&#39;</span><span class="p">,</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">task</span><span class="o">=</span><span class="s1">&#39;subject8&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>/home/kemove/anaconda3/envs/open-mmlab/lib/python3.8/site-packages/torch/nn/modules/rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn(&quot;dropout option adds dropout after all but last &quot;
</pre></div>
</div>
</section>
<section id="id1">
<h2>Load the data<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">csv</span> <span class="o">=</span> <span class="s1">&#39;./data/hpe_dataset/csv/pose_regression_timeseries_subject_8/test.csv&#39;</span> <span class="c1"># The path contains the samosa dataset</span>
<span class="n">hpe_testdataset</span> <span class="o">=</span> <span class="n">SoundPoseLSTMDataset</span><span class="p">(</span><span class="n">csv</span><span class="p">,</span><span class="n">sound_length</span><span class="o">=</span><span class="mi">2400</span><span class="p">,</span><span class="n">input_feature</span><span class="o">=</span><span class="s1">&#39;raw&#39;</span><span class="p">,</span><span class="n">mean</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">get_raw_mean</span><span class="p">())</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">),</span><span class="n">std</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">get_raw_std</span><span class="p">())</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">))</span>
<span class="n">index</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># Randomly select an index</span>
<span class="n">sample</span><span class="o">=</span> <span class="n">hpe_testdataset</span><span class="o">.</span><span class="fm">__getitem__</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>INFO - 2024-11-20 11:30:56,592 - mean_std - mean value: [-1.9971873e-05, -1.60138708e-08, -5.86972669e-08, -1.24919584e-07]
INFO - 2024-11-20 11:30:56,592 - mean_std - std value: [0.00657787, 0.00519175, 0.00472543, 0.00824625]
INFO - 2024-11-20 11:30:56,989 - hpe - the number of samples: 2940
</pre></div>
</div>
</section>
<section id="model-inference">
<h2>Model inference<a class="headerlink" href="#model-inference" title="Permalink to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Method 1</span>
<span class="n">hpe_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">predicted_result</span> <span class="o">=</span> <span class="n">hpe_model</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;sound&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="n">vis_images</span> <span class="o">=</span> <span class="n">make_images</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;targets&#39;</span><span class="p">],</span><span class="n">predicted_result</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="c1">#Method 2</span>
<span class="kn">from</span> <span class="nn">pysensing.acoustic.inference.predict</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">predicted_result</span>  <span class="o">=</span> <span class="n">hpe_predict</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;sound&#39;</span><span class="p">],</span><span class="s1">&#39;SoundPoseLSTMDataset&#39;</span><span class="p">,</span><span class="n">hpe_model</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">vis_images</span> <span class="o">=</span> <span class="n">make_images</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;targets&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span><span class="n">predicted_result</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="n">seq_num</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">vis_images</span><span class="p">[</span><span class="n">seq_num</span><span class="p">][</span><span class="s1">&#39;img&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_acoustic_hpe_tutorial_002.png" srcset="../_images/sphx_glr_acoustic_hpe_tutorial_002.png" alt="acoustic hpe tutorial" class = "sphx-glr-single-img"/></section>
<section id="model-embedding">
<h2>Model embedding<a class="headerlink" href="#model-embedding" title="Permalink to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pysensing.acoustic.inference.embedding</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">sample_embedding</span> <span class="o">=</span> <span class="n">hpe_embedding</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;sound&#39;</span><span class="p">],</span><span class="s1">&#39;SoundPoseLSTMDataset&#39;</span><span class="p">,</span><span class="n">hpe_model</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>And that’s it. We’re done with our acoustic humna pose estimation tutorials. Thanks for reading.</p>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (2 minutes 46.766 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-acoustic-acoustic-hpe-tutorial-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/7a6f102b877e68e9956fa06c9728eff5/acoustic_hpe_tutorial.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">acoustic_hpe_tutorial.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/3f90f032a2f8d490ea6cc694e6ba4690/acoustic_hpe_tutorial.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">acoustic_hpe_tutorial.py</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../csi/add_noise_tutorial.html" class="btn btn-neutral float-right" title="Acoustic Preprocessing.Filter Tutorial" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="acoustic_ped_det_tutorial.html" class="btn btn-neutral" title="Acoustic Pedestrian Detection Tutorial" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr class="rating-hr hr-top">
      <div class="rating-container">
        <div class="rating-prompt">Rate this Tutorial</div>
        <div class="stars-outer">
          <i class="far fa-star" title="1 Star" data-behavior="tutorial-rating" data-count="1"></i>
          <i class="far fa-star" title="2 Stars" data-behavior="tutorial-rating" data-count="2"></i>
          <i class="far fa-star" title="3 Stars" data-behavior="tutorial-rating" data-count="3"></i>
          <i class="far fa-star" title="4 Stars" data-behavior="tutorial-rating" data-count="4"></i>
          <i class="far fa-star" title="5 Stars" data-behavior="tutorial-rating" data-count="5"></i>
        </div>
      </div>
    <hr class="rating-hr hr-bottom"/>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2024, Pysensing contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>
<!--        <div class="pytorch-content-right" id="pytorch-content-right">-->
<!--          <div class="pytorch-right-menu" id="pytorch-right-menu">-->
<!--              <a class="twitter-timeline" data-width="100%" href="https://twitter.com/pypose_org?ref_src=twsrc%5Etfw">News from Twitter @pypose_rog<br>You need VPN if you see this.</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>-->
<!--          </div>-->
<!--        </div>-->
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../_static/doctools.js"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access documentation for Pysensing</p>
          <a class="with-right-arrow" href="https://yizhuoyang.github.io/html_test/docs/main/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get started with tutorials and examples</p>
          <a class="with-right-arrow" href="https://github.com/pysensing/pysensing">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Get Started</h2>
          <p>Find resources and how to start using pysensing</p>
          <a class="with-right-arrow" href="https://github.com/pysensing/pysensing">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://github.com/pysensing/pysensing" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://github.com/pysensing/pysensing">Pysensing</a></li>
            <li><a href="https://github.com/pysensing/pysensing">Get Started</a></li>
            <li><a href="https://github.com/pysensing/pysensing">Features</a></li>
            <li><a href="https://github.com/pysensing/pysensing">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://github.com/pysensing/pysensing">Resources</a></li>
            <li><a href="https://github.com/pysensing/pysensing">Tutorials</a></li>
            <li><a href="https://yizhuoyang.github.io/html_test/docs/main/index.html">Docs</a></li>
            <li><a href="https://github.com/pysensing/pysensing" target="_blank">Github Issues</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://github.com/pysensing/pysensing" target="_blank">Twitter</a></li>
            <li><a href="https://github.com/pysensing/pysensing" target="_blank">GitHub</a></li>

          </ul>  
          </div>
        </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://github.com/pysensing/pysensing" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/pysensing/pysensing">Get Started</a>
          </li>
          <li>
            <a href="https://github.com/pysensing/pysensing">Tutorials</a>
          </li>
          <li>
            <a href="https://yizhuoyang.github.io/html_test/docs/main/index.html">Docs</a>
          </li>
          <li>
            <a href="https://github.com/pysensing/pysensing">About Us</a>
          </li>
          <li>
            <a href="https://github.com/pysensing/pysensing">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>