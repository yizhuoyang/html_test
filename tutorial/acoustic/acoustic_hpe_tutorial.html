


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Acoustic Human Pose estimation Tutorial &mdash; Pysensing Tutorials  documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-rendered-html.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Bluetooth Localization Tutorial" href="../bluetooth/bluetooth_localization_tutorial.html" />
    <link rel="prev" title="Acoustic Pedestrian Detection Tutorial" href="acoustic_ped_det_tutorial.html" />
  <!-- Google Analytics -->
  
  <!-- End Google Analytics -->
  

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://www.pysensing.org" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://www.pysensing.org/get-started/locally">Get Started</a>
          </li>

          <!-- <li class="active"> -->
          <li>
            <a href="https://www.pysensing.org/tutorial/">Tutorials</a>
          </li>

          <!-- <li> -->
          <li>
            <a href="https://www.pysensing.org/docs">Doc</a>
          </li>

          <li>
            <a href="https://www.pysensing.org/about-us">About Us</a>
          </li>

          <li>
            <a href="https://github.com/pysensing/pysensing">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Tutorials" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Acoustic</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="acoustic_filter_tutorial.html">Acoustic Preprocessing.Filter Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="acoustic_transform_tutorial.html">Acoustic Preprocssing.Transfrom Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="acoustic_sig_aug_tutorial.html">Acoustic Augmentation.Signal_aug Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="acoustic_spectrogram_aug_tutorial.html">Acoustic augmentation.spectrogram_aug Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="acoustic_har_tutorial.html">Acoustic Human Activity Recognition Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="acoustic_hgr_tutorial.html">Acoustic Hand Gesture Recognition Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="acoustic_ped_det_tutorial.html">Acoustic Pedestrian Detection Tutorial</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Acoustic Human Pose estimation Tutorial</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Bluetooth</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../bluetooth/bluetooth_localization_tutorial.html">Bluetooth Localization Tutorial</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">CSI</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../csi/add_noise_tutorial.html">CSI Augmentation.adding_noise Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../csi/deformation_tutorial.html">CSI Preprocessing.transform Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../csi/denoising_tutorial.html">CSI Preprocessing.denoising Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../csi/HAR_HID_tutorial.html">CSI Augmentation.deformation Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../csi/HFD_tutorial.html">CSI classification tasks Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../csi/HPE_tutorial.html">CSI human pose estimation Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../csi/Reconstruction_tutorial.html">CSI human reconstruction Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../csi/transform_tutorial.html">CSI human fall detection Tutorial</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">intraoral_scan</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../intraoral_scan/tooth_alignment.html">Tutorial for tooth alignment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intraoral_scan/tooth_segmentation.html">Tutorial for tooth segmentation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">mmwave_raw</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../mmwave_raw/mmwave_raw_tutorial.html">Data loading and prepocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mmwave_raw/mmwave_raw_tutorial.html#model-loading-and-inference">model loading and inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">mmwave_PC</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../mmwave_PC/mmwave_PC_har_tutorial.html">Tutorial for Human Activity Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mmwave_PC/mmwave_PC_hgr_tutorial.html">Tutorial for Human Gesture Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mmwave_PC/mmwave_PC_hpe_tutorial.html">Tutorial for Human Pose Estimation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">rssi</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../rssi/rssi_localization_fingerprinting_tutorial.html">RSSI Localization Fingerprinting Tutorial</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">uwb</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../uwb/uwb_preprocessing_tutorial.html">Tutorial for UWB Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../uwb/uwb_augmentation_tutorial.html">Tutorial for UWB Data Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../uwb/uwb_har_tutorial.html">Tutorial for UWB Human Activity Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../uwb/uwb_localization_tutorial.html">Tutorial for UWB Localization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../uwb/uwb_occupancy_detection_tutorial.html">Tutorial for UWB Occupancy Detection</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Tutorials
          
        </a> &gt;
      </li>

        
      <li>Acoustic Human Pose estimation Tutorial</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/acoustic/acoustic_hpe_tutorial.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- <div class="pytorch-call-to-action-links">
            <div id="tutorial-type">acoustic/acoustic_hpe_tutorial</div>

            <div id="google-colab-link">
              <img class="call-to-action-img" src="../_static/images/pytorch-colab.svg"/>
              <div class="call-to-action-desktop-view">Run in Google Colab</div>
              <div class="call-to-action-mobile-view">Colab</div>
            </div>
            <div id="download-notebook-link">
              <img class="call-to-action-notebook-img" src="../_static/images/pytorch-download.svg"/>
              <div class="call-to-action-desktop-view">Download Notebook</div>
              <div class="call-to-action-mobile-view">Notebook</div>
            </div>
            <div id="github-view-link">
              <img class="call-to-action-img" src="../_static/images/pytorch-github.svg"/>
              <div class="call-to-action-desktop-view">View on GitHub</div>
              <div class="call-to-action-mobile-view">GitHub</div>
            </div>
          </div> -->

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-acoustic-acoustic-hpe-tutorial-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="acoustic-human-pose-estimation-tutorial">
<span id="sphx-glr-acoustic-acoustic-hpe-tutorial-py"></span><h1>Acoustic Human Pose estimation Tutorial<a class="headerlink" href="#acoustic-human-pose-estimation-tutorial" title="Permalink to this heading">¶</a></h1>
<p>!pip install pysensing</p>
<p>In this tutorial, we will be implementing codes for acoustic Human pose estimation</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pysensing.acoustic.datasets.utils.hpe_vis</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pysensing.acoustic.models.hpe</span> <span class="kn">import</span> <span class="n">Speech2pose</span><span class="p">,</span><span class="n">Wipose_LSTM</span>
<span class="kn">from</span> <span class="nn">pysensing.acoustic.models.get_model</span> <span class="kn">import</span> <span class="n">load_hpe_model</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="simple">
<dt>Listening Human Behavior: 3D Human Pose Estimation with Acoustic Signals</dt><dd></dd>
</dl>
<p>Implementation of “Listening Human Behavior: 3D Human Pose Estimation with Acoustic Signals”.</p>
<p>This dataset contains the audio reflected by human to estimate the 3D human pose with the acoustic signals.</p>
<p>Reference: <a class="reference external" href="https://github.com/YutoShibata07/AcousticPose_Public">https://github.com/YutoShibata07/AcousticPose_Public</a></p>
<section id="load-the-data">
<h2>Load the data<a class="headerlink" href="#load-the-data" title="Permalink to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Method 1: Use get_dataloader</span>
<span class="kn">from</span> <span class="nn">pysensing.acoustic.datasets.get_dataloader</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">train_loader</span><span class="p">,</span><span class="n">val_loader</span><span class="p">,</span><span class="n">test_loader</span> <span class="o">=</span> <span class="n">load_hpe_dataset</span><span class="p">(</span>
    <span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span>
    <span class="n">dataset_name</span><span class="o">=</span><span class="s1">&#39;pose_regression_timeseries_subject_1&#39;</span><span class="p">,</span>
    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Method 2</span>
<span class="n">csv</span> <span class="o">=</span> <span class="s1">&#39;./data/hpe_dataset/csv/pose_regression_timeseries_subject_1/test.csv&#39;</span> <span class="c1"># The path contains the samosa dataset</span>
<span class="n">data_dir</span> <span class="o">=</span> <span class="s1">&#39;./data&#39;</span>
<span class="n">hpe_testdataset</span> <span class="o">=</span> <span class="n">SoundPose2DDataset</span><span class="p">(</span><span class="n">csv</span><span class="p">,</span><span class="n">sound_length</span><span class="o">=</span><span class="mi">2400</span><span class="p">,</span><span class="n">input_feature</span><span class="o">=</span><span class="s1">&#39;logmel&#39;</span><span class="p">,</span>
                                     <span class="n">mean</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">get_mean</span><span class="p">())</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)[:</span><span class="mi">4</span><span class="p">],</span>
                                     <span class="n">std</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">get_std</span><span class="p">())</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)[:</span><span class="mi">4</span><span class="p">],</span>
                                     <span class="p">)</span>
<span class="n">index</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># Randomly select an index</span>
<span class="n">sample</span><span class="o">=</span> <span class="n">hpe_testdataset</span><span class="o">.</span><span class="fm">__getitem__</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;targets&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;sound&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Size([12, 63])
torch.Size([4, 12, 128])
</pre></div>
</div>
</section>
<section id="load-speech2pose-model">
<h2>Load Speech2pose model<a class="headerlink" href="#load-speech2pose-model" title="Permalink to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Method 1</span>
<span class="n">hpe_model</span> <span class="o">=</span> <span class="n">Speech2pose</span><span class="p">(</span><span class="n">out_cha</span><span class="o">=</span><span class="mi">63</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># model_path = &#39;path to pretrian weights&#39;</span>
<span class="c1"># state_dict = torch.load(model_path,weights_only=True)</span>
<span class="c1"># hpe_model.load_state_dict(state_dict)</span>

<span class="c1"># Method 2</span>
<span class="n">hpe_model</span> <span class="o">=</span> <span class="n">load_hpe_model</span><span class="p">(</span><span class="s1">&#39;speech2pose&#39;</span><span class="p">,</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">task</span><span class="o">=</span><span class="s1">&#39;subject8&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="modle-inference">
<h2>Modle Inference<a class="headerlink" href="#modle-inference" title="Permalink to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1">#Method 1</span>
<span class="n">sample</span><span class="o">=</span> <span class="n">hpe_testdataset</span><span class="o">.</span><span class="fm">__getitem__</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
<span class="n">hpe_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">predicted_result</span> <span class="o">=</span> <span class="n">hpe_model</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;sound&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="n">vis_images</span> <span class="o">=</span> <span class="n">make_images</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;targets&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span><span class="n">predicted_result</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="c1">#Method 2</span>
<span class="kn">from</span> <span class="nn">pysensing.acoustic.inference.predict</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">predicted_result</span>  <span class="o">=</span> <span class="n">hpe_predict</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;sound&#39;</span><span class="p">],</span><span class="s1">&#39;SoundPose2DDataset&#39;</span><span class="p">,</span><span class="n">hpe_model</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">vis_images</span> <span class="o">=</span> <span class="n">make_images</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;targets&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span><span class="n">predicted_result</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="n">seq_num</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">vis_images</span><span class="p">[</span><span class="n">seq_num</span><span class="p">][</span><span class="s1">&#39;img&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_acoustic_hpe_tutorial_001.png" srcset="../_images/sphx_glr_acoustic_hpe_tutorial_001.png" alt="acoustic hpe tutorial" class = "sphx-glr-single-img"/><div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>/home/kemove/anaconda3/envs/il/lib/python3.9/site-packages/torch/nn/modules/conv.py:306: UserWarning: Using padding=&#39;same&#39; with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1008.)
  return F.conv1d(input, weight, bias, self.stride,
</pre></div>
</div>
</section>
<section id="modle-embedding">
<h2>Modle Embedding<a class="headerlink" href="#modle-embedding" title="Permalink to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pysensing.acoustic.inference.embedding</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">sample_embedding</span> <span class="o">=</span> <span class="n">hpe_embedding</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;sound&#39;</span><span class="p">],</span><span class="s1">&#39;SoundPose2DDataset&#39;</span><span class="p">,</span><span class="n">hpe_model</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="modle-training">
<h2>Modle Training<a class="headerlink" href="#modle-training" title="Permalink to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pysensing.acoustic.inference.training.AcousticPose_utils.hpe_train</span> <span class="kn">import</span> <span class="n">train_model</span><span class="p">,</span><span class="n">generate_configs</span>

<span class="n">args</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;root_dir&quot;</span><span class="p">:</span> <span class="s2">&quot;./data/hpe_dataset/testing_result&quot;</span><span class="p">,</span>
    <span class="s2">&quot;save_name&quot;</span><span class="p">:</span> <span class="s2">&quot;seq1&quot;</span><span class="p">,</span>
    <span class="s2">&quot;input_feature&quot;</span><span class="p">:</span> <span class="s2">&quot;logmel&quot;</span><span class="p">,</span>
    <span class="s2">&quot;batchsize&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
    <span class="s2">&quot;max_epoch&quot;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
    <span class="s2">&quot;csv_path&quot;</span><span class="p">:</span> <span class="s2">&quot;./data/hpe_dataset/csv&quot;</span><span class="p">,</span>
    <span class="s2">&quot;dataset_name&quot;</span><span class="p">:</span> <span class="s2">&quot;pose_regression_timeseries_subject_1&quot;</span><span class="p">,</span>
    <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;speech2pose&quot;</span><span class="p">,</span>
    <span class="s2">&quot;sound_length&quot;</span><span class="p">:</span> <span class="mi">2400</span><span class="p">,</span>
    <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">config_path</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;root_dir&quot;</span><span class="p">]</span><span class="o">+</span><span class="s1">&#39;/&#39;</span><span class="o">+</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;save_name&quot;</span><span class="p">]</span><span class="o">+</span><span class="s2">&quot;/config.yaml&quot;</span>
<span class="n">generate_configs</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
<span class="n">resume_training</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">random_seed</span> <span class="o">=</span> <span class="mi">0</span>

<span class="n">train_model</span><span class="p">(</span>
    <span class="n">config_path</span><span class="o">=</span><span class="n">config_path</span><span class="p">,</span>
    <span class="n">resume</span><span class="o">=</span><span class="n">resume_training</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="n">random_seed</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Modle Training</span>
<span class="c1"># ------------------------</span>
<span class="kn">from</span> <span class="nn">pysensing.acoustic.inference.training.AcousticPose_utils.hpe_test</span> <span class="kn">import</span> <span class="n">evaluate_model</span>
<span class="n">args</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;root_dir&quot;</span><span class="p">:</span> <span class="s2">&quot;./data/hpe_dataset/testing_result&quot;</span><span class="p">,</span>
    <span class="s2">&quot;save_name&quot;</span><span class="p">:</span> <span class="s2">&quot;seq1&quot;</span><span class="p">,</span>
    <span class="s2">&quot;batchsize&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
    <span class="s2">&quot;max_epoch&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
    <span class="s2">&quot;csv_path&quot;</span><span class="p">:</span> <span class="s2">&quot;./data/hpe_dataset/csv&quot;</span><span class="p">,</span>
    <span class="s2">&quot;dataset_name&quot;</span><span class="p">:</span> <span class="s2">&quot;pose_regression_timeseries_subject_1&quot;</span><span class="p">,</span>
    <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;speech2pose&quot;</span><span class="p">,</span>
    <span class="s2">&quot;sound_length&quot;</span><span class="p">:</span> <span class="mi">2400</span><span class="p">,</span>
    <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">config_path</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;root_dir&quot;</span><span class="p">]</span><span class="o">+</span><span class="s1">&#39;/&#39;</span><span class="o">+</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;save_name&quot;</span><span class="p">]</span><span class="o">+</span><span class="s2">&quot;/config.yaml&quot;</span>
<span class="n">evaluation_mode</span> <span class="o">=</span> <span class="s2">&quot;test&quot;</span>
<span class="n">model_path</span> <span class="o">=</span> <span class="kc">None</span>

<span class="n">evaluate_model</span><span class="p">(</span>
    <span class="n">config_path</span><span class="o">=</span><span class="n">config_path</span><span class="p">,</span>
    <span class="n">mode</span><span class="o">=</span><span class="n">evaluation_mode</span><span class="p">,</span>
    <span class="n">model_path</span><span class="o">=</span><span class="n">model_path</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>{&#39;csv_path&#39;: &#39;./data/hpe_dataset/csv&#39;, &#39;model&#39;: &#39;speech2pose&#39;, &#39;pretrained&#39;: True, &#39;use_class_weight&#39;: True, &#39;batch_size&#39;: 32, &#39;width&#39;: 224, &#39;height&#39;: 224, &#39;num_workers&#39;: 8, &#39;max_epoch&#39;: 50, &#39;learning_rate&#39;: 0.01, &#39;sound_length&#39;: 2400, &#39;dataset_name&#39;: &#39;pose_regression_timeseries_subject_1&#39;, &#39;input_feature&#39;: &#39;logmel&#39;, &#39;topk&#39;: (1, 3), &#39;smooth_loss&#39;: False, &#39;ratio&#39;: 0.0, &#39;gan&#39;: &#39;none&#39;, &#39;finetune&#39;: 0, &#39;aug&#39;: &#39;none&#39;}
Finished making configuration files.
/home/kemove/anaconda3/envs/il/lib/python3.9/site-packages/pysensing/acoustic/inference/training/AcousticPose_utils/libs/dataset.py:195: FutureWarning: DataFrame.fillna with &#39;method&#39; is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.
  self.df[self.df[&quot;sound_length&quot;] == sound_length]
/home/kemove/anaconda3/envs/il/lib/python3.9/site-packages/pysensing/acoustic/inference/training/AcousticPose_utils/libs/dataset.py:195: FutureWarning: DataFrame.fillna with &#39;method&#39; is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.
  self.df[self.df[&quot;sound_length&quot;] == sound_length]
epoch: 0        epoch time[sec]: 4      train loss: 38.0937     val loss: 163.1282      val_rmse: 12.77217      val_mae: 8.43311
epoch: 1        epoch time[sec]: 1      train loss: 3.3102      val loss: 50.4458       val_rmse: 7.10252       val_mae: 4.40765
epoch: 2        epoch time[sec]: 1      train loss: 1.5292      val loss: 1.8193        val_rmse: 1.34880       val_mae: 0.82698
epoch: 3        epoch time[sec]: 1      train loss: 1.1860      val loss: 1.4522        val_rmse: 1.20507       val_mae: 0.72901
epoch: 4        epoch time[sec]: 1      train loss: 1.0476      val loss: 1.2415        val_rmse: 1.11424       val_mae: 0.64408
epoch: 5        epoch time[sec]: 1      train loss: 0.8655      val loss: 1.5326        val_rmse: 1.23800       val_mae: 0.72606
epoch: 6        epoch time[sec]: 1      train loss: 0.8991      val loss: 1.7280        val_rmse: 1.31454       val_mae: 0.75788
epoch: 7        epoch time[sec]: 1      train loss: 0.7731      val loss: 1.4055        val_rmse: 1.18552       val_mae: 0.67471
epoch: 8        epoch time[sec]: 1      train loss: 1.0249      val loss: 33.7593       val_rmse: 5.81028       val_mae: 3.50439
epoch: 9        epoch time[sec]: 1      train loss: 0.8726      val loss: 3.8771        val_rmse: 1.96904       val_mae: 1.12149
epoch: 10       epoch time[sec]: 1      train loss: 0.7102      val loss: 1.1204        val_rmse: 1.05849       val_mae: 0.56559
epoch: 11       epoch time[sec]: 1      train loss: 0.5611      val loss: 1.5317        val_rmse: 1.23764       val_mae: 0.69886
epoch: 12       epoch time[sec]: 1      train loss: 0.5923      val loss: 3.0224        val_rmse: 1.73850       val_mae: 1.06073
epoch: 13       epoch time[sec]: 1      train loss: 0.5095      val loss: 3.3477        val_rmse: 1.82968       val_mae: 1.13779
epoch: 14       epoch time[sec]: 1      train loss: 0.6626      val loss: 2.4439        val_rmse: 1.56328       val_mae: 0.91174
epoch: 15       epoch time[sec]: 1      train loss: 0.5039      val loss: 1.3499        val_rmse: 1.16187       val_mae: 0.62304
epoch: 16       epoch time[sec]: 1      train loss: 0.4427      val loss: 0.8582        val_rmse: 0.92637       val_mae: 0.49949
epoch: 17       epoch time[sec]: 1      train loss: 0.4141      val loss: 0.7087        val_rmse: 0.84182       val_mae: 0.44726
epoch: 18       epoch time[sec]: 1      train loss: 0.4481      val loss: 1.9027        val_rmse: 1.37937       val_mae: 0.81386
epoch: 19       epoch time[sec]: 1      train loss: 0.4947      val loss: 0.7230        val_rmse: 0.85029       val_mae: 0.46919
epoch: 20       epoch time[sec]: 1      train loss: 0.4163      val loss: 0.7290        val_rmse: 0.85384       val_mae: 0.46518
epoch: 21       epoch time[sec]: 1      train loss: 0.3556      val loss: 0.4851        val_rmse: 0.69652       val_mae: 0.38136
epoch: 22       epoch time[sec]: 1      train loss: 0.3800      val loss: 0.5969        val_rmse: 0.77261       val_mae: 0.41563
epoch: 23       epoch time[sec]: 1      train loss: 0.3724      val loss: 0.6957        val_rmse: 0.83409       val_mae: 0.47226
epoch: 24       epoch time[sec]: 1      train loss: 0.3783      val loss: 0.7569        val_rmse: 0.87001       val_mae: 0.49752
epoch: 25       epoch time[sec]: 1      train loss: 0.4418      val loss: 1.0745        val_rmse: 1.03658       val_mae: 0.56204
epoch: 26       epoch time[sec]: 1      train loss: 0.3735      val loss: 1.5351        val_rmse: 1.23899       val_mae: 0.71568
epoch: 27       epoch time[sec]: 1      train loss: 0.4474      val loss: 1.1726        val_rmse: 1.08285       val_mae: 0.59331
epoch: 28       epoch time[sec]: 1      train loss: 0.4119      val loss: 2.5204        val_rmse: 1.58757       val_mae: 0.93299
epoch: 29       epoch time[sec]: 1      train loss: 0.5752      val loss: 0.9867        val_rmse: 0.99332       val_mae: 0.55090
epoch: 30       epoch time[sec]: 1      train loss: 0.3881      val loss: 0.9181        val_rmse: 0.95816       val_mae: 0.57685
epoch: 31       epoch time[sec]: 1      train loss: 0.3227      val loss: 1.1632        val_rmse: 1.07851       val_mae: 0.59697
epoch: 32       epoch time[sec]: 1      train loss: 0.4059      val loss: 0.7456        val_rmse: 0.86350       val_mae: 0.51384
epoch: 33       epoch time[sec]: 1      train loss: 0.3700      val loss: 0.4349        val_rmse: 0.65946       val_mae: 0.36350
epoch: 34       epoch time[sec]: 1      train loss: 0.2653      val loss: 0.5030        val_rmse: 0.70920       val_mae: 0.42115
epoch: 35       epoch time[sec]: 1      train loss: 0.2663      val loss: 0.5556        val_rmse: 0.74541       val_mae: 0.39126
epoch: 36       epoch time[sec]: 1      train loss: 0.2956      val loss: 0.4150        val_rmse: 0.64422       val_mae: 0.37835
epoch: 37       epoch time[sec]: 1      train loss: 0.2606      val loss: 0.3849        val_rmse: 0.62038       val_mae: 0.36641
epoch: 38       epoch time[sec]: 1      train loss: 0.2569      val loss: 0.4467        val_rmse: 0.66835       val_mae: 0.35664
epoch: 39       epoch time[sec]: 1      train loss: 0.2712      val loss: 0.6237        val_rmse: 0.78972       val_mae: 0.40919
epoch: 40       epoch time[sec]: 1      train loss: 0.2585      val loss: 0.4085        val_rmse: 0.63916       val_mae: 0.37400
epoch: 41       epoch time[sec]: 1      train loss: 0.2806      val loss: 0.5592        val_rmse: 0.74777       val_mae: 0.42447
epoch: 42       epoch time[sec]: 1      train loss: 0.2215      val loss: 0.3250        val_rmse: 0.57006       val_mae: 0.30865
epoch: 43       epoch time[sec]: 1      train loss: 0.2627      val loss: 0.2415        val_rmse: 0.49140       val_mae: 0.26592
epoch: 44       epoch time[sec]: 1      train loss: 0.2536      val loss: 0.3004        val_rmse: 0.54807       val_mae: 0.30688
epoch: 45       epoch time[sec]: 1      train loss: 0.2531      val loss: 0.2808        val_rmse: 0.52989       val_mae: 0.30597
epoch: 46       epoch time[sec]: 1      train loss: 0.2785      val loss: 0.8624        val_rmse: 0.92864       val_mae: 0.49453
epoch: 47       epoch time[sec]: 1      train loss: 0.2600      val loss: 0.6742        val_rmse: 0.82110       val_mae: 0.47749
epoch: 48       epoch time[sec]: 1      train loss: 0.3157      val loss: 0.3889        val_rmse: 0.62361       val_mae: 0.34720
epoch: 49       epoch time[sec]: 1      train loss: 0.2532      val loss: 0.4986        val_rmse: 0.70615       val_mae: 0.38780
/home/kemove/anaconda3/envs/il/lib/python3.9/site-packages/pysensing/acoustic/inference/training/AcousticPose_utils/libs/dataset.py:195: FutureWarning: DataFrame.fillna with &#39;method&#39; is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.
  self.df[self.df[&quot;sound_length&quot;] == sound_length]
---------- Start evaluation for test data ----------
loss: 0.24147   RMSE: 0.49      MAE: 0.27       Acc: 0.86, 0.96, 0.98, 0.99
arm RMSE: 0.64  arm MAE: 0.38   Acc: 0.79, 0.95, 0.98, 0.99
leg RMSE: 0.40  leg MAE: 0.22   Acc: 0.93, 0.98, 0.99, 0.99
body RMSE: 0.33 body MAE: 0.16  Acc: 0.94, 0.99, 1.00, 1.00
Done.
</pre></div>
</div>
</section>
<section id="load-the-wipose-lstm-model">
<h2>Load the Wipose_LSTM model<a class="headerlink" href="#load-the-wipose-lstm-model" title="Permalink to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Method 1</span>
<span class="n">hpe_model</span> <span class="o">=</span> <span class="n">Wipose_LSTM</span><span class="p">(</span><span class="n">in_cha</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span><span class="n">out_cha</span><span class="o">=</span><span class="mi">63</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># model_path = &#39;path to trained model&#39;</span>
<span class="c1"># state_dict = torch.load(model_path,weights_only=True)</span>

<span class="c1"># Method 2</span>
<span class="n">hpe_model</span> <span class="o">=</span> <span class="n">load_hpe_model</span><span class="p">(</span><span class="s1">&#39;wipose&#39;</span><span class="p">,</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">task</span><span class="o">=</span><span class="s1">&#39;subject8&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>/home/kemove/anaconda3/envs/il/lib/python3.9/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  warnings.warn(&quot;dropout option adds dropout after all but last &quot;
</pre></div>
</div>
</section>
<section id="id1">
<h2>Load the data<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">csv</span> <span class="o">=</span> <span class="s1">&#39;./data/hpe_dataset/csv/pose_regression_timeseries_subject_8/test.csv&#39;</span> <span class="c1"># The path contains the samosa dataset</span>
<span class="n">hpe_testdataset</span> <span class="o">=</span> <span class="n">SoundPoseLSTMDataset</span><span class="p">(</span><span class="n">csv</span><span class="p">,</span><span class="n">sound_length</span><span class="o">=</span><span class="mi">2400</span><span class="p">,</span><span class="n">input_feature</span><span class="o">=</span><span class="s1">&#39;raw&#39;</span><span class="p">,</span><span class="n">mean</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">get_raw_mean</span><span class="p">())</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">),</span><span class="n">std</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">get_raw_std</span><span class="p">())</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">))</span>
<span class="n">index</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># Randomly select an index</span>
<span class="n">sample</span><span class="o">=</span> <span class="n">hpe_testdataset</span><span class="o">.</span><span class="fm">__getitem__</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="model-inference">
<h2>Model inference<a class="headerlink" href="#model-inference" title="Permalink to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Method 1</span>
<span class="n">hpe_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">predicted_result</span> <span class="o">=</span> <span class="n">hpe_model</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;sound&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="n">vis_images</span> <span class="o">=</span> <span class="n">make_images</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;targets&#39;</span><span class="p">],</span><span class="n">predicted_result</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="c1">#Method 2</span>
<span class="kn">from</span> <span class="nn">pysensing.acoustic.inference.predict</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">predicted_result</span>  <span class="o">=</span> <span class="n">hpe_predict</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;sound&#39;</span><span class="p">],</span><span class="s1">&#39;SoundPoseLSTMDataset&#39;</span><span class="p">,</span><span class="n">hpe_model</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">vis_images</span> <span class="o">=</span> <span class="n">make_images</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;targets&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span><span class="n">predicted_result</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="n">seq_num</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">vis_images</span><span class="p">[</span><span class="n">seq_num</span><span class="p">][</span><span class="s1">&#39;img&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_acoustic_hpe_tutorial_002.png" srcset="../_images/sphx_glr_acoustic_hpe_tutorial_002.png" alt="acoustic hpe tutorial" class = "sphx-glr-single-img"/></section>
<section id="model-embedding">
<h2>Model embedding<a class="headerlink" href="#model-embedding" title="Permalink to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pysensing.acoustic.inference.embedding</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">sample_embedding</span> <span class="o">=</span> <span class="n">hpe_embedding</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;sound&#39;</span><span class="p">],</span><span class="s1">&#39;SoundPoseLSTMDataset&#39;</span><span class="p">,</span><span class="n">hpe_model</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>And that’s it. We’re done with our acoustic humna pose estimation tutorials. Thanks for reading.</p>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (3 minutes 10.114 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-acoustic-acoustic-hpe-tutorial-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/7a6f102b877e68e9956fa06c9728eff5/acoustic_hpe_tutorial.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">acoustic_hpe_tutorial.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/3f90f032a2f8d490ea6cc694e6ba4690/acoustic_hpe_tutorial.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">acoustic_hpe_tutorial.py</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../bluetooth/bluetooth_localization_tutorial.html" class="btn btn-neutral float-right" title="Bluetooth Localization Tutorial" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="acoustic_ped_det_tutorial.html" class="btn btn-neutral" title="Acoustic Pedestrian Detection Tutorial" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr class="rating-hr hr-top">
      <div class="rating-container">
        <div class="rating-prompt">Rate this Tutorial</div>
        <div class="stars-outer">
          <i class="far fa-star" title="1 Star" data-behavior="tutorial-rating" data-count="1"></i>
          <i class="far fa-star" title="2 Stars" data-behavior="tutorial-rating" data-count="2"></i>
          <i class="far fa-star" title="3 Stars" data-behavior="tutorial-rating" data-count="3"></i>
          <i class="far fa-star" title="4 Stars" data-behavior="tutorial-rating" data-count="4"></i>
          <i class="far fa-star" title="5 Stars" data-behavior="tutorial-rating" data-count="5"></i>
        </div>
      </div>
    <hr class="rating-hr hr-bottom"/>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2024, Pysensing contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>
<!--        <div class="pytorch-content-right" id="pytorch-content-right">-->
<!--          <div class="pytorch-right-menu" id="pytorch-right-menu">-->
<!--              <a class="twitter-timeline" data-width="100%" href="https://twitter.com/pypose_org?ref_src=twsrc%5Etfw">News from Twitter @pypose_rog<br>You need VPN if you see this.</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>-->
<!--          </div>-->
<!--        </div>-->
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../_static/doctools.js"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access documentation for Pysensing</p>
          <a class="with-right-arrow" href="https://www.pysensing.org/docs">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get started with tutorials and examples</p>
          <a class="with-right-arrow" href="https://www.pysensing.org/tutorial/">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Get Started</h2>
          <p>Find resources and how to start using pysensing</p>
          <a class="with-right-arrow" href="https://pysensing.org/get-started/locally">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://www.pysensing.org" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://www.pysensing.org">Pysensing</a></li>
            <li><a href="https://www.pysensing.org/get-started/locally">Get Started</a></li>
            <!-- <li><a href="https://github.com/pysensing/pysensing">Features</a></li> -->
            <li><a href="https://github.com/pysensing/pysensing">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://github.com/pysensing/pysensing">Resources</a></li>
            <li><a href="https://www.pysensing.org/tutorial/">Tutorials</a></li>
            <li><a href="https://www.pysensing.org/docs">Docs</a></li>
            <!-- <li><a href="https://github.com/pysensing/pysensing/issues" target="_blank">Github Issues</a></li> -->
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <!-- <li><a href="https://github.com/pysensing/pysensing" target="_blank">Twitter</a></li> -->
            <li><a href="https://github.com/pysensing/pysensing" target="_blank">GitHub</a></li>

          </ul>  
          </div>
        </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://www.pysensing.org" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://www.pysensing.org/get-started/locally">Get Started</a>
          </li>
          <li>
            <a href="https://www.pysensing.org/tutorial/">Tutorials</a>
          </li>
          <li>
            <a href="https://www.pysensing.org/docs">Docs</a>
          </li>
          <li>
            <a href="https://www.pysensing.org/about-us">About Us</a>
          </li>
          <li>
            <a href="https://github.com/pysensing/pysensing">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>