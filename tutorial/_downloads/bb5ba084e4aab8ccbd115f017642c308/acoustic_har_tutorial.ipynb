{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Acoustic Human Activity Recognition Tutorial\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "!pip install pysensing\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this tutorial, we will be implementing codes for acoustic Human activity recognition\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\ntorch.backends.cudnn.benchmark = True\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pysensing.acoustic.datasets.har as har_datasets\nimport pysensing.acoustic.models.har as har_models\nimport pysensing.acoustic.models.get_model as acoustic_models\nimport pysensing.acoustic.inference.embedding as embedding\nseed = 42\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\nnp.random.seed(seed)\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SAMoSA: Sensoring Activities with Motion abd Subsampled Audio\nSAMSoSA dataset is designed to use audio and IMU data collected by a watch to predict the actions of the users.\n\nThere are totally 27 actions in the dataset. \n\nIn the library, we provide a dataloader to use only audio data to predict these actions. \n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the data\nMethod 1: Use get_dataloader\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pysensing.acoustic.datasets.get_dataloader import *\ntrain_loader,test_loader = load_har_dataset(\n    root='./data',\n    dataset='samosa',\n    download=True)\n\n# Method 2: Manually setup the dataloader\nroot = './data' # The path contains the samosa dataset\nsamosa_traindataset = har_datasets.SAMoSA(root,'train')\nsamosa_testdataset = har_datasets.SAMoSA(root,'test')\n# Define the dataloader\nsamosa_trainloader = DataLoader(samosa_traindataset,batch_size=64,shuffle=True,drop_last=True)\nsamosa_testloader = DataLoader(samosa_testdataset,batch_size=64,shuffle=True,drop_last=True)\ndataclass = samosa_traindataset.class_dict\ndatalist  = samosa_traindataset.audio_data\n# Example of the samples in the dataset\nindex = 50  # Randomly select an index\nspectrogram,activity= samosa_traindataset.__getitem__(index)\nplt.figure(figsize=(10,5))\nplt.imshow(spectrogram.numpy()[0])\nplt.title(\"Spectrogram for activity: {}\".format(activity))\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the model\nMethod 1:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "samosa_model = har_models.HAR_SAMCNN(dropout=0.6).to(device)\n# Method 2:\nsamosa_model = acoustic_models.load_har_model('samcnn',pretrained=True).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model training and testing\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pysensing.acoustic.inference.training.har_train import *\n# Model training\nepoch = 1\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(samosa_model.parameters(), 0.0001)\nhar_train_val(samosa_model,samosa_trainloader,samosa_testloader, epoch, optimizer, criterion, device, save_dir = './data',save = True)\n\n# Model testing\ntest_loss = har_test(samosa_model,samosa_testloader,criterion,device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modle inference for single sample\nMethod 1\nYou may aslo load your own trained model by setting the path\nsamosa_model.load_state_dict(torch.load('path_to_model')) # the path for the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "spectrogram,activity= samosa_testdataset.__getitem__(3)\nsamosa_model.eval()\n#Direct use the model for sample inference\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\npredicted_result = samosa_model(spectrogram.unsqueeze(0).float().to(device))\nprint(\"The ground truth is {}, while the predicted activity is {}\".format(activity,torch.argmax(predicted_result).cpu()))\n\n# Method 2\n# Use inference.predict\nfrom pysensing.acoustic.inference.predict import *\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\npredicted_result  = har_predict(spectrogram,'SAMoSA',samosa_model, device)\nprint(\"The ground truth is {}, while the predicted activity is {}\".format(activity,torch.argmax(predicted_result).cpu()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modle inference for single sample\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pysensing.acoustic.inference.embedding import *\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nsample_embedding = har_embedding(spectrogram,'SAMoSA',samosa_model, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementation of \"AudioIMU: Enhancing Inertial Sensing-Based Activity Recognition with Acoustic Models\"\nThis dataset is designed to use audio and IMU data collected by a watch to predict the actions of the users, 23 different activities are collected in the dataset.\n\nBut different from the orginal paper, the reimplemeted paper only takes the audio data for human activity recognition. Subjects 01, 02, 03, 04 are used for testing while the other are used for training.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the data\nMethod 1: Use get_dataloader\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pysensing.acoustic.datasets.get_dataloader import *\ntrain_loader,test_loader = load_har_dataset(\n    root='./data',\n    dataset='audioimu',\n    download=True)\n\n# Method2\nroot = './data' # The path contains the audioimu dataset\naudioimu_traindataset = har_datasets.AudioIMU(root,'train')\naudioimu_testdataset = har_datasets.AudioIMU(root,'test')\n# Define the Dataloader\naudioimu_trainloader = DataLoader(audioimu_traindataset,batch_size=64,shuffle=False,drop_last=True)\naudioimu_testloader = DataLoader(audioimu_testdataset,batch_size=64,shuffle=False,drop_last=True)\n#List the activity classes in the dataset\ndataclass = audioimu_traindataset.classlist\n# Example of the samples in the dataset\nindex = 0  # Randomly select an index\nspectrogram,activity= audioimu_testdataset.__getitem__(index)\nprint(spectrogram.shape)\nplt.figure(figsize=(18,6))\nplt.imshow(spectrogram.numpy()[0])\nplt.title(\"Spectrogram for activity: {}\".format(activity))\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the model\nMethod 1\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "audio_model = har_models.HAR_AUDIOCNN().to(device)\n# Method2\naudio_model = acoustic_models.load_har_model('audiocnn',pretrained=True).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model training and testing\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pysensing.acoustic.inference.training.har_train import *\nepoch = 1\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(audio_model.parameters(), 0.0001)\nloss = har_train_val(audio_model,audioimu_trainloader,audioimu_testloader, epoch, optimizer, criterion, device, save_dir='./data',save = False)\n\n# Model testing\ntest_loss = har_test(audio_model,audioimu_testloader,criterion,device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model inference\nYou may aslo load your own trained model by setting the path\naudio_model.load_state_dict(torch.load('path_to_model')) # the path for the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nspectrogram,activity= audioimu_testdataset.__getitem__(6)\naudio_model.eval()\npredicted_result = audio_model(spectrogram.unsqueeze(0).float().to(device))\nprint(\"The ground truth is {}, while the predicted activity is {}\".format(activity,torch.argmax(predicted_result).cpu()))\n\n#Method 2\n#Use inference.predict\nfrom pysensing.acoustic.inference.predict import *\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\npredicted_result  = har_predict(spectrogram,'AudioIMU',audio_model, device)\nprint(\"The ground truth is {}, while the predicted activity is {}\".format(activity,torch.argmax(predicted_result).cpu()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model embedding\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pysensing.acoustic.inference.embedding import *\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nsample_embedding = har_embedding(spectrogram,'AudioIMU',audio_model, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And that's it. We're done with our acoustic humna activity recognition tutorials. Thanks for reading.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}