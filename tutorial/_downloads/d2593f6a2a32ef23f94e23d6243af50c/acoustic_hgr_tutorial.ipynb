{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Acoustic Hand Gesture Recognition Tutorial\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "!pip install pysensing\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this tutorial, we will be implementing codes for acoustic Hand Gesture Recognition\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torchaudio\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\nimport os\nimport numpy as np\nimport torch.nn as nn\nimport tqdm\nimport sys\nfrom pysensing.acoustic.datasets.hgr import AMG\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hand Gesture Recognition with Acoustic Myography and Wavelet Scattering Transform\nReimplementation of \"Hand Gesture Recognition with Acoustic Myography and Wavelet Scattering Transform\"\n\nThis dataset contains acoustic myography of different hand gestures. The aucoustic data is in 8 channel. In this library, subjects\nAA01, CU14, DH18, NL20, NM08 and SR11 are selected as testing data, while the remaining used for training.\n\nThe classes contains in the dataset are [Pronation, Supination, Wrist Flexion, Wrist Extension, Radial Deviation, Ulnar Deviation,\nHand close, Hand open, Hook grip, Fine pinch, Tripod grip, Index finger flexion, Thumb finger flexion, and No movement (Rest)]\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the data\nMethod 1: Use get_dataloader\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pysensing.acoustic.datasets.get_dataloader import *\ntrain_loader,test_loader = load_hgr_dataset(\n    root='./data',\n    download=True)\n\n# Method 2: Manually setup the dataloader\nroot = './data' # The path contains the samosa dataset\namg_traindataset = AMG(root,'train')\namg_testdataset = AMG(root,'test')\n# Define the Dataloader\namg_trainloader = DataLoader(amg_traindataset,batch_size=32,shuffle=False,drop_last=True)\namg_testloader = DataLoader(amg_testdataset,batch_size=32,shuffle=False,drop_last=True)\n#List the activity classes in the dataset\ndataclass = amg_traindataset.class_dict\n# Example of the samples in the dataset\nindex = 128\n# Randomly 3elect an index\nspectrogram,activity= amg_traindataset.__getitem__(index)\nplt.figure(figsize=(10,6))\nplt.imshow(spectrogram.numpy()[0])\nplt.title(\"Spectrogram for activity: {}\".format(activity))\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And that's it. We're done with our acoustic hand gesture recognition tutorials. Thanks for reading.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}