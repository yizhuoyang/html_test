{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nThis notebook is a tutroial for using the mmwave raw data before the FFT\n( Time, chirps, virtual antennas, virtual antenna per chirp) specified\nin cubelearn\n\nhttps://github.com/zhaoymn/cubelearn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data loading and prepocessing\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\n# loading example data sample from the cubelearn HGR data\n# {user}_{gesture(label)}_{idx}.npy}\n# data have shape size (2, T, 128, 12, 256), where 2 is the real and complex part of the raw data, \n# T is the timestamps (10 for HGR and AGR, 20 for HAR), 128 is the number of chirps in a frame, 12 is the virtual antennas \n# https://github.com/zhaoymn/cubelearn?tab=readme-ov-file\nuser = 7\nlabel = 2\nsample = 1\n\n#replace with your data path, please download and unzip data from https://github.com/zhaoymn/cubelearn?tab=readme-ov-file\n#HAR data path should be .../HAR_data/activity_organized/{user}_{label}_{sample}.npy\nraw_data = np.load(f'./{user}_{label}_{sample}.npy')\n\n#combine the real and complex part\ndata = raw_data[0, :, :, :, :] + raw_data[1,:,:,:,:] * 1j\n\n#DAT and RDAT models takes partial input for efficiency, skip this in other model\ndata = data[:,:64,:,:128]\n\n#Data type is complex64\ndata = np.array(data, dtype=np.complex64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# model loading and inference\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport requests\nfrom pysensing.mmwave_raw.models.network import DAT_2DCNNLSTM\n\n# URL of the pretrained model\npretrained_model_url = \"https://pysensing.oss-ap-southeast-1.aliyuncs.com/pretrain/mmwave_raw/HAR/DAT_2DCNNLSTM_HAR.pth\"\n# */pretrain/modality/task/model_name.pth\n# modelname = {DAT_2DCNNLSTM_HAR,DAT_2DCNNLSTM_AGR,DAT_2DCNNLSTM_HGR,RDAT_3DCNNLSTM_HAR,RDAT_3DCNNLSTM_AGR,RDAT_3DCNNLSTM_HGR}\nlocal_model_path = \"./DAT_2DCNNLSTM_HAR.pth\"\n\n# Download the pretrained weights\nresponse = requests.get(pretrained_model_url)\nwith open(local_model_path, \"wb\") as f:\n    f.write(response.content)\n\n#loading the model and pretrained weight\nmodel = DAT_2DCNNLSTM(HAR=True)\nmodel.load_state_dict(torch.load(local_model_path, weights_only=True)['model_state_dict'])\nmodel.eval()\n\n#convert data to torch tensor\ndata = torch.tensor(data)\n\n#unsqueeze for the batch dimension\nx = data.unsqueeze(0) \none_hot = model(x)\n\n#class prediction\nclass_idx = torch.argmax(one_hot)\n\nprint(f\"The prediction is {class_idx==label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Embedding extraction\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For lstm models the embedding is extracted after the lstm (recommened)\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pysensing.mmwave_raw.inference.embedding import embedding\n\nemb = embedding(x,model,'cpu',True)\n\nprint(emb.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For non-lstm model the embedding is extracted after the final max\npooling layer berfore the FCs, might have different shape for different\nmodels\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pysensing.mmwave_raw.models.network import DAT_3DCNN\n\nmodel_ = DAT_3DCNN()\n\nemb = embedding(x,model_,'cpu',False)\n\nprint(emb.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "for non DAT and RDAT models don\u2019t forget to use the whole data\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pysensing.mmwave_raw.models.network import RAT_3DCNN\nmodel_ = RAT_3DCNN()\ndata_ = raw_data[0, :, :, :, :] + raw_data[1,:,:,:,:] * 1j\ndata_ = data_[:,:128,:,:256] #whole data cube\ndata_ = np.array(data_, dtype=np.complex64)\ndata_ = torch.tensor(data_)\nx_ = data_.unsqueeze(0) \nemb = embedding(x_,model_,'cpu',False)\nprint(emb.shape)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}