{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Acoustic Pedestrian Detection Tutorial\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "!pip install pysensing\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this tutorial, we will be implementing codes for acoustic Human pose estimation\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport pysensing.acoustic.preprocessing.transform as transform\nfrom pysensing.acoustic.inference.utils import *\nfrom pysensing.acoustic.datasets.ped_det import AVPed,AFPILD\nfrom pysensing.acoustic.models.ped_det import PED_CNN,PED_CRNN\nfrom pysensing.acoustic.models.get_model import load_ped_det_model\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AV-PedAware: Self-Supervised Audio-Visual Fusion for Dynamic Pedestrian Awareness\nReimplementation of \"AV-PedAware: Self-Supervised Audio-Visual Fusion for Dynamic Pedestrian Awareness\".\n\nThis dataset contains the footstep sound of the pedestains which used for pedestrian localization..\n\nNote: Different from original paper which utilizes both audio and visual data to train the network. This library only focuses on using only audio data for pedestrian localization.\n\nThe dataset can be downloaded from https://github.com/yizhuoyang/AV-PedAware\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the data\nThe dataset can be downloaded from this github repo: https://github.com/yizhuoyang/AV-PedAware\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "root = '/home/kemove/yyz/pysensing/avped' # The path contains the AVPed dataset\navped_traindataset = AVPed(root,'train')\navped_testdataset = AVPed(root,'test')\nindex = 20\n# Randomly select an index\nspectrogram,position,lidar= avped_traindataset.__getitem__(index)\nplt.figure(figsize=(5,3))\nplt.imshow(spectrogram.numpy()[0])\nplt.title(\"Spectrogram\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Method 1:\navped_model = PED_CNN(0.2).to(device)\n# Method 2:\navped_model = load_ped_det_model('ped_cnn',pretrained=True).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modle Training and Testing\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Model training\nfrom pysensing.acoustic.inference.training.ped_det_train import *\navped_trainloader = DataLoader(avped_traindataset,batch_size=64,shuffle=True,drop_last=True)\navped_testloader  = DataLoader(avped_traindataset,batch_size=64,shuffle=True,drop_last=True)\nepoch = 1\noptimizer = torch.optim.Adam(avped_model.parameters(), 0.001)\nloss = ped_det_train_val(avped_model,avped_trainloader,avped_testloader, epoch, optimizer, device, save_dir='/data',save = False)\n\n# Model testing\nloss = ped_det_test(avped_model,avped_testloader,  device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modle Inference\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Method 1\nspectrogram,position,lidar= avped_testdataset.__getitem__(1)\navped_model.eval()\n#Direct prediction use the model\npredicted_result = avped_model(spectrogram.unsqueeze(0).float().to(device))\nposition = position.unsqueeze(0).numpy()\npredicted_result = predicted_result.cpu().detach().numpy()\ndraw_scenes(lidar,position,predicted_result)\n\n# Method 2\n#Use inference.predict\nfrom pysensing.acoustic.inference.predict import *\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\npredicted_result  = ped_det_predict(spectrogram,'AVPed',avped_model, device=device)\npredicted_result = predicted_result.cpu().detach().numpy()\ndraw_scenes(lidar,position,predicted_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modle Embedding\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pysensing.acoustic.inference.embedding import *\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nsample_embedding = ped_det_embedding(spectrogram,'AVPed',avped_model, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AFPILD: Acoustic footstep dataset collected using one microphone array and LiDAR sensor for person identification and localization\nReimplementation of \"AFPILD: Acoustic footstep dataset collected using one microphone array and LiDAR sensor for person identification and localization\".\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "3\n# This dataset contains footstep sound of the pedestains which used for pedestrian localization and classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Method 1: Use get_dataloader\nfrom pysensing.acoustic.datasets.get_dataloader import *\ntrain_loader,test_loader = load_ped_det_dataset(\n    root='./data',\n    dataset='AFPILD',\n    download=True)\n\n# Method 2\nroot = './data' # The path contains the AFPILD dataset\nafpild_traindataset = AFPILD(root,'ideloc_ori_cloth','train')\nafpild_testdataset = AFPILD(root,'ideloc_ori_cloth','test')\n# Define the Dataloader\nafpild_trainloader = DataLoader(afpild_traindataset,batch_size=64,shuffle=True,drop_last=True)\nafpild_testloader = DataLoader(afpild_testdataset,batch_size=64,shuffle=True,drop_last=True)\n#List the activity classes in the dataset\nindex = 330\n# Randomly select an index\ndata_dict,label = afpild_testdataset.__getitem__(index)\n\nfig, axs = plt.subplots(1, 2, figsize=(7, 4))\n\naxs[0].imshow(data_dict['spec'][:,:,0], aspect='auto', origin='lower')\naxs[0].set_title('Spectrogram')\naxs[0].set_xlabel('Time')\naxs[0].set_ylabel('Frequency')\n\naxs[1].imshow(data_dict['gcc'][:,:,0], aspect='auto', origin='lower')\naxs[1].set_title('GCC')\naxs[1].set_xlabel('Time')\naxs[1].set_ylabel('Lag')\n\nperson_id, angle = label\nprint(f\"Person ID: {person_id}, Angle: {angle:.2f} radians\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model training\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pysensing.acoustic.inference.training.AFPILD_utils.training import afpild_train\n\nafpild_train(\n      config_file=\"./data/AFPILD/afpild_spec_gcc_fusion.json\",\n      root_dir='./data/AFPILD',\n      task='accil_ana_shoe',\n      epochs=1,\n      num_workers=4,\n      dataset_dir='./data/AFPILD/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model testing\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pysensing.acoustic.inference.training.AFPILD_utils.testing import afpild_testing\nafpild_testing(\n    config_file=\"./data/AFPILD/afpild_spec_gcc_fusion.json\",\n    root_dir= \"./data/AFPILD\",\n    dataset_dir=\"./data/AFPILD/\",\n    resume_path=\"./data/AFPILD/saved/AFPILD-CRNN/20241030055348/model/model_best.pth\", # Path to the trained model\n    task='accil_ori_rd',\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model inference\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Load the model 1\navped_model = PED_CRNN(task='ideloc_ori_cloth.pth').to(device)\n# avped_model.load_state_dict(torch.load('path to weights',weights_only=True)['models']['model'])\n\n# Load the model 2\navped_model = load_ped_det_model('ped_crnn',pretrained=True,task='ideloc_ori_cloth').to(device)\n\n# Model prediction 1\ndata_dict_tensor = {k: torch.Tensor(v).to(device).unsqueeze(0).float() for k, v in data_dict.items()}\noutput = avped_model(data_dict_tensor).squeeze(0).detach().cpu().numpy()\n#print(\"The predicted person id is: {}, the ground truth is: {}\".format(np.argmax(output[:40]),int(label[0])))\n#print(\"The predicted angle is: {}, the ground truth is: {}\".format(output[-1],label[1]))\n\n# Model prediction 2\nfrom pysensing.acoustic.inference.predict import *\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\npredicted_result  = ped_det_predict(data_dict,'AFPILD',avped_model, device=device)\npredicted_result = predicted_result.cpu().detach().numpy()\nprint(\"The predicted person id is: {}, the ground truth is: {}\".format(np.argmax(output[:40]),int(label[0])))\nprint(\"The predicted angle is: {}, the ground truth is: {}\".format(output[-1],label[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model embedding\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pysensing.acoustic.inference.embedding import *\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nsample_embedding = ped_det_embedding(data_dict,'AFPILD',avped_model, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And that's it. We're done with our acoustic humna pose estimation tutorials. Thanks for reading.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}