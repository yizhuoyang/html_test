{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Tutorial for Human Activity Recognition\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "!/usr/bin/env python\n coding: utf-8\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# ------------------------\n# In[1]:\n\n\nimport yaml\nimport torch\nimport torch.nn as nn\nfrom tqdm import tqdm\nimport os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset with radHAR: \n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "radHAR dataset is designed to use mmWave PC data collected by IWR1443Ti to predict the actions of the users.\nThere are totally 5 actions in the dataset: ['boxing','jack','jump','squats','walk']\nIn the library, we provide a dataloader to use mmWave PC data , converted into voxel image, and predict these actions. \n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the data\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In[2]:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pysensing.mmwave.PC.dataset.har import load_har_dataset\n# The path contains the radHAR dataset\ntrain_dataset, test_dataset = load_har_dataset(\"radHAR\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the voxel image\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In[3]:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n# Example of the samples in the dataset\nindex = 9  # Randomly select an index\nvoxels,activity = train_dataset.__getitem__(index)\n\n\nprint(voxels.shape, type(voxels))\n\nplt.figure(figsize=(10,6))\nplt.imshow(voxels[0].transpose(1,2,0).mean(-1))\nplt.title(\"Voxel image for activity: {}\".format(activity))\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create model \n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "raHAR utilizes MLP-based model as a baseline har method. From model.har, we can import desired har model designed for mmWave PC. The model parameter for har_MLP reimplemented for radHAR is as follows:\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In[4]:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pysensing.mmwave.PC.model.har import har_MLP\nmodel = har_MLP(dataset=\"radHAR\", num_classes=5)\nprint(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Train\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "pysensing library support quick training of model with the following steps. The training interface incorporates pytorch loss functions, optimizers and dataloaders to facilate training. An example is provided for how to define the aforemetioned terms.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# In[5]:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Create pytorch dataloaders\ntrain_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=128, num_workers=16)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=16)\n\n\n# Define pytorch loss function as criterion \ncriterion = nn.CrossEntropyLoss()\n\n\n# Define pytorch optimizer for training\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\n# GPU acceleration with cuda\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A quick training using har_train. The resulted model parameters will be saved into \"train_{num_epochs}.pth\".\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In[6]:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Pysensing training interface\nfrom pysensing.mmwave.PC.inference.har import har_train\n# har_train(model, train_loader, num_epochs=1, optimizer=optimizer, criterion=criterion, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model inference\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the pretrained model, e.g. from  https://pysensing.oss-ap-southeast-1.aliyuncs.com/pretrain/mmwave_pc/HAR/radHAR_MLP.pth\n, and perform human action recognition!\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In[7]:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# load pretrained model\nfrom pysensing.mmwave.PC.inference import load_pretrain\nmodel = load_pretrain(model, \"radHAR\", \"har_MLP\").to(device)\nmodel.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test the model on testing dataset.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In[8]:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pysensing.mmwave.PC.inference.har import har_test\n# har_test(model, test_loader, criterion=criterion, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model inference on sample and deep feature embedding of input modality in HAR task.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In[9]:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "idx = 5\npc, label= test_dataset.__getitem__(idx)\nprint(pc.shape)\npc = torch.tensor(pc).unsqueeze(0).float().to(device)\npredicted_result = model(pc)\nprint(\"The predicted gesture is {}, while the ground truth is {}\".format(label,torch.argmax(predicted_result).cpu()))\n\n# Deep feature embedding\nfrom pysensing.mmwave.PC.inference.embedding import embedding\nemb = embedding(input = pc, model=model, dataset_name = \"radHAR\", model_name = \"har_MLP\", device=device)\nprint(\"The shape of feature embedding is: \", emb.shape)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}