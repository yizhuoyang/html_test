{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Acoustic Preprocessing.Filter Tutorial\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# !pip install pysensing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this tutorial, we will be implementing a simple acoustic.preprocessing.filtering\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torchaudio\nimport matplotlib.pyplot as plt\nimport pysensing.acoustic.preprocessing.filtering as filtering\nimport pysensing.acoustic.preprocessing.transform as transform\nimport matplotlib.gridspec as gridspec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the audio\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# First, the example audio is loaded\n\n# Define the plot function\ndef plot_wave_and_spec(waveform,sample_rate):\n    specgram = transform.spectrogram()(waveform)\n    specgram = transform.amplitude2db()(specgram)\n    n_fft = waveform.size(-1)\n    freqs = torch.linspace(0, sample_rate / 2, int(n_fft / 2) + 1)\n\n    plt.figure(figsize=(10, 6))\n    plt.subplot(2, 1, 1)\n    plt.imshow(specgram[0].squeeze().numpy(), aspect='auto', origin='lower', cmap='inferno', extent=[0, waveform.size(1) / sample_rate, 0, sample_rate / 2])\n    plt.title('Spectrogram')\n    plt.ylim(0,5000)\n    plt.xlabel('Time (s)')\n    plt.ylabel('Frequency (Hz)')\n\n    plt.subplot(2, 1, 2)\n    plt.plot(waveform.t().numpy())\n    plt.title('Waveform')\n    plt.xlabel('Sample')\n    plt.ylabel('Amplitude')\n    plt.tight_layout()\n    plt.show()\n\n# Load the data\nwaveform, sample_rate = torchaudio.load('example_data/example_audio.wav')\n#plot the original audio data and spectrogram\nplot_wave_and_spec(waveform,sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. High pass filter\nUse high pass filter\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "highpass_trans = filtering.highpass(sample_rate=44100,cutoff_freq=2000.0)\nhighpass       = highpass_trans(waveform)\nplot_wave_and_spec(highpass,sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Low pass filter\nUse low pass filter\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lowpass_trans = filtering.lowpass(sample_rate=44100,cutoff_freq=200.0)\nlowpass       = lowpass_trans(waveform)\nplot_wave_and_spec(lowpass,sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Bandpass filter\nUse bandpass filter\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bandpass_trans = filtering.bandpass(sample_rate=44100,central_freq=1000)\nbandpass       = bandpass_trans(waveform)\nplot_wave_and_spec(bandpass,sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Bandreject filter\nUse bandreject filter\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bandreject_trans = filtering.bandreject(sample_rate=44100,central_freq=1000)\nbandreject       = bandreject_trans(waveform)\nplot_wave_and_spec(bandreject,sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Allpass filter\nUse allpass filter\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "allpass_trans = filtering.allpass(sample_rate=44100,central_freq=2000)\nallpass     = allpass_trans(waveform)\nplot_wave_and_spec(allpass,sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And that's it. We're done with our acoustic preprocessing tutorials. Thanks for reading.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}