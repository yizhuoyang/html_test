{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Acoustic Human Pose estimation Tutorial\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "!pip install pysensing\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this tutorial, we will be implementing codes for acoustic Human pose estimation\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pysensing.acoustic.datasets.utils.hpe_vis import *\nfrom pysensing.acoustic.models.hpe import Speech2pose,Wipose_LSTM\nfrom pysensing.acoustic.models.get_model import load_hpe_model\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Listening Human Behavior: 3D Human Pose Estimation with Acoustic Signals\n ----------------------------------------------------------------------------------\nImplementation of \"Listening Human Behavior: 3D Human Pose Estimation with Acoustic Signals\".\n\nThis dataset contains the audio reflected by human to estimate the 3D human pose with the acoustic signals.\n\nReference: https://github.com/YutoShibata07/AcousticPose_Public\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Method 1: Use get_dataloader\nfrom pysensing.acoustic.datasets.get_dataloader import *\ntrain_loader,val_loader,test_loader = load_hpe_dataset(\n    root='./data',\n    dataset_name='pose_regression_timeseries_subject_1',\n    download=True)\n\n# Method 2\ncsv = './data/hpe_dataset/csv/pose_regression_timeseries_subject_1/test.csv' # The path contains the samosa dataset\ndata_dir = './data'\nhpe_testdataset = SoundPose2DDataset(csv,sound_length=2400,input_feature='logmel',\n                                     mean=np.array(get_mean()).astype(\"float32\")[:4],\n                                     std=np.array(get_std()).astype(\"float32\")[:4],\n                                     )\nindex = 10 # Randomly select an index\nsample= hpe_testdataset.__getitem__(index)\nprint(sample['targets'].shape)\nprint(sample['sound'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Speech2pose model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Method 1\nhpe_model = Speech2pose(out_cha=63).to(device)\n# model_path = 'path to pretrian weights'\n# state_dict = torch.load(model_path,weights_only=True)\n# hpe_model.load_state_dict(state_dict)\n\n# Method 2\nhpe_model = load_hpe_model('speech2pose',pretrained=True,task='subject8').to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modle Inference\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#Method 1\nsample= hpe_testdataset.__getitem__(index)\nhpe_model.eval()\npredicted_result = hpe_model(sample['sound'].unsqueeze(0).float().to(device))\nvis_images = make_images(sample['targets'].numpy(),predicted_result.cpu().detach().numpy().squeeze(0))\n\n#Method 2\nfrom pysensing.acoustic.inference.predict import *\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\npredicted_result  = hpe_predict(sample['sound'],'SoundPose2DDataset',hpe_model, device=device)\nvis_images = make_images(sample['targets'].numpy(),predicted_result.cpu().detach().numpy().squeeze(0))\n\nseq_num = 0\nfig = plt.figure(figsize=(12, 12))\nplt.imshow(vis_images[seq_num]['img'])\nplt.axis('off')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modle Embedding\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pysensing.acoustic.inference.embedding import *\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nsample_embedding = hpe_embedding(sample['sound'],'SoundPose2DDataset',hpe_model, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modle Training\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pysensing.acoustic.inference.training.AcousticPose_utils.hpe_train import train_model,generate_configs\n\nargs = {\n    \"root_dir\": \"./data/hpe_dataset/testing_result\",\n    \"save_name\": \"seq1\",\n    \"input_feature\": \"logmel\",\n    \"batchsize\": 64,\n    \"max_epoch\": 50,\n    \"csv_path\": \"./data/hpe_dataset/csv\",\n    \"dataset_name\": \"pose_regression_timeseries_subject_1\",\n    \"model\": \"speech2pose\",\n    \"sound_length\": 2400,\n    \"learning_rate\": 0.01,\n}\nconfig_path = args[\"root_dir\"]+'/'+args[\"save_name\"]+\"/config.yaml\"\ngenerate_configs(args)\nresume_training = False\nrandom_seed = 0\n\ntrain_model(\n    config_path=config_path,\n    resume=resume_training,\n    seed=random_seed,\n)\n\n# Modle Training\n# ------------------------\nfrom pysensing.acoustic.inference.training.AcousticPose_utils.hpe_test import evaluate_model\nargs = {\n    \"root_dir\": \"./data/hpe_dataset/testing_result\",\n    \"save_name\": \"seq1\",\n    \"batchsize\": 64,\n    \"max_epoch\": 20,\n    \"csv_path\": \"./data/hpe_dataset/csv\",\n    \"dataset_name\": \"pose_regression_timeseries_subject_1\",\n    \"model\": \"speech2pose\",\n    \"sound_length\": 2400,\n    \"learning_rate\": 0.01,\n}\nconfig_path = args[\"root_dir\"]+'/'+args[\"save_name\"]+\"/config.yaml\"\nevaluation_mode = \"test\"\nmodel_path = None\n\nevaluate_model(\n    config_path=config_path,\n    mode=evaluation_mode,\n    model_path=model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the Wipose_LSTM model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Method 1\nhpe_model = Wipose_LSTM(in_cha=4,out_cha=63).to(device)\n# model_path = 'path to trained model'\n# state_dict = torch.load(model_path,weights_only=True)\n\n# Method 2\nhpe_model = load_hpe_model('wipose',pretrained=True,task='subject8').to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "csv = './data/hpe_dataset/csv/pose_regression_timeseries_subject_8/test.csv' # The path contains the samosa dataset\nhpe_testdataset = SoundPoseLSTMDataset(csv,sound_length=2400,input_feature='raw',mean=np.array(get_raw_mean()).astype(\"float32\"),std=np.array(get_raw_std()).astype(\"float32\"))\nindex = 0 # Randomly select an index\nsample= hpe_testdataset.__getitem__(index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model inference\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Method 1\nhpe_model.eval()\npredicted_result = hpe_model(sample['sound'].unsqueeze(0).float().to(device))\nvis_images = make_images(sample['targets'],predicted_result.cpu().detach().squeeze(0))\n\n#Method 2\nfrom pysensing.acoustic.inference.predict import *\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\npredicted_result  = hpe_predict(sample['sound'],'SoundPoseLSTMDataset',hpe_model, device=device)\nvis_images = make_images(sample['targets'].numpy(),predicted_result.cpu().detach().numpy().squeeze(0))\n\nseq_num = 2\nfig = plt.figure(figsize=(12, 12))\nplt.imshow(vis_images[seq_num]['img'])\nplt.axis('off')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model embedding\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pysensing.acoustic.inference.embedding import *\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nsample_embedding = hpe_embedding(sample['sound'],'SoundPoseLSTMDataset',hpe_model, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And that's it. We're done with our acoustic humna pose estimation tutorials. Thanks for reading.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}